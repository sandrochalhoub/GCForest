\documentclass{article}
\usepackage{neurips_2021}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%% http://ctan.org/pkg/xcolor
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{xspace}
\usepackage{array}
%\usepackage{amsthm}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{arrows,shadows,fit,calc,positioning,decorations.pathreplacing,matrix,shapes,petri,topaths,fadings,mindmap,backgrounds,shapes.geometric}

\usepackage{pgfplots}
\usepackage{fp}
\usepackage{subfig}

% \usepackage{geometry}p
\usepackage{xifthen}
\usepackage{rotating}
\usepackage{forest}
\usepackage{relsize}

\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}


\input{src/macros.tex}
	

\DontPrintSemicolon

\title{A Simple and Efficient Anytime Algorithm for Computing Optimal Decision Trees}


\author{%
  Emir Demirovi\'c \\
  TU DELFT \\
	The Netherlands \\
  \texttt{e.demirovic@tudelft.nl} \\
	\And
	Emmanuel Hebrard \\
	LAAS-CNRS \\
	Universit\'e de Toulouse, CNRS \\
	France \\
	\texttt{hebrard@laas.fr} \\
	\And
	Louis Jean \\
	LAAS-CNRS \\
	Universit\'e de Toulouse, CNRS \\
	France \\
	\texttt{ljean@laas.fr} \\
}


\begin{document}


\maketitle






\begin{abstract}
	In this paper we introduce a {simple} algorithm to learn optimal decision trees of bounded depth. This algorithm, \blossom, is as memory and time efficient as heuristics, and yet more efficient than most exact methods on most data sets. 
	Its worst case time complexity is the same as state-of-the-art dynamic programming methods. However, its anytime behavior is vastly superior.
	Experiments show that whereas existing exact methods hardly scale to deep trees, our algorithm learns trees comparable to standard heuristics without significant computational overhead, and can significantly improve their accuracy when given more computation time.
	
	% State-of-the-art exact methods often have poor anytime behavior, and hardly scale to deep trees.
	% Experiments show that they are typically orders of magnitude slower than the proposed algorithm to compute optimally accurate classifiers of a given depth.
%On the other hand, \blossom\ finds, without significant computational overhead, solutions comparable to those returned by standard greedy heuristics, and can quickly improve their accuracy when given more computation time.
% the first solution found by \blossom\ is comparable to those found by standard greedy heuristics and that significantly improve upon greedy heuristics. On the 
\end{abstract}



\section{Introduction}

In conclusion of their short paper showing that computing decision trees of maximum accuracy is NP-complete, Hyafil and Rivest write: ``Accordingly, it is to be expected that that good heuristics for constructing near-optimal binary decision trees will be the best solution to this problem in the near future.''~\cite{NPhardTrees}. Indeed, heuristic approaches such as \cart\cite{breiman1984classification}, \idthree~\cite{10.1023/A:1022643204877} or \cfour~\cite{c4-5} have been prevalent long afterward, and are still vastly more commonly used in practice than exact approaches. In this paper, we propose a new exact algorithm (\blossom) which, while being effective at proving optimality, does not have computational or memory overhead compared to greedy heuristics.

%\medskip

It is well established that optimal trees (for some combination of accuracy, depth and size) generalize better to unseen data.
% than heuristic trees. 
%This experiment has been confirmed many times, 
Previous experiments show a significative gain in test accuracy,
in particular for the objective criterion considered in this paper: maximizing the training accuracy given an upper bound on the depth~\cite{avellanedaefficient,bertsimas2017optimal,bertsimas2007classification,DBLP:journals/corr/abs-2007-12652,DBLP:conf/ijcai/Hu0HH20,dl8}. %We rely on this prior work and hence we do not reproduce in this paper experiments comparing optimized trees to heuristic trees on unseen data.
%\footnote{Hence we shall not reproduce once again such experiments in this paper.} 
Other objective criteria have been considered. For instance, the algorithm \gosdt~\cite{NEURIPS2019_ac52c626} optimizes a linear combination of accuracy and number of leaves. However maximizing the accuracy under a constrained depth has valuable properties, e.g.,
this is easier to tackle algorithmically, and the predictions of
 shallower trees are easier to interpret and explain. 


%\medskip

Despite these desirable features, exact methods have not been widely adopted yet for a simple reason: they do not scale. There has been a significant progress lately, and the most recent approaches show very promising results. However, no exact method can replace heuristics in all contexts. 
For SAT~\cite{avellanedaefficient,narodytska2018learning} and Integer Programming approaches~\cite{aghaei2020learning,bertsimas2017optimal,bertsimas2007classification,verwer2019learning}, the size of the encoding is a first hurdle. All these models require a number of variables at least proportional to the size of the tree and to the number of datapoints. As a result, scaling beyond a few thousands datapoints is difficult. 
On the other hand, dynamic programming algorithms \olddleight~\cite{dl8} and \dleight~\cite{dl85} scale very well to large data sets. Moreover, these algorithms leverage branch independence: sibling subtrees can be optimized independently, which has a significant impact on computational complexity. However, \dleight tends to be memory hungry and furthermore, is not anytime.
The constraint programming approach of Verhaeghe \textit{et al.} emulates these positive features using dedicated propagation algorithms and search strategies~\cite{verhaeghe2019learning}, while being potentially anytime, although it does not quite match \dleight's efficiency.
Finally, a recently introduced algorithm, \murtree~\cite{DBLP:journals/corr/abs-2007-12652}, improves on the dynamic programming approaches in several ways: as the algorithm introduced in this paper it explores the search space in a more flexible way. Moreover, it implements several methods dedicated to exploring the whole search space very fast: delaying feature frequency counts to a specialized algorithm for subtree of depth two, and implementing an efficient recomputation method for the classification error, for instance.
As a result, it outperforms previous exact methods: it is more memory efficient, orders of magnitude faster than \dleight, and has a better anytime behavior. However, experimental results show that for deeper trees, none of these methods can reliably outperform heuristics, whereas \blossom\ does. Moreover, it is more memory efficient than \murtree, and its pseudo-code is significantly simpler.
 

% % \medskip
%
% In this paper we introduce a relatively \emph{simple} algorithm (\blossom), that is as memory and time efficient as heuristics, and yet more efficient than most exact methods on most data sets.
% This algorithm can be seen as an instance of the more general framework introduced in \cite{DBLP:journals/corr/abs-2007-12652}, however tuned to have the best scalability to large trees and the best anytime behavior as possible.
% %As a result, it is comparable to \murtree on shallow trees, while clearly outperforming the state of the art on deep trees.

In a nutshell, \blossom emulates the dynamic programming algorithm \dleight~\cite{dl8}, while always expanding non-terminal branches (a.k.a ``buds'') before optimizing grown branches. As a result, this algorithm is in a sense strictly better than both the standard dynamic programming approach (because it is anytime and at least as fast) and than classic heuristics (because it emulates them during search, without significant overhead).
%but explores the search space so as to improve its anytime behaviour.
Our experimental results show that it outperforms the state of the art, to the exception of \murtree on relatively shallow trees (typically for maximum depth up to 4), for which its more sophisticated (albeit more complex) algorithmic features can pay off.
%In particular, on data sets that \dleight can tackle, \blossom can always find classifiers at least as accurate faster, and when the former can prove optimality, the latter does it orders of magnitude faster.




% Therefore, we shall not reproduce once again such experiments and only consider efficiency with respect to the primary objective.
% Other criteria have been used, for instance a number of approaches have considered the problem of computing a perfectly accurate tree of minimal size and/or depth~\cite{DBLP:conf/cp/BessiereHO09,narodytska2018learning}. This criterion is usually considered less useful because it is not robust to noisy data sets and is prone to overfitting. It is possible to adapt these approaches to accuracy maximization under a fixed depth~\cite{DBLP:conf/ijcai/Hu0HH20}, although the resulting method is not extremely efficient.
% Alternatively, Hu \textit{et al.} advocate a combination of accuracy and tree size~\cite{hu2019optimal}. We leave more complex criteria for future work as the main focus of this paper is to improve the state of the art for the most commonly used objective.
% %Alternatively, a number of approaches have considered the problem of computing a perfectly accurate tree of minimal size and/or depth
%
%
% %The idea of computing optimal decision tree classifiers is very old. The problem being
% %Ever since the pionneering work on decision tree classifiers~\cite{breiman1984classification,10.1023/A:1022643204877}, the question of computing \emph{optimal} decision trees has been alive, and recently a number of algorithms have been introduced for that purpose.
% Several variants and criteria have been proposed. %, e.g. computing the perfect classifier of minimum size~\cite{},
% Among those, computing a depth-bounded decision tree of maximum accuracy is often the preferred criterion~\cite{bertsimas2017optimal,hu2019optimal,dl8,verhaeghe2019learning}, because while being straightforward, it captures many desired features: it is resilient to noisy data, and shallow trees are both easier to explain and less prone to overfitting.
%
% \medskip
%
% Despite the vast offer of exact methods that can potentially provide optimal decision trees, or at least should, given enough time, improve on heuristics, the latter are still vastly more commonly used in practice. Some methods are memory-hungry, some are not anytime, and as far as we know there is not a single algorithm that can provide optimal classifiers while scaling to large data sets, feature space, or tree depth as heuristics do.
% % \begin{itemize}
% % 	\item MinDT -> does not scale, problem with noise
% % 	\item other SAT approaches -> do not scale in some way or another
% % 	\item DL8 -> does not scale in memory, slower
% % 	\item BinOCT -> ? (probably much slower)
% %   \item Murtree -> by far the most efficient, however, does not scale as well on deep trees and not as anytime
% % \end{itemize}
%
%
%
% \medskip
%
%
%
% %
% %
% % \begin{itemize}
% % 	\item Same worst-case complexity than DL8
% % 	\item No memory usage
% % 	\item Better anytime behaviour than DL8 (in fact as good as the state of the art heuristics)
% % 	\item Therefore, strictly better than greedy heuristics and almost always better than DL8
% % \end{itemize}
% %
% %
% % We consider the problem of finding the bounded-depth decision tree of maximum accuracy.
% % The state of the art includes MIP approaches (BinOCT), a MaxSAT approach based on the SAT encoding proposed by Narodytska et al, and DL8.5.
% % The latter algorithm is by far the most efficient, however, it is not \emph{anytime}: the left branch must be optimally solved before a solution of the right branch can be found. Moreover the use of a cache structure means that it uses a lot of memory. This algorithm is practical for a maximum depth of 4 (although using gigabytes of memory) but often not much beyond. Therefore, in a number of cases, a greedy heuristic (such as CART) is still the best method in practice.
% %
% % \medskip
%
% % In this note we introduce what is essentially an anytime version of DL8.5, without cache.
% % This algorithm therefore uses linear (in the size of the tree) memory and anytime, hence in principle strictly better than CART. Moreover, on instance where DL8.5 can find a solution, the algorithm described in this note is significantly faster (by about a factor 10).

\section{Preliminaries}

A data set on a binary feature set \features is a pair $\langle \negex,\posex \rangle$ where $\negex$ and $\posex$ are subsets of the feature space $\prod_{\afeat \in \features}\{\afeat,\bar{\afeat}\}$, and are standing, respectively, for negative and positive datapoints.
% $2^{\features}$.
% It is associated a label function $\classlabel : 2^{\features} \mapsto \{\posclass,\negclass\}$ such that:
% $$
% \forall \aclass \in \{\posclass,\negclass\}, \forall \ex \in \setex{\aclass}, \classlabel[\ex] = \aclass
% $$
%We denote $\allex$ the 
We denote the union of all datapoints by $\allex = \negex \cup \posex$ and $\bar{\features} = \{\bar{\afeat} \mid \afeat \in \features\}$ the set of negated features.
% %$\posclass$ and $\negclass$ are class labels and
% A datapoint $\ex$ can equivalently be seen as a subset of $\features$, or as the conjunction:
% $$
% \bigwedge_{\afeat \in \ex}\afeat \wedge \bigwedge_{\afeat \in \features \setminus \ex}\bar{\afeat}
% $$


A \emph{binary decision tree} is a tree whose 
%leaves are labelled with either $\posclass$ or $\negclass$, 
internal vertices are labelled with features and the two edges exiting a node labelled with $\afeat$ are respectively labelled with the feature $\afeat$ and its negation $\bar{\afeat}$.
To a \emph{branch} of a decision tree we associate the ordered set of labels on its edges, from root to leaf.

%$\abranch \in 2^{\features \cup \bar{\features}}$ of labels on its edges. Moreover, \abranch\ is naturally ordered from root to leaf.

%can therefore be identified to the ordered set of labels on the corresponding edges.
%To a \emph{branch} of a decision tree we can associate the set of labels on the corresponding edges.
%conjunction of features which are on the path from the root to a leaf. If the feature vertex is exited by a $1$-edge, the feature is positive in the conjunction, otherwise it is negative.

 
%Moreover, if we also consider data points as conjunctions of features (where every feature appears either positively or negatively),
%given a branch $\abranch \subseteq \features$ 
Given a data set $\langle \negex,\posex \rangle$, we can associate a data set $\langle \negex[\abranch],\posex[\abranch] \rangle$ to a branch $\abranch$ where $\negex[\abranch] = \{\ex \mid \ex \in \negex, \abranch \subseteq \ex\}$ and $\posex[\abranch] = \{\ex \mid \ex \in \posex, \abranch \subseteq \ex\}$.
% \begin{eqnarray*}
% \negex[\abranch] = \{\ex \mid \ex \in \negex, \abranch \subseteq \ex\}\\
% \posex[\abranch] = \{\ex \mid \ex \in \posex, \abranch \subseteq \ex\}
% \end{eqnarray*}
%
% For instance, the branch $\abranch = \{\afeat_i, \bar{\afeat_j}, \bar{\afeat_k}, \afeat_l\}$ has length 4 and
%
We write $\grow{\abranch}{\afeat}$ as a shortcut for $\abranch \cup \{\afeat\}$.
The classification error for branch $\abranch$ is $\error[\abranch]=\min(|\negex[\abranch]|, |\posex[\abranch]|)$.
% %Let $\error[\abranch]$ be $\min(|\negex[\abranch]|, |\posex[\abranch]|)$,
% and we write $\error[\abranch,\afeat]$ for $\error[\grow{\abranch}{\afeat}] + \error[\grow{\abranch}{\bar{\afeat}}]$.
% %A branch \abranch\ is said \emph{pure} iff $\error[\abranch]=0$.

% \medskip

Given a binary data set $\langle \negex,\posex \rangle$ on the features \features, %with label function $\classlabel$,
the \emph{minimum error bounded depth decision tree problem} consists in finding a binary decision tree with vertex labels in \features\ whose branches have cardinality at most $\mdepth$ and the sum of the classification error %($\error[\abranch]$) 
of its maximal branches is minimum.
%of depth at most $\maxd$ whose sum of error $\error[\abranch]$ for all branches $\abranch$ from the root to the leaves is equal to $\epsilon$.




\subsection{Dynamic Programming Algorithm}

The solver DL8.5 is a dynamic programming algorithm for the minimum error bounded depth decision tree problem. It relies on the observation that given a feature test, the two resulting branches are independent subproblems. Algorithm~\ref{alg:dynprog} gives a simplified view of DL8.5.


	% \begin{algorithm}
	% 	\caption{Dynamic Programming Algorithm\label{alg:dynprog}}
	% 	\TitleOfAlgo{\dynprog}
	% 	  \KwData{$\negex,\posex,\maxd,\abranch[=(\tru)]$}
	% 	  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
	% 		\lIf{$\maxd = 0$ or $\error[\abranch] = 0$} {
	% 		\Return $\error[\abranch]$
	% 		}
	% 		$\best \gets \error[\abranch]$\;
	% 		\ForEach{$\afeat \in \features \setminus \abranch$} {
	% 				$\best \gets \min(\best, \dynprog(\negex,\posex, \grow{\abranch}{\afeat}, \maxd-1) + \dynprog(\negex,\posex, \grow{\abranch}{\bar{\afeat}}, \maxd-1))$\;
	% 		}
	% 		\Return $\best$\;
	% \end{algorithm}
	
	\begin{algorithm}
		\begin{footnotesize}
		\caption{Dynamic Programming Algorithm\label{alg:dynprog}}
		\TitleOfAlgo{\dynprog}
		  \KwData{$\negex,\posex,\features,\maxd$}
		  \KwResult{The minimum error on $\negex,\posex,\features$ for decision trees of depth $\maxd$}
			$\error \gets \min(|\negex|,|\posex|)$\;
			
			\If{$\maxd > 0$ and $\error > 0$} {
			\ForEach{$\afeat \in \features$} {
					$\error \gets \min(\error, \dynprog(\negex(\{\afeat\}),\posex(\{\afeat\}),\features \setminus \{\afeat\},\maxd-1)$\;
					\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  $ + \dynprog(\negex(\{\bar{\afeat}\}),\posex(\{\bar{\afeat}\}),\features \setminus \{\afeat\},\maxd-1))$\;
			}
			}
			\Return $\error$\;
			\end{footnotesize}
	\end{algorithm}
	
	
	% \end{document}
	
	
	Let $\numex = |\posex| + |\negex|$, $\numfeat = |\features|$, and let $\mdepth$ be the maximum depth.
	We can safely assume $\mdepth \leq \numfeat$ (otherwise all features can be tested on every branch) and $\mdepth \leq \log \numex$ (otherwise we could have one distinct branch per datapoint), and it is often assumed that
	 $\mdepth \ll \numfeat$ and $\mdepth \ll \log \numex$. 
	 
	 
	 Algorithm~\ref{alg:dynprog} explores, in the worst case, $2^{\mdepth}$ branches for each of the $\Perm{\numfeat}{\mdepth}$ permutations of $\mdepth$ features in $\features$ for a total of $\Theta(\Perm{\numfeat}{\mdepth}2^{\mdepth})$ recursive calls.\footnote{More than half of the calls are at depth $\mdepth$ so counting leaves is sufficient.} 
	% 
	 Note that this is a significant improvement with respect to the $\Theta(\numfeat^{2^{\mdepth}})$ trees (with redundant branches) explored by a brute-force algorithm.
	 
	 Moreover, at each call, the data set must be split into two subsets. This takes time linear in the size of the data set. However,  %it can be amortized over the $2^{\mdepth-1}$ branches of depth $\mdepth$.
	 Consider
	 an ordered set of $\mdepth$ features and the $2^{\mdepth-1}$ branches testing these features in that order. Their corresponding data sets form a partition of the original data set. Therefore, the $\mdepth$-th split can done in $\Theta(\numex)$ time amortized over these $2^{\mdepth-1}$ branches.
	 It follows that the overall time complexity for splitting the data set is in $\Theta(\Perm{\numfeat}{\mdepth}\numex)$.\footnote{Again, more than half of the splits are at depth $\mdepth$.}
	 
	 
		 %
	 % for a ``level'' $l \in [1,\mdepth]$, the $2^l$ branches of length $l$ and testing the same feature in the same order (e.g., the branches $\{a,b\}$ Figure~\ref{fig:searchtree}).
	 % The data sets associated to these branches form a partition of the original data set. Therefore, the $l$-th split is done in $\Theta(\numex)$ time amortized over these $2^l$ branches.
	 % It follows that the overall time complexity for splitting the data set is in $\Theta(\Perm{\numfeat}{\mdepth}\numex)$.
	 
	 Algorithm~\ref{alg:dynprog} therefore runs in $\Theta((\numex+2^\mdepth){\Perm{\numfeat}{\mdepth}}) \subset O((\numex + 2^{\mdepth})\numfeat^\mdepth)$ time. Moreover, with the above assumptions on $\numex,\numfeat$ and $\mdepth$, $O(\numex\numfeat^\mdepth)$ is a good approximation of its worst-case time complexity.
	 
	 
	 % \medskip
	 
	 Notice that computing an optimal decision tree of bounded depth (for a polynomially computable definition of ``optimal'') is therefore polynomial unless the maximum depth is an input.
	 
	 
	 
	%  However, for a given level $l \in [1,\mdepth]$, this splitting procedure can be done in time $\Theta(\numex)$ amortised over the $2^l$ branches
	%
	%  %a given level of a particular decision tree,
	%  since the data sets associated to branches up to a given level form a partition of the original data set.
	%
	%
	%
	% %The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta(2^{\mdepth-1}{\numfeat \choose \mdepth})$, to explore at most ${\numfeat \choose \mdepth}$ combinations of features for at most $2^{\mdepth-1}$ branches.
	% %The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta(2^{\mdepth}{\numfeat \choose \mdepth})$, to explore $\Theta({\numfeat \choose \mdepth})$ combinations of features for $\Theta(2^{\mdepth})$ branches.
	% The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta(2^{\mdepth}\numfeat^{\mdepth})$.
	% Moreover, at each call, the data set must be split into two subsets. However, this splitting procedure can be done in time $\Theta(\numex)$ amortised over a given level of a particular decision tree, since the data sets associated to branches up to a given level form a partition of the original data set.
	% Therefore, since Algorithm~\ref{alg:dynprog} independently explores $\Theta({\numfeat \choose \mdepth})$ sets of branches of depth $\mdepth$, it runs in
	% $\Theta((\numex+2^\mdepth){\numfeat \choose \mdepth})$ time (hence $O(\numex\numfeat^\mdepth)$ time, since we suppose $\mdepth \ll \numfeat$ and $\mdepth \ll \log \numex$).
	%
	% Note that this is a significant improvement with respect to the $\Theta(\numfeat^{2^{\mdepth}})$ trees (with redundant branches) explored by a brute-force algorithm.
	%  %
	%  % a brute-force algorithm, since the total number of decision trees of depth $\mdepth$ with $\numfeat$ attributes is $\prod_{x=1}^{\mdepth}(1-x+\numfeat)^{2^{\mdepth}}$. If we assume $\mdepth \in O(1)$, this is $\Theta(\numfeat^{2^{\mdepth}})$ distinct trees.
	%  %




\section{An Anytime Algorithm}

Algorithm~\ref{alg:bud} shows the pseudo-code of an iterative, anytime, version of Algorithm~\ref{alg:dynprog} (highlighted code can be ignored for now). This algorithm
%In a nutshell, Algorithm~\ref{alg:bud} 
explores the same search space as Algorithm~\ref{alg:dynprog}: the same branch is never explored twice. However, incomplete branches are expanded before trying alternative features for already explored branches. In other words, instead of optimizing the left subtree before exploring the right subtree as in Algorithm~\ref{alg:dynprog}, Algorithm~\ref{alg:bud} first fully expands a decision tree before exploring alternatives for any branch, see Figure~\ref{fig:searchtree} for an illustration of the branch exploration order.

%For instance, consider a data set with three binary features $\features = \{a,b,c\}$. Figure~\ref{fig:searchtree} shows the branches explored by both Algorithm~\ref{dynprog} and Algorithm~\ref{alg:bud}. Both algorithms explore first the branches $\{a,b\}$ and $\{a,\bar{b}\}$. However, whereas Algorithm~\ref{dynprog} explores next the branches $\{a,c\}$ and $\{a,\bar{c}\}$, Algorithm~\ref{alg:bud} explores next the branches $\{\bar{a},b\}$ and $\{\bar{a},\bar{b}\}$, hence immediately 


%For $\mdepth=2$ Algorithm~\ref{dynprog} explores the branches 


	\begin{figure}
	\begin{center}
		\tabcolsep=0pt
		\scalebox{1}{
			\begin{forest}
				for tree={%
					l sep=20pt,
					s sep=3.5pt,
					node options={shape=rectangle, minimum width=10pt, inner sep=0pt, font=\footnotesize},
		  		edge={thick, -latex, shorten >=1pt, shorten <=1pt},
				}
				[{$\emptyset$}
					[{$a$}
						[{\begin{tabular}{c}$b$\\1\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\2\end{tabular}}]
						[{\begin{tabular}{c}$c$\\7\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\8\end{tabular}}]
					]
					[{$\bar{a}$}
						[{\begin{tabular}{c}$b$\\3\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\4\end{tabular}}]
						[{\begin{tabular}{c}$c$\\5\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\6\end{tabular}}]
					]
					[{$b$}
						[{\begin{tabular}{c}$a$\\9\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\10\end{tabular}}]
						[{\begin{tabular}{c}$c$\\15\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\16\end{tabular}}]
					]
					[{$\bar{b}$}
						[{\begin{tabular}{c}$a$\\11\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\12\end{tabular}}]
						[{\begin{tabular}{c}$c$\\13\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\14\end{tabular}}]
					]
					[{$c$}
						[{\begin{tabular}{c}$a$\\17\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\18\end{tabular}}]
						[{\begin{tabular}{c}$b$\\23\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\24\end{tabular}}]
					]
					[{$\bar{c}$}
						[{\begin{tabular}{c}$a$\\19\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\20\end{tabular}}]
						[{\begin{tabular}{c}$b$\\21\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\22\end{tabular}}]
					]
				]
			\end{forest}
		}
	\end{center}
	\caption{\label{fig:searchtree} The search tree for decision trees. \texttt{DynProg} explores it depth first, whereas \texttt{Blossom} explores the leaves in the order given below.}
	%\caption{\label{fig:searchtree} The search tree for decision trees. \dynprog explores it depth first, whereas \blossom explores branches in the order given below the leaves.}
	\end{figure}


% \medskip

Let $\afeat_i <_{\abranch} \afeat_j$ if and only if feature $\afeat_i$ is selected before feature $\afeat_j$ when expanding branch $\abranch$ at Line~\ref{line:assignment}. Algorithm~\ref{alg:bud} has the following invariants, from which a formal proof of correctness easily follows:

\begin{itemize}
	\item \sequence\ represents the current decision tree: if $(\abranch,\afeat) \in \sequence$, then the current tree tests feature $\afeat$ at the extremity of branch $\abranch$. We say that the branch $\abranch$ is in the current tree, and that feature $\afeat$ is tested on branch $\abranch$.
	
	\item If $(\abranch,\afeat) \in \sequence$, then every subtree of $\abranch$ starting with a feature test $\aofeat <_{\abranch} \afeat$ has already been explored and $\best[\abranch]$ contains the minimum of their errors. The set $\dom[\abranch]$ contains all \emph{untried} feature tests for branch $\abranch$ ($\dom[\abranch] = \{\aofeat \mid \aofeat \in \features ~\wedge~ \afeat <_{\abranch} \aofeat \}$).
	
	\item If $(\abranch,\afeat) \in \sequence$ but one of its children $\grow{\abranch}{\afeat}$ or $\grow{\abranch}{\afeat}$ (call it $\aobranch$) is not in the current tree, then:

	\begin{itemize}
		\item it is \emph{terminal} ($|\aobranch|=k$ or $\error[\aobranch]=0$), or
		\item it is a \emph{bud} yet to be expanded ($\aobranch \in \bud$), or
		\item it is \emph{optimal}: all possible feature tests have been tried for $\aobranch$.
	\end{itemize}
\end{itemize}


\begin{algorithm}[t]
\begin{footnotesize}
		\caption{Blossom Algorithm\label{alg:bud}}
		\TitleOfAlgo{\blossom}
		  \KwData{$\negex,\posex, \maxd$}
		  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth $\maxd$}
		$\sequence \gets []$\;
		% $\bud \gets \emptyset$\;
		$\bud \gets \newbud(\emptyset,\emptyset)$\;
		
		% $\bud \gets \{\emptyset\}$\;
		% $\dom[\emptyset] \gets \features$\;
		% $\best[\emptyset] \gets \min(\negex, \posex)$\;
		% \HiLi $\opt[\emptyset] \gets \texttt{false}$\;
		
		
		\While{$|\sequence| + |\bud| > 0$}{
		\lnl{line:dive}\If{$\bud \neq \emptyset$}{
			%$\abranch \gets \select{\bud}$\;
			\lnl{line:budchoice}pick and remove $\abranch$ from $\bud$\;
			
			% \lnl{line:leaves}\eIf{$|\abranch| = \maxd$ or $\error[\abranch] = 0$} {
			% 	% $\error \gets \error + \error[\abranch,\afeat]$\;
			% 	\lnl{line:best}$\best[\abranch] \gets \error[\abranch]$\;
			% }{
			\lnl{line:notterminal}\If{$|\abranch| < \maxd$ and $\best[\abranch] > 0$}{
			\lnl{line:assignment} pick and remove $\afeat$ from $\dom[\abranch]$\;
			% $\dom[\abranch] \gets \dom[\abranch] \setminus \{\afeat\}$\;
			push $(\abranch,\afeat)$ on $\sequence$\;
			% split $\negex[\abranch]$ and $\posex[\abranch]$ w.r.t. $\afeat$\;
			% \lnl{line:branching}\ForEach{$v \in \{\afeat, \bar{\afeat}\}$}{
			% 			\lnl{line:newbud}$\bud \gets \bud \cup \{\grow{\abranch}{v}\}$\;
			% 			\lnl{line:domain}$\dom(\grow{\abranch}{v}) \gets \features \setminus \{\afeat \mid \afeat \in \abranch ~\vee~ \bar{\afeat} \in \abranch\}$\;
			% 			$\best(\grow{\abranch}{v}) \gets \min(\negex[\grow{\abranch}{v}], \posex[\grow{\abranch}{v}])$\;
			% 			\HiLi $\opt(\grow{\abranch}{v}) \gets \texttt{false}$\;
			% 		}
			% }
			\lnl{line:branching} \lForEach{$v \in \{\afeat, \bar{\afeat}\}$}{
				$\bud \gets \newbud{\bud,\grow{\abranch}{v}}$ %\newbud{$\grow{\abranch}{v}$}
				}
			}
		}
		\lnl{line:else}\Else {
			% $\best[\emptyset] \gets \min(\best[\emptyset], \error)$\;
			\lnl{line:backtrack}\While{$|\sequence| > 0$}{
				\lnl{line:pop}pop $(\abranch,\afeat)$ from $\sequence$\;
				\lnl{line:storebest}$\best[\abranch] \gets \min(\best[\abranch], \best[\grow{\abranch}{\afeat}] +  \best[\grow{\abranch}{\bar{\afeat}}])$\;
				% $\error \gets \error - \best[\grow{\abranch}{\afeat}] -  \best[\grow{\abranch}{\bar{\afeat}}]$\;
				\lnl{line:optimal}\If{$\dom[\abranch] \neq \emptyset$ 
				\colorbox{yellow!50}{and $|\abranch|<\mdepth$}
				} {
					\lnl{line:fail} \HiLi \If{$\forall \abranch' \in \ancestors[\abranch], \lb{\abranch',\abranch} < \best[\abranch']$} {
					\lnl{line:right}$\bud \gets \bud \cup \{\abranch\}$\;
					\Break\;
					}
					% \lIf{$\opt[\abranch]$}{$\error \gets \error - \best[\abranch]$}
				} 
				% \HiLi \lnl{line:markoptimal} \lElse{
				% 	$\opt[\abranch] \gets \texttt{true}$
				% }
				% \lElse {
				% 	$\error \gets \error + \best[\abranch]$%$\error[\abranch,\afeat]$
				% }
				% {
				% 	$\opt[\abranch] \gets \tru$\;
				% }
			}
			
			
			
			% \lnl{line:backtrack}\Repeat{($\dom[\abranch] \neq \emptyset$ or $|\sequence|=0$) and $\opt[\abranch]=0$}{
			% 	pop $(\abranch,\afeat)$ from $\sequence$\;
			% 	\lnl{line:storebest}$\best[\abranch] \gets \min(\best[\abranch], \best[\grow{\abranch}{\afeat}] +  \best[\grow{\abranch}{\bar{\afeat}}])$\;
			% 	% $\error \gets \error - \best[\grow{\abranch}{\afeat}] -  \best[\grow{\abranch}{\bar{\afeat}}]$\;
			% 	\lIf{$\dom[\abranch] \neq \emptyset$} {
			% 		$\bud \gets \bud \cup \{\abranch\}$
			% 		% \lIf{$\opt[\abranch]$}{$\error \gets \error - \best[\abranch]$}
			% 	}
			% 	\lElse {
			% 		$\opt[\abranch] \gets 1$\;
			% 	}
			% 	% \lElse {
			% 	% 	$\error \gets \error + \best[\abranch]$%$\error[\abranch,\afeat]$
			% 	% }
			% 	% {
			% 	% 	$\opt[\abranch] \gets \tru$\;
			% 	% }
			% }
		}
		}
		\Return $\best[\emptyset]$\;
	  % \setcounter{AlgoLine}{0}
	   \SetKwProg{myproc}{Procedure}{}{}
	   \myproc{\newbud{$\bud, \abranch$}}{
		% \lnl{line:newbud}$\bud \gets \bud \cup \{\abranch\}$\;
		\lnl{line:splitting}compute $\negex[\abranch]$ and $\posex[\abranch]$ \colorbox{yellow!50}{and $p(\afeat,\negex[\abranch])$ and $p(\afeat,\posex[\abranch]), \forall \afeat \in \features$}\;
		\lnl{line:domain}$\dom(\abranch) \gets \features \setminus \{\afeat \mid \afeat \in \abranch ~\vee~ \bar{\afeat} \in \abranch\}$ \colorbox{yellow!50}{sorted by increasing Gini score}\;
		$\best(\abranch) \gets \min(\negex[\abranch], \posex[\abranch])$\;
		% \colorbox{yellow!50}{$\opt(\abranch) \gets \texttt{false}$}\;
		\Return{$\bud \cup \{\abranch\}$}\;
		}
	\end{footnotesize}
	\end{algorithm}


As long as there is a bud ($\bud \neq \emptyset$), we pick any one $\abranch \in \bud$ at Line~\ref{line:budchoice} and check if it can or need to be expanded in Line~\ref{line:notterminal}. %If its length is $\mdepth$ the error at this leaf is recorded in $\best[\abranch]$. 
If so, we pick a feature $\afeat$ marked as \emph{untried} for \abranch, unmark it, 
expand the tree with the test $\afeat$ at branch $\abranch$. The two children $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ can then be added to $\bud$.
%add the pair $(\abranch,\afeat)$ to \sequence\ and expand the tree with the two branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$. 

If there is no bud ($\bud = \emptyset$), then the current tree is complete: every branch $\abranch$ is either terminal or optimal. In that case we pop the last assignment $(\abranch,\afeat)$ from \sequence\ 
%, mark the feature $\afeat$ as tried for branch $\abranch$ 
and update the best error of its subtrees. If there is at least one untried feature for branch $\abranch$, we add $\abranch$ to $\bud$.
Otherwise, it is optimal since all features have been tried, and $\best[\abranch]$ contains the minimum error for any subtree of branch $\abranch$. 
%and its error is the sum of the errors of its best subtrees. 
This branch will never be expanded anymore since it is not added to $\bud$.
%
When the algorithm ends, $\best[\emptyset]$ contains the minimum error of any decision tree of depth $\mdepth$. % on the data set.


% Algorithm~\ref{alg:bud} starts from a singleton set \bud\ of open branches or \emph{buds},
% % (open branches are branches of length strictly less than $\maxd$ that are not pure). % The set \nodes\ contains all the nodes of the current tree, open or closed, it is initially equal to \bud. Finally,
% and an initially empty stack of decisions $\sequence$.
%
% \begin{itemize}
% 	\item As long as there is a bud ($\bud \neq \emptyset$), we pick any one $\abranch \in \bud$ and check if it can be expanded in Line~\ref{line:leaves}. If its length is $\mdepth$ the error at this leaf is recorded in $\best[\abranch]$. Otherwise, we pick a feature $\afeat$ marked as \emph{untried} for \abranch, add the pair $(\abranch,\afeat)$ to \sequence\ and expand the tree with the two branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$.
% 	%
% 	%
% 	%
% 	%
% 	%  a feature $\afeat$ marked as \emph{available} for \abranch, add the pair $(\abranch,\afeat)$ to \sequence\ and expand the tree with the two branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$.
% 	% %These new nodes are added to $\nodes$.
% 	% They are added to $\bud$ if their depth is strictly less than $\maxd-1$ (the last feature test is chosen according to minimum error) and if they are not pure, otherwise they are terminal tests and we record the corresponding error.
%
% \item If there is no bud ($\bud = \emptyset$), then the tree is complete: every branch $\abranch$ is such that $|\abranch| = \mdepth$ or $\error[\abranch]=0$. In that case we pop the last assignment $(\abranch,\afeat)$ from \sequence, mark the feature $\afeat$ as tried for branch $\abranch$ and update the recorded best error of its subtrees. If there is at least one untried feature for branch $\abranch$, we add $\abranch$ to $\bud$.
% Otherwise, we consider it \emph{terminal} and its error is the sum of the error of its best subtrees. This branch will never be expanded anymore since it is not added to $\bud$.
% % and we
% %store the minimum error recorded for any of the possible features.
%
% \end{itemize}


% \medskip

To simplify the pseudo-code, we use branches to index array-like data structures in Algorithm~\ref{alg:bud} (e.g. $\dom[\abranch]$). Actually, a set of \emph{indices} (at most $2^{\mdepth}$ in the worst case) are used as proxy for branches in all contexts, since the current tree cannot have more than $2^{\mdepth}$ branches. At Line~\ref{line:storebest}, the indices for $\grow{\abranch}{\afeat}$, $\grow{\abranch}{\bar{\afeat}}$ are released, and a free index is marked as used when expanding a branch at Line~\ref{line:branching}. Moreover, the pseudo-code in Algorithm~\ref{alg:bud} does not show how the best subtrees of optimal branches are recorded, nor how the overall best error is updated when completing a new decision tree at Line~\ref{line:else}.
%The worst case space complexity of the algorithm is therefore in $\Theta(2^{\mdepth}\numfeat)$. Under the standard assumption that $2^{\mdepth} \leq \numex$, this is less than the size of the input.

% where $\sizetree \leq 2^{\mdepth}$ is the maximum size of the explored tree, that is the maximum length of $\sequence$.


%The classification error of a tree is equal to the sum of the error of its terminal banches, the algorithm returns the minimum value encountered when exploring the possible decision trees.
%In other words, this algorithm will first build a complete tree by expanding non-pure, non-maximal branches in any order. When all leaves are pure of a maximal depth, the last test of the last expanded branch w



% \medskip





		

		\begin{theorem}
			The worst case time complexity of Algorithm~\ref{alg:bud} is $\Theta((\numex+2^\mdepth){\Perm{\numfeat}{\mdepth}}) \subset O((\numex + 2^{\mdepth})\numfeat^\mdepth)$ and its worst case space complexity is in $\Theta(2^{\mdepth}\numfeat)$.
			\end{theorem}
			
			
			\begin{proof}
				From the invariants, we can see that Algorithm~\ref{alg:bud} explores the same set of $\Perm{\numfeat}{\mdepth}2^\mdepth$ branches (i.e., the $2^{\mdepth}$ outcomes of each permutation of ${\mdepth}$ features).
				%
				Moreover, the ``yes'' branch of Condition~\ref{line:dive} dominates the time complexity since at most one element is added to $\sequence$, whereas  Loop~\ref{line:backtrack} suppresses exactly one element of $\sequence$ at every iteration (and each of its iterations is in constant time).
				
				The time complexity is therefore dominated by the splitting procedure whereby $\negex[\grow{\abranch}{\afeat}]$, $\negex[\grow{\abranch}{\bar{\afeat}}]$, $\posex[\grow{\abranch}{\afeat}]$ and $\posex[\grow{\abranch}{\bar{\afeat}}]$ are computed from $\negex[\abranch]$ and $\posex[\abranch]$. As discussed earlier, this takes linear time amortized over the $2^{\mdepth}$ branches sharing the same set of $\mdepth$ features. Therefore, the overall time complexity for the splitting operations is in $\Theta(\Perm{\numfeat}{\mdepth}\numex)$.
				
				
				Since branches can be stored in constant space (an index, the parent branch and the two children),
				the worst case space complexity $\Theta(2^{\mdepth}\numfeat)$ to record which feature have been tried (the sets $\dom$).
				\end{proof}
				
				
				%
				%
				% We say that a branch is \emph{explored} if it is picked and removed from \bud\ at Line~\ref{line:budchoice}, or, equivalently, if it is added \bud\, because since Loop~\ref{line:backtrack} terminates, every added branch will eventually be picked.
				%
				%
				% % We show that every terminal branch is explored exactly once, by recursion of the maximum depth $\mdepth$.
				% % For $\mdepth=0$, the unique branch is $\emptyset$, it is explored and the algorithm returns immediatly.
				% %
				% % Now suppose that for $\mdepth=d$ every terminal branch is explored exactly once. Now, let $\mdepth=d+1$ and consider a terminal branch $\grow{\abranch}{v}$.
				%
				% A branch added at Line~\ref{line:right} cannot be terminal, since the pair $(\abranch,\afeat)$ has been popped out of \sequence, witnessing the branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$.
				%
				% Therefore, if a branch $\grow{\abranch}{v}$ is explored more than once, it must be added twice at Line~\ref{line:newbud}. However, $\dom[\abranch]$ forbids that unless $\branch$ was itself added twice at Line~\ref{line:newbud} because $\dom[\abranch]$ is only reset at Line~\ref{line:domain}. This argument can be repeated for the ancestors of $\abranch$ until reaching $\emptyset$ which is explored only once.
				%
				% Now we need to show that every terminal branch is explored at least once.
				% We show that by recursion of the maximum depth $\mdepth$.
				% For $\mdepth=0$, the unique branch is $\emptyset$, and it is explored.
				%
				% Now suppose that for $\mdepth=d$ every terminal branch is explored, let $\mdepth=d+1$ and consider a terminal branch $\grow{\abranch}{v}$. Since increasing $k$ can only increase the number of explored branches, by the recursion hypothesis, $\abranch$ is explored. Moreover, since $|\abranch|<\mdepth$, a feature $\afeat$ will be selected and $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ added to $\bud$. Therefore, if $v=\afeat$ or $v=\bar{afeat}$, then the claim holds.
				% Otherwise, the pair $(\abranch,\afeat)$ is added to $\sequence$ and it will eventually [TODO, NEED AN ARGUMENT HERE?] be popped out at Line~\ref{line:pop}.
				% Then, $\afeat$ will be removed from $\dom[\abranch]$ and $\abranch$ reinserted into $\bud$.
				%
				%
				%
				%
				% We first show that every terminal branch is explored exactly once, by recursion of the maximum depth $\mdepth$.
				% For $\mdepth=0$, the unique branch is $\emptyset$, it is explored and the algorithm returns immediatly.
				%
				% Now suppose that for $\mdepth=d$ every terminal branch is explored exactly once. Now, let $\mdepth=d+1$ and consider a terminal branch $\grow{\abranch}{v}$.
				%
				%
				%
				% We first show that a terminal branch is explored
				%
				%
				%
				%
				%
				% The proof follows from the fact that Algorithm~\ref{alg:bud} explores exactly once every permutation of (negated) features of size $\mdepth$.
				%
				% Therefore, any branch added to $\bud$ will eventually be picked at Line~\ref{line:budchoice}.
				% We say that a branch is explored if it is added (and therefore picked from) $\bud$.
				%
				% We prove the claim by recursion on $\mdepth$.
				% For $\mdepth = 0$, the empty branch $\emptyset$ is explored and the algorithm terminates by returning $\error[\emptyset] = \min(|\negex|, |\posex|)$.
				%
				% Now suppose that, for $\mdepth \leq d$, every branch of length $\mdepth$ is explored exactly once, and consider the case $\mdepth=d+1$.
				% Let $\grow{\abranch}{v}$ be a branch of length $d+1$. By the recursion hypothesis, and since the value of $\mdepth$ is only tested at Line~\ref{line:leaves}, $\abranch$ is also explored when $\mdepth=d+1$.
				% Moreover, since $|\abranch|<\mdepth$, a feature $\afeat$ will be selected and $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ added to $\bud$. Therefore, if $v=\afeat$ or $v=\bar{afeat}$, then the claim holds.
				% Otherwise, the pair $(\abranch,\afeat)$ is added to $\sequence$ and it will eventually [TODO, NEED AN ARGUMENT HERE?] be popped out at Line~\ref{line:pop}.
				% Then, $\afeat$ will be removed from $\dom[\abranch]$ and $\abranch$ reinserted into $\bud$.
				%
				% Moreover, the algorithm will not stop until $\sequence$ is not empty, hence $(\abranch, \afeat)$ will eventually be popped out of $\sequence$, $\afeat$ removed from $\dom[\abranch]$ and $\abranch$ reinserted into $\bud$. Therefore, for every $\afeat \in \features$, such that neither $\afeat \in \abranch$ nor $\bar{\afeat}\in \abranch$,
				%  $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ will eventually be explored exactly once.
				%
				%
				% Let $\abranch$ be a branch of length $d$.
				% By the recursion hypothesis, and since the value of $\mdepth$ is only tested at Line~\ref{line:leaves}, $\abranch$ is also explored when $\mdepth=d+1$. We ignore the case where $\error[\abranch]=0$, since no extension of $\abranch$ need be explored.
				%
				% Moreover, since $|\abranch|<\mdepth$, a feature $\afeat$ will be selected and $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ added to $\bud$, and the pair $(\abranch,\afeat)$ added to $\sequence$.
				%
				% \medskip
				%
				%
				%  both branches $\grow{\abranch}{\afeat}$
				%
				%
				% Without loss of generality, let $\abranch$ of length $d$ and show that the same extentions of $\abranch$ are explored by both algorithms.
				%
				%
				% both algorithms explore exactly the same set of branches, albeit not in the same order:
				% they explore every permutation of (negated) feature of size $\mdepth$.
				%
				% We show that this is true for Algorithm~\ref{alg:bud} by recursion on the maximum depth $\mdepth$.
				% For $\mdepth = 0$, both algorithm return $\error[\emptyset]=\min(|\negex|, |\posex|)$.
				%
				% Now suppose that, $\mdepth \leq d$, \dynprog and \blossom explore exactly the same set of branches: a recursive call of \dynprog ends on the branch $\abranch$ if and only if $\best[\abranch]$ is set in Line~\ref{line:best} of \blossom.
				% %every branch $\abranch$ explored by \dynprog (a recursive call ends on this branch) is also explored by \blossom ($\best[\abranch]$ is set in Line~\ref{line:best}), and
				% Now, let $\mdepth=d+1$. Without loss of generality, we can take an arbitrary branch $\abranch$ of length $d$ and show that the same extentions of $\abranch$ are explored by both algorithms.
				% If $\error[\abranch]=0$, then no extension of $\abranch$ is explored by either algorithm.
				% Otherwise, if $\afeat \not\in \abranch$ and $\bar{\afeat}\not\in \abranch$, then
				% \dynprog explores the branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$.
				% %every branch $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ for $\afeat \in \features \setminus \abranch$.
				% Since $\abranch$ is explored when $\mdepth = d$ and since $\error[\abranch] \neq 0$, then $\abranch$ will fail the test on Line~\ref{line:leaves} and a feature $\afeat$ such that $\afeat \not\in \abranch$ and $\bar{\afeat}\not\in \abranch$ will be selected,
				% %for a feature $\afeat \in \features \setminus \abranch$,
				% the branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ will be explored, and the pair $(\abranch, \afeat)$ will be added to $\sequence$.
				% Moreover, the algorithm will not stop until $\sequence$ is not empty, hence $(\abranch, \afeat)$ will eventually be popped out of $\sequence$, $\afeat$ removed from $\dom[\abranch]$ and $\abranch$ reinserted into $\bud$. Therefore, for every $\afeat \in \features$, such that neither $\afeat \in \abranch$ nor $\bar{\afeat}\in \abranch$,
				%  $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ will eventually be explored exactly once.
			% 	\hfill$\square$
			% \end{proof}
			
			% \begin{proof}[sketch]
			% 	The proof follows from the fact that both algorithms explore exactly the same set of branches, albeit not in the same order:
			% 	they explore every permutation of (negated) feature of size $\mdepth$.
			%
			% 	We show that this is true for Algorithm~\ref{alg:bud} by recursion on the maximum depth $\mdepth$.
			% 	For $\mdepth = 0$, both algorithm return $\error[\emptyset]=\min(|\negex|, |\posex|)$.
			%
			% 	Now suppose that, $\mdepth \leq d$, \dynprog and \blossom explore exactly the same set of branches: a recursive call of \dynprog ends on the branch $\abranch$ if and only if $\best[\abranch]$ is set in Line~\ref{line:best} of \blossom.
			% 	%every branch $\abranch$ explored by \dynprog (a recursive call ends on this branch) is also explored by \blossom ($\best[\abranch]$ is set in Line~\ref{line:best}), and
			% 	Now, let $\mdepth=d+1$. Without loss of generality, we can take an arbitrary branch $\abranch$ of length $d$ and show that the same extentions of $\abranch$ are explored by both algorithms.
			% 	If $\error[\abranch]=0$, then no extension of $\abranch$ is explored by either algorithm.
			% 	Otherwise, if $\afeat \not\in \abranch$ and $\bar{\afeat}\not\in \abranch$, then
			% 	\dynprog explores the branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$.
			% 	%every branch $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ for $\afeat \in \features \setminus \abranch$.
			% 	Since $\abranch$ is explored when $\mdepth = d$ and since $\error[\abranch] \neq 0$, then $\abranch$ will fail the test on Line~\ref{line:leaves} and a feature $\afeat$ such that $\afeat \not\in \abranch$ and $\bar{\afeat}\not\in \abranch$ will be selected,
			% 	%for a feature $\afeat \in \features \setminus \abranch$,
			% 	the branches $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ will be explored, and the pair $(\abranch, \afeat)$ will be added to $\sequence$.
			% 	Moreover, the algorithm will not stop until $\sequence$ is not empty, hence $(\abranch, \afeat)$ will eventually be popped out of $\sequence$, $\afeat$ removed from $\dom[\abranch]$ and $\abranch$ reinserted into $\bud$. Therefore, for every $\afeat \in \features$, such that neither $\afeat \in \abranch$ nor $\bar{\afeat}\in \abranch$,
			% 	 $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$ will eventually be explored exactly once.
			% 	\hfill$\square$
			% \end{proof}
			
			\medskip
			
			The key difference between Algorithms~\ref{alg:dynprog} and \ref{alg:bud} is the order in which branches are explored (see Figure~\ref{fig:searchtree}). In particular, \dynprog must complete the first recursive call before outputing a full tree. 
			%Therefore, the computation time for finding a first complete tree is $\Theta((\numex+2^{\mdepth})\Perm{\numfeat-1}{\mdepth-1})$, that is $O(\numex(\numfeat-1)^{\mdepth-1})$ time.
			Therefore, it finds a first complete tree in $\Theta((\numex+2^{\mdepth})\Perm{\numfeat-1}{\mdepth-1})$, that is $O(\numex(\numfeat-1)^{\mdepth-1})$ time. 	
			On the other hand, \blossom finds a first tree in linear time: $\Theta(2^{\mdepth}+\numex\mdepth) = \Theta(\numex\mdepth)$.
			Another difference with actual implementations of Algorithm~\ref{alg:dynprog} (\olddleight\ and \dleight) is that the latter methods use a cache structure in order to reduce the number of branches that need to be explored. 
			%Indeed, by using memory, 
%there is no need to explore every permutation			
			% % it is sufficient to explore every \emph{combination}
			% % %\footnote{Actually, some combination may be completely avoided using bounds reasoning and subset lookup.}
			% % (instead of every permutation)
			% of $\mdepth$ features.
			% For one thing,
			%  % since
			%  the order of the tests does not matter, given a single branch and using bounds reasoning and subset lookup the set of branches to explore can even be reduced further.
			 %
			 Our experimental evaluations, however, show that the overhead of cache lookups may not always be beneficial. Moreover, the space complexity of managing the cache may be prohibitive. On the other hand, Algorithm~\ref{alg:bud} is essentially memoryless, since, under the standard assumption that $2^{\mdepth} \leq \numex$, its worst-case space complexity is less than the size of the input.
			
			
		
			
			
			
% 			% First, notice that the task of splitting the data set on eevry branch of the search tree can be done exactly as in DL8, that is, in $\Theta(\numex)$ amortised time for each of the $\mdepth$ tree levels.
%
% 			% \begin{proof}
%
% 			First, all branches eventually reached. Consider an arbitrary branch $\abranch$.
%
%
% 			\medskip
%
%
% 			$\best[\abranch]$ is the minimal error of any subtree rooted at $\abranch$ with a feature in $\features \setminus \dom[\abranch]$ is $\best[\abranch]$.
% 			This is true for pure branches
%
%
% 			 and maximum-depth branches (code after condition in Line~\ref{line:leaves}). Now consider a branch $\abranch$ such that $|\abranch|<\mdepth$ and $\forall \afeat \in \features, \error[\abranch,\afeat] > 0$.
%
% 			\medskip
%
%
%
%
% 			Let a branch $\abranch$ be \emph{explored} iff it was put in the stack $\sequence$ and $\dom[\abranch] = \emptyset$.
% 			If a branch $\abranch$ is explored, then $\best[\abranch]$ is the minimal error of any subtree rooted at $\abranch$.
% 			This is true for pure branches and maximum-depth branches (code after condition in Line~\ref{line:leaves}). Now consider a branch $\abranch$ such that $|\abranch|<\mdepth$ and $\forall \afeat \in \features, \error[\abranch,\afeat] > 0$.
%
% 			\medskip
%
%
%
% 				First, we show that the algorithm is correct. In particular the following property holds:
% 				after Line~\ref{line:storebest}, the minimum error of any subtree rooted at $\abranch$ with a feature in $\features \setminus \dom[\abranch]$ is $\best[\abranch]$.
%
%
% 				Observe that $|\abranch| \leq \mdepth-2$. Indeed, no pair $(\abranch,\afeat)$ is put on $\sequence$ unless $|\abranch| \leq \mdepth-2$. Now suppose that $|\abranch|=\mdepth-2$. Then $\best[\grow{\abranch}{\afeat}]$ is by definition the minimal error of any single-node tree rooted at $\grow{\abranch}{\afeat}$ and likewise, $\best[\grow{\abranch}{\bar{\afeat}}]$ is the minimal error of any single-node tree rooted at $\grow{\abranch}{\bar{\afeat}}$. Therefore, $\best[\grow{\abranch}{\afeat}] + \best[\grow{\abranch}{\bar{\afeat}}]$ is the minimal error of a depth 2 tree rooted at $\abranch$ with a test on $\afeat$. Since all features in $\features \setminus \dom[\abranch]$ have been tried (or belong to $\abranch$) and the minimum was kept, the property holds.
%
% 				Suppose now that the property holds for $|\abranch|=\mdepth-x$ with $x>2$. Then by the same reasoning as above, the property will also hold for $|\abranch|=\mdepth-x-1$. Therefore is always hold.
%
%
%
% 			\medskip
%
%
%
%
%
%
% 			The key is to observe that given a branch $\abranch$ and a feature $\afeat$, just as in DL8, the complexity of computing
% 			$\error[\abranch,\afeat]$ is equal to the complexity of computing $\error[\grow{\abranch}{\afeat}]$ plus the complexity of computing $\error[\grow{\abranch}{\bar{\afeat}}]$.
% 			Indeed, wlog, let the branch $\grow{\abranch}{\afeat}$ be chosen before $\grow{\abranch}{\bar{\afeat}}$.
% 				Both branches will be completed up to the maximal depth, however,
% 			the test appended to $\grow{\abranch}{\afeat}$ will no change until
%
%
%
%
% 			the best possible errors for $\grow{\abranch}{\afeat}$ and $\grow{\abranch}{\bar{\afeat}}$
%
%
% 			% \end{proof}
%
% 			We first show that the number of assignment of tests (Line~\ref{line:assignment}) is $2^{\mdepth-1}\numfeat^{\mdepth-1}$.
% 			For $\mdepth=1$ this is true since there is a single assigned test (with the feature $\argmin_{\afeat \in \features}(\error[\emptyset,\afeat])$).
%
% 			Now suppose that the property holds for depth $\mdepth-1$ and consider depth $\mdepth$.
%
%
%
%
% 			No proof is given, but independent subtrees are not explored in Algorithm~\ref{alg:bud} hence both algorithms do the same computation, except not in the same order. Notice that Algorithm~\ref{alg:bud} eagerly compute the conditional error $\error[\abranch,\afeat]$ for every feature $\afeat \in \features$, which incurs an extra factor $\numfeat$ for the data set partitionning task. However, in return the last test of each branch is chosen in $O(1)$ so we gain the same factor $\numfeat$.
%
% 			The big difference is that whereas the cost of finding a first solution is $\Theta((\numex + 2^{\mdepth-1})\numfeat^{\mdepth-1})$ for Algorithm~\ref{alg:dynprog}, it is equal to $\Theta(\mdepth\numex + 2^\mdepth)$ for Algorithm~\ref{alg:bud}, which in practice is very important, as shown in the experimental section.
%
% 	%T(m,k) = 2mT(m-1, k-1)
%
%
%
%
% % \clearpage






	
	
	% \begin{algorithm}
	% 	\caption{Anytime Algorithm\label{alg:bud}}
	% 	\TitleOfAlgo{\blossom}
	% 	  \KwData{$\negex,\posex, \maxd$}
	% 	  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
	% 	$\sequence \gets []$\;
	% 	$\bud \gets \{\emptyset\}$\;
	% 	$\error \gets \min(|\negex|,|\posex|)$\;
	% 	$\dom \gets (\lambda : {2^{\features}} \mapsto \features)$\;
	% 	$\best \gets (\lambda : {2^{\features}} \mapsto \infty)$\;
	% 	% $\opt \gets (\lambda : {2^{\features}} \mapsto \fal)$\;
	%
	% 	\While{$|\sequence| + |\bud| > 0$}{
	% 	\eIf{$\bud \neq \emptyset$}{
	% 		%$\abranch \gets \select{\bud}$\;
	% 		pick and remove $\abranch$ from $\bud$\;
	% 		\lnl{line:assignment}$\afeat \gets \argmin_{\afeat \in \dom[\abranch]}(\error[\abranch,\afeat])$\;
	% 		\lnl{line:leaves}\eIf{$\error[\abranch,\afeat] = 0$ or $|\abranch| = \maxd-1$} {
	% 			$\error \gets \error + \error[\abranch,\afeat]$\;
	% 			$\best[\abranch] \gets \error[\abranch,\afeat]$\;
	% 			$\dom[\abranch] \gets \emptyset$\;
	% 			% $\opt[\abranch] \gets \tru$\;
	% 		}{
	% 		$\dom[\abranch] \gets \dom[\abranch] \setminus \{\afeat\}$\;
	% 		push $(\abranch,\afeat)$ on $\sequence$\; % $ \gets \sequence \oplus (\abranch,\afeat)$\;
	% 		\ForEach{$v \in \{\afeat, \bar{\afeat}\}$}{
	% 				\lIf{$\error[\abranch,v] > 0$}{
	% 					$\bud \gets \bud \cup \{\abranch \wedge v\}$
	% 				}
	% 		}
	% 		}
	% 	}{
	% 		$\best[\emptyset] \gets \min(\best[\emptyset], \error)$\;
	% 		\Repeat{$\dom[\abranch] \neq \emptyset$ or $|\sequence|=0$}{
	% 			pop $(\abranch,\afeat)$ from $\sequence$\;
	% 			\lnl{line:storebest}$\best[\abranch] \gets \min(\best[\abranch], \best[\grow{\abranch}{\afeat}] +  \best[\grow{\abranch}{\bar{\afeat}}])$\;
	% 			$\error \gets \error - \best[\grow{\abranch}{\afeat}] -  \best[\grow{\abranch}{\bar{\afeat}}]$\;
	% 			\lIf{$\dom[\abranch] \neq \emptyset$} {
	% 				$\bud \gets \bud \cup \{\abranch\}$
	% 				% \lIf{$\opt[\abranch]$}{$\error \gets \error - \best[\abranch]$}
	% 			}
	% 			\lElse {
	% 				$\error \gets \error + \best[\abranch]$%$\error[\abranch,\afeat]$
	% 			}
	% 			% {
	% 			% 	$\opt[\abranch] \gets \tru$\;
	% 			% }
	% 		}
	% 	}
	% 	}
	% 	\Return $\best[\emptyset]$\;
	% \end{algorithm}
	
	
	

	
	
	
	
	% \begin{figure}
	% \begin{center}
	% 	\tabcolsep=0pt
	% 	\scalebox{1}{
	% 		\begin{forest}
	% 			for tree={%
	% 				l sep=20pt,
	% 				s sep=3.5pt,
	% 				node options={shape=rectangle, minimum width=10pt, inner sep=0pt, font=\footnotesize},
	% 	  		edge={thick, -latex, shorten >=1pt, shorten <=1pt},
	% 			}
	% 			[{$\emptyset$}
	% 			 [{$a,\bar{a}$}
	% 			  [{$a \wedge b$,$a \wedge \bar{b}$}
	% 					[{$a \wedge b \wedge c$,$a \wedge b \wedge \bar{c}$}]
	% 					[{$a \wedge \bar{b} \wedge c$,$a \wedge \bar{b} \wedge \bar{c}$}]
	% 				]
	% 			  [{$\bar{a} \wedge b$,$\bar{a} \wedge \bar{b}$}]
	% 			 ]
	% 			 [{$b$}]
	% 			 [{$c$}]
	% 			]
	% 		\end{forest}
	% 	}
	% \end{center}
	% \caption{\label{fig:searchtree} The search tree for decision trees. \texttt{DynProg} explores it depth first, whereas \texttt{Bud-first-search} explores branches in the order given below the leaves.}
	% %\caption{\label{fig:searchtree} The search tree for decision trees. \dynprog explores it depth first, whereas \blossom explores branches in the order given below the leaves.}
	% \end{figure}
	
	
	% \begin{tabular}{c|ll}
	% 	\# & \bud & \sequence \\
	% 	\hline
	% 	1 & $\{\emptyset\}$ & $[]$ \\
	% 	2 & $\{a,\bar{a}\}$ & $[(\emptyset,a)]$ \\
	% 	3 & $\{a \wedge b,a \wedge \bar{b},\bar{a}\}$ & $[(\emptyset,a),(a,b)]$ \\
	% 	4 & $\{a \wedge \bar{b},\bar{a}\}$ & $[(\emptyset,a),(a,b)]$ \\
	% 	5 & $\{\bar{a}\}$ & $[(\emptyset,a),(a,b)]$ \\
	% 	6 & $\{\bar{a} \wedge b, \bar{a} \wedge \bar{b}\}$ & $[(\emptyset,a),(a,b),(\bar{a},b)]$ \\
	% 	7 & $\{\bar{a} \wedge \bar{b}\}$ & $[(\emptyset,a),(a,b),(\bar{a},b)]$ \\
	% 	8 & $\{\}$ & $[(\emptyset,a),(a,b),(\bar{a},b)]$ \\
	% 	9 & $\{\bar{a} \wedge c, \bar{a} \wedge \bar{c}\}$ & $[(\emptyset,a),(a,b),(\bar{a},c)]$ \\
	% 	10 & $\{\bar{a} \wedge \bar{c}\}$ & $[(\emptyset,a),(a,b),(\bar{a},c)]$ \\
	% 	11 & $\{\}$ & $[(\emptyset,a),(a,b),(\bar{a},c)]$ \\
	% 	12 & $\{a \wedge c, a \wedge \bar{c}\}$ & $[(\emptyset,a),(a,c)]$ \\
	% 	13 & $\{a \wedge \bar{c}\}$ & $[(\emptyset,a),(a,c)]$ \\
	% 	14 & $\{\}$ & $[(\emptyset,a),(a,c)]$ \\
	% \end{tabular}


% For readability, we cut the algorithm into four blocks. The initialisation procedure (Algorithm~\ref{alg:init}) set up the data structures used in all other procedures:
% \begin{itemize}
% 	\item \sequence\ is simply the list of nodes in the current tree, ordered as they are explored.
% 	\item \nodes\ is the set of integers used to index a node of the current tree
% 	\item \bud\ is the set of nodes which do no have an assigned test yet
% 	\item \mdepth\ stores the depth of a node
% 	\item \test\ stores the feature tested at a node
% 	\item \dom\ stores the set of possible features which have no yet been tried for this node
% 	\item \best\ stores the error of the best subtree rooted at a node
% 	\item \opt\ indicates whether the best subtree of a given node is optimal
% 	\item \child\ stores the children of a node (children can be nodes or $\{\posclass, \negclass\}$)
% 	\item $\error{\anode}$ $\min(|\posex(\anode)|,|\negex(\anode)|)$
% 	\item $\error{\anode,\afeat}$ $\min(|\posex(\anode=\afeat)|,|\negex(\anode=\afeat)|)$
% \end{itemize}
%
% Algorithm~\ref{alg:search} is a bactracking procedure which expands a current decision tree
%
% 	\begin{algorithm}
% 		\caption{Data Structures\label{alg:init}}
% 		\TitleOfAlgo{Initialise}
% 		$\sequence \gets []$\;
% 		$\bud \gets \emptyset$\;
% 		$\nodes \gets \emptyset$\;
% 		$\ub \gets \min(|\negex|,|\posex|)$\;
% 		$\error \gets ub$\;
%
% 		$\child \gets (\lambda : \mathbb{N} \times \{\fal, \tru\} \mapsto \emptyset)$\;
% 		$\mdepth \gets (\lambda : \mathbb{N} \mapsto 0)$\;
%
% 		$\test \gets (\lambda : \mathbb{N} \mapsto \emptyset)$\;
% 		$\dom \gets (\lambda : \mathbb{N} \mapsto \features)$\;
%
% 		$\best \gets (\lambda : \mathbb{N} \mapsto \infty)$\;
% 		$\opt \gets (\lambda : \mathbb{N} \mapsto \fal)$\;
% 	\end{algorithm}
%
%
%
% 	\begin{algorithm}
% 		\caption{Create a new node after branching\label{alg:alloc}}
% 		\TitleOfAlgo{\grow}
% 	  \KwData{integer \anode}
%
% 		$\nodes \gets \nodes \cup \{\anode\}$\;
% 		$\dom[\anode] \gets \features$ sorted by decreasing conditional error $\min(|\posex(\anode=\afeat)|,|\negex(\anode=\afeat)|)$\;
% 		$\test[\anode] \gets \pop(\dom[\anode])$\;
%
%
% 		\eIf{$\mdepth[\anode]=k-1$ or $\error{\anode,\test[\anode]}$}
% 		{
% 			$\best[\anode] = \error{\anode,\test[\anode]}$\; %\min(|\posex(\anode=\test[\anode])|,|\negex(\anode=\test[\anode])|)$\;
% 			$\opt[\anode] = \tru$\;
% 			\ForEach{$branch \in \{\tru, \fal\}$}
% 			{
% 				% $\child[\anode,branch] \gets (|\posex(\anode=\test[\anode])| > |\negex(\anode=\test[\anode])|)$\;
% 				\lIf{$|\posex(\anode=\test[\anode])| > |\negex(\anode=\test[\anode])|$}{$\child[\anode,branch] \gets \posclass$}
% 				\lElse{$\child[\anode,branch] \gets \negclass$}
% 			}
% 		}
% 		{
% 			$\bud \gets \bud \cup \{n\}$\;
% 			$\best[\anode] = \min(|\posex(\anode)|, |\negex(\anode)|)$\;
% 			$\opt[\anode] \gets \fal$\;
% 		}
%
%
% 	\end{algorithm}
%
%
% 	\begin{algorithm}
% 		\caption{Suppress a node and all its descendants\label{alg:free}}
% 		\TitleOfAlgo{\prune}
% 	  \KwData{integer \anode}
%
% 		$\bud \gets \bud \setminus \{\anode\}$\;
% 		$\nodes \gets \nodes \setminus \{\anode\}$\;
%
% 		\ForEach{$branch \in \{\tru, \fal\}$}
% 		{
% 		\lIf{$\child[\anode,branch] \not\in \{\posclass, \negclass\}$}
% 		{
% 			$\prune{\child[\anode,branch]}$
% 		}
% 		}
%
% 		\lIf{$\mdepth[\anode] = k-1$ or $\opt[\anode]$}{$error \gets error - \best[\anode]$}
%
% 	\end{algorithm}
%
%
% \begin{algorithm}
% 	\caption{Search loop\label{alg:search}}
%   \TitleOfAlgo{\dt}
%   \KwData{$\negex,\posex, k$}
%   \KwResult{A decision tree}
%
% 	$\bnode \gets 0$\;
% 	$\posex(1),\negex(1) \gets \posex, \negex$\;
% 	$\grow{\bud, \sequence, 1}$\;
%
% 	\While{\textbf{true}}{
% 		\eIf{$\bud = \emptyset$} {
% 			$\ub \gets \min(\ub,\error)$\;
% 			$deadend \gets \fal$\;
% 			\Repeat{$deadend$}{
% 				\lIf{$\bnode > 0$}{$\opt[\bnode] \gets \tru$}
% 				\lIf{$\bnode = 1$}{\Return}
% 				$\bnode \gets \pop{\sequence}$\;
% 				$\best[\bnode] \gets \min(\best[\bnode], \best(\child[\bnode,\tru]) + \best(\child[\bnode,\fal]))$\;
% 				$\test[\bnode] \gets \pop{\dom[\bnode]}$\;
% 				$\prune(\child[\bnode,\tru])$\;
% 				$\prune(\child[\bnode,\fal])$\;
%
% 				$deadend \gets \best[\bnode] = 0 ~\vee~ \dom[\bnode] = \emptyset$\;
% 				\If{$deadend$}
% 				{
% 				$\opt[\bnode] \gets \tru$\;
% 				$\error \gets \error + \error{\bnode}$\; %$\best[\bnode]$\;
% 				}
% 			}
% 			$\bud \gets \bud \cup \{b\}$\;
% 			$\error \gets \error + \min(|\posex(\bnode)|, |\negex(\bnode)|)$\;
% 		}
% 		{
% 			\If{$b = 0$}{
% 				$b=\select{\bud}$\;
% 				% $\bud \gets \bud \setminus \{b\}$\;
% 				$\push(\bnode,\sequence)$\;
% 			}
% 			$c_{\tru}, c_{\fal} = \argmin_{x,y}(\mathbb{N} \setminus \nodes)$\;
% 			$\posex(c_{\tru}),\negex(c_{\tru}),\posex(c_{\fal}),\negex(c_{\fal}) \gets \branch(\posex(\bnode),\negex(\bnode),\test[\bnode])$\;
% 			\ForEach{$branch \in \{\tru, \fal\}$}{
% 				\eIf{$\min(|\posex(c_{branch})|,|\negex(c_{branch})|) = 0$}
% 				{
% 					\lIf{$|\posex(c_{branch})|>|\negex(c_{branch})|$}{$\child[\bnode,branch] \gets \posclass$}
% 					\lElse{$\child[\bnode,branch] \gets \negclass$}
% 				}{
% 					$\child[\bnode,branch] \gets c_{branch}$\;
% 					$\mdepth[c_{branch}] \gets \mdepth[\bnode]+1$\;
% 					$\grow(\bud, \sequence, c_{branch})$\;
% 				}
% 			}
% 			$\bnode \gets 0$\;
% 		}
% 	}
%
% \end{algorithm}

%\clearpage

\section{Extensions of the Algorithm}
\label{sec:ext}

The pseudo-code given in Algorithm~\ref{alg:bud} shows the basic structure of the algorithm. 
Some important (but rather tedious) parts of the algorithms have been omitted, such as how the best subtrees are stored in Line~\ref{line:storebest} when the best classification score is updated.
We discuss here some additional features that have an impact on the efficiency of the algorithm.




\subsection{Heuristic Ordering}
\label{sec:heuristic}

In order to quickly find accurate trees, it is important to select first the most promising features. We tried three heuristics based on scores to minimize: The \emph{classification error}, the \emph{entropy}~\cite{10.1023/A:1022643204877}, and the \emph{Gini impurity}~\cite{breiman1984classification}. 
Each of these heuristics associates a score to a feature $\afeat$ at a branch $\abranch$:
\begin{eqnarray}
	\textrm{classification error}: & \error[\grow{\abranch}{\afeat}] + \error[\grow{\abranch}{\bar{\afeat}}] \\
	%\textrm{minimum entropy:} & \sum_{v \in \{\afeat,\bar{\afeat}\}} \frac{|\allex(\abranch \wedge v)|}{|\allex[\abranch]|} \cdot -\sum_{\aclass \in \{\negclass,\posclass\}} \frac{|\setex{\aclass}(\abranch \wedge v)|}{|\setex{\aclass}(\abranch)} \log_{2} \frac{|\setex{\aclass}(\abranch \wedge v)|}{|\setex{\aclass}(\abranch)|} \\
	\textrm{entropy:} & \sum\limits_{v \in \{\afeat,\bar{\afeat}\}} -p(v,\allex) \sum\limits_{\aclass \in \{\negclass,\posclass\}} p(v,\setex{\aclass}) \log_{2} p(v,\setex{\aclass}) \\
	\textrm{Gini impurity:} &  \sum\limits_{v \in \{\afeat,\bar{\afeat}\}} p(v,\allex)(1 - \sum\limits_{\aclass \in \{\negclass,\posclass\}} p(v,\setex{\aclass})^2)
\end{eqnarray}
With $p(v,{\cal S}) = \frac{|{\cal S}(\grow{\abranch}{v})|}{|{\cal S}(\abranch)|}$ the ratio of datapoints with feature $v$ in the set ${\cal S}$.
% With $p(v,{\cal S}) = \frac{|{\cal S}(\abranch \wedge v)|}{|{\cal S}(\abranch)|}$.
%The minimum error is simply defined as 
%
%and the
 The
feature tests at Line~\ref{line:assignment} of Algorithm~\ref{alg:bud} are explored in non-decreasing order of the chosen score.
%with respect to one of the scores above.

In the data sets we used, the Gini impurity was significantly better, and hence all reported experiment results are using Gini impurity unless stated otherwise. For branches of length $\mdepth-1$, however, we use the error instead. Indeed, the optimal feature $\afeat$ for a branch $\abranch$ that cannot be extended further is the one minimizing 
% $\error[\abranch,\afeat]$. 
$\error[\grow{\abranch}{\afeat}] + \error[\grow{\abranch}{\bar{\afeat}}]$.
This means that we actually do not have to try other features for that node. This is implemented by the highlighted code at Line~\ref{line:optimal}: since one cannot improve on the first feature for test at depth $\mdepth$, branches of length $\mdepth-1$ do not have to be put back into \bud, and can be backtracked upon.

 % which means that we effectively restrict search to branches of length $\mdepth-1$.


%We order the possible features for branch $\abranch$ in non-decreasing order with respect to a score above and 
%explore the features in that order in Line~\ref{line:assignment}.
Computing the frequencies $p(\afeat,{\negex[\abranch]})$ and $p(\afeat,{\posex[\abranch]})$ of every feature $\afeat$ can be done in  $\Theta(\numfeat\numex)$ time where 
$\numex = |\negex[\abranch]|+|\posex[\abranch]|$, while $p(\bar{\afeat},{\negex[\abranch]})$ and $p(\bar{\afeat},{\posex[\abranch]})$ can be obtained by taking the complement to $|\negex[\abranch]|$ and $|\posex[\abranch]|$, respectively.
%\footnote{$p(\bar{\afeat},{\negex[\abranch]}) = |\negex[\abranch]| - p({\afeat},{\negex[\abranch]})$ and $p(\bar{\afeat},{\posex[\abranch]}) = |\posex[\abranch]| - p({\afeat},{\posex[\abranch]})$ can then be queried in constant time} 
In other words, this is more expensive than the splitting procedure by a factor $\numfeat$, but can be similarly amortized. However, since the depth of the branches is effectively reduced by one, the number of terminal branches is reduced by the same factor $\numfeat$, hence this incurs no asymptotic increase in complexity.
Furthermore, ordering the features (at Line~\ref{line:domain})
%Computing this order 
costs $\Theta(\numfeat \log \numfeat)$ for each of the $2^{\mdepth-1}\numfeat^{\mdepth-1}$ branches added to $\bud$ at Line~\ref{line:branching}. Again, since the depth of the branches is effectively reduced by one, the resulting complexity 
%(excluding the time for splitting the data set) 
is $O((\numex + 2^{\mdepth} \log \numfeat) \numfeat^{\mdepth})$. This very slight increase is often inconsequencial, as 
$\numex$ is still often the dominating term.
% long as we have $\numex \geq 2^{\mdepth} \log \numfeat$.

The feature ordering has a  significant impact on how quickly the algorithm can improve the accuracy of the classifier. Moreover, it also has an indirect, and  less significant, impact on the time necessary to prove optimality, because of the lower bound technique detailed in the next section.


\subsection{Lower Bound}
\label{sec:lb}

It is possible to fail early using a lower bound on the error given prior decisions, similarly as \dleight\ does~\cite{dl8}.
%, following the idea introduced in \cite{dl8}. 
% The idea is that once
When some subtrees along a branch $\abranch$ are optimal and the sum of their errors is larger than the current upper bound, 
%(the best solution found so far) 
then there is no need to continue exploring branch $\abranch$. 


%Line~\ref{line:leaves} can be changed to ``\textbf{If} $\bud \neq \emptyset ~\& \not\exists \abranch \in \bud, \dominated{\abranch}$ \textbf{then}''. %Notice that when a branch is ``pruned'' in this way, its 


%In this case, we can fail by forcing 


Observe that $\best[\abranch]$ is an upper bound on the classification error for any subtree rooted at $\abranch$, since this value comes from an actual tree (of depth $\mdepth - |\abranch|$ for the data set $\langle \negex[\abranch],\posex[\abranch] \rangle$). We can propagate this bound to parent nodes efficiently (in $O(|\abranch|)$ time). Here we assume that this is done recursively for the parent branch, every time the value $\best[\abranch]$ is  updated. %, by recursively applying the same update procedure to the parent.
%
Now, when the condition in Line~\ref{line:optimal} fails for a branch $\abranch$, it means that $\best[\abranch]$ is \emph{optimal}: no subtree rooted at $\abranch$ of maximum depth $\mdepth - |\abranch|$ has a classification error lower than $\best[\abranch]$. This is true either because every subtree has been explored, or, with the changes described in Section~\ref{sec:heuristic}, because $\mdepth - |\abranch| = 1$ and the feature $\afeat$ with least 
%$\error[\abranch,\afeat]$ 
$\error[\grow{\abranch}{\afeat}] + \error[\grow{\abranch}{\bar{\afeat}}]$
has been chosen. 
Let $\opt[\abranch]=1$ if the branch is optimal and $\opt[\abranch]=0$ otherwise. Notice that this is equivalent, for a branch $\grow{\abranch}{v}$ ending on test $v \in \{\afeat,\bar{\afeat}\}$, to checking if $|\abranch|=\mdepth-1$, or if $(\abranch,\afeat) \in \sequence$ but there is no pair $(\grow{\abranch}{v}, g)$ in sequence.
%\footnote{Alternatively, and indeed in our implementation, this information can be stored in an array.}
Moreover, let $\ancestors[\abranch]$ denote the ancestors of branch $\abranch$ in the current tree, i.e., $\ancestors[\abranch] = \{\aobranch \mid (\aobranch,v) \in \sequence ~\wedge~ \aobranch \subset \abranch\}$.
%
% Either way, a branch $\grow{\abranch}{v}$ ending on test $v \in \{\afeat,\bar{\afeat}\}$
% is optimal if and only if
% The extra Line~\ref{line:markoptimal} simply stores this information in the array $\opt$ which shall be used to compute a lower bound.
%
%loop in Line~\ref{line:backtrack} makes two or more iterations, it means that for the penultimate branch $\abranch$ popped out of \sequence, all possible subtrees have been explored, and therefore, $\best[\abranch]$ is also a lower bound on the classification error for any subtree rooted at $\abranch$. Let $\opt[\abranch]$ be 1 if $\abranch$ has been ``backtracked over'' in this way and 0 otherwise.
%
%Let $\abranch$ be a bud in $\bud$. Trivially, if $|\abranch|=\mdepth-1$ then this branch will entail $\min\{\error[\afeat] \mid \afeat \in \dom[\abranch]\}$ misclassifications.
Now, consider the highlighted code in Line~\ref{line:fail}.
For any ancestor $\abranch'$ of $\abranch$, we define a lower bound $\lb{\abranch',\abranch}$, {given the feature tests} $\abranch \setminus \abranch'$ as follows:
$$
\lb{\abranch',\abranch} = \sum\limits_{\abranch' \subset \grow{\abranch''}{\afeat} \subseteq \abranch}\opt[\grow{\abranch''}{\bar{\afeat}}] \cdot \best[\grow{\abranch''}{\bar{\afeat}}]
$$
In plain words, $\lb{\abranch',\abranch}$ is the sum the errors of optimal ``sibling'' branches between $\abranch'$ and $\abranch$.
%\footnote{An example illustrating this bound is given in Example~\ref{ex:lb} in  appendix.} %We illustrate this bound in Example~\ref{ex:lb}.
As long as these choices of feature tests stand (i.e., as long as $\abranch$ belongs to the current tree), these subtrees cannot be improved, hence this lower bound is correct.


%
% This lower bound is correct as long as the branch $\abranch$ belongs to the decision tree.
% The procedure $\dominated{\abranch}$ can therefore simply check, for all parent $\abranch'$ of $\abranch$ up until the root ($\emptyset$), whether $\lb{\abranch',\abranch} \geq \best[\abranch']$. As a result, a branch $\abranch$ which is guaranteed, by this reasoning, to never belong to a non-dominated tree will not be explored further.


% \begin{example}[Lower bound reasoning]
% 	\label{ex:lb}
%
%
% 	Figure~\ref{fig:lowerbound} shows a snapshot of the excution of \blossom. Every node is labelled with the feature test on that node, and with the values of $\best[\abranch]$ for the branch $\abranch$ ending on that node. When all subtrees of a branch $\abranch$ have been explored (hence $\opt[\abranch]=1$), this is marked by a ``$^*$''. We assume that the branch considered at Line~\ref{line:fail} is $\abranch = \{r, \bar{a}, \bar{c}, g\}$. For instance, we can suppose that a tree rooted at $\abranch$ with feature $e$ has been found (misclassifying 2 data points). Then, search moved to the sibling branch $\{r, \bar{a}, \bar{c}, \bar{g}\}$, which was then optimized for a total error of $4$, and now the pair $(\abranch,e)$ is popped out of \sequence. For all branches $\abranch'$ of $\abranch$, we give the values of $\lb{\abranch',\abranch}$ and $\best[\abranch']$ between brackets. Since there exists $\abranch'$ such that $\lb{\abranch',\abranch} \geq \best[\abranch']$ (e.g., $\emptyset$ and $\{r, \bar{a}\}$), we know that $\abranch$ cannot belong to an improving solution, and hence there is no need to try to extend it further.
%
% 	 % the current best classifier cannot be improved as long as
%
%
% 	\begin{figure}
% 	\begin{center}
% % \subfloat[upper bounds] {
% 		\scalebox{1}{
% 			\begin{forest}
% 				for tree={%
% 					l sep=25pt,
% 					s sep=10pt,
% 					node options={shape=rectangle, minimum width=10pt, inner sep=1pt, font=\footnotesize},
% 		  		edge={-latex, shorten >=1pt, shorten <=1pt},
% 				}
% 				[{$r:[50,50]$}
% 					[{$a:[19,22]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$b$}}
% 					 [{$h:15$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$a$}}
% 					 	 [{$e:10^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{h}$}}
% 					 	 ]
% 						 [{$c:3$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$h$}}
% 						 	[{$d:0^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$c$}}]
% 							[{$f:3^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{c}$}}]
% 						 ]
% 					 ]
% 					 [{$c:[19,17]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{a}$}}
% 					 	[{$f:15^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$c$}}
% 							% [$\posclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$f$}}]
% 							% [$\negclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{f}$}}]
% 						]
% 						[{$g:[4,6]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{c}$}}
% 							[{$e:2$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$g$}}
% 							]
% 							[{$h:4^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{g}$}}]
% 					 	]
% 					 ]
% 					]
% 					[{$d:31^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{b}$}}
% 					]
% 				]
% 			\end{forest}
% 		}
% 		% }
% 		% \subfloat[lower bounds w.r.t. $\{b,\bar{a},c,\bar{g}\}$] {
% 		% \scalebox{1}{
% 		% 	\begin{forest}
% 		% 		for tree={%
% 		% 			l sep=25pt,
% 		% 			s sep=10pt,
% 		% 			node options={shape=rectangle, minimum width=10pt, inner sep=1pt, font=\footnotesize},
% 		%   		edge={-latex, shorten >=1pt, shorten <=1pt},
% 		% 		}
% 		% 		[{$b,[49,50]$}
% 		% 			[{$a,[18,22]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$b$}}
% 		% 			 [{$.$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$a$}}
% 		% 			 ]
% 		% 			 [{$c,[18,17]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{a}$}}
% 		% 			 	[{$f,15^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$c$}}
% 		% 					% [$\posclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$f$}}]
% 		% 					% [$\negclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{f}$}}]
% 		% 				]
% 		% 				[{$g,[3,\infty]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{c}$}}
% 		% 					[., edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$g$}}
% 		% 					]
% 		% 					[{$h,3^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{g}$}}]
% 		% 			 	]
% 		% 			 ]
% 		% 			]
% 		% 			[{$d:31^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{b}$}}
% 		% 			]
% 		% 		]
% 		% 	\end{forest}
% 		% }
% 		% }
% 	\end{center}
% 	\caption{\label{fig:lowerbound} Example of lower bound computation w.r.t. the branch }
% 	%\caption{\label{fig:searchtree} The search tree for decision trees. \dynprog explores it depth first, whereas \blossom explores branches in the order given below the leaves.}
% 	\end{figure}
%
% \end{example}


% \medskip

This reasoning is more effective when good upper bounds are found early, hence the feature ordering  discussed in the previous section has an impact. Moreover, the choice of branch in Line~\ref{line:budchoice}) has an impact as well. We found that the simplest branch selection strategy was also the one giving the best results: we expand first the branch that was inserted into \bud\ first (i.e., \bud\ is \emph{FIFO}). One possible explanation is that by avoiding to unnecessarily ``jump'' to different parts of the decision tree, this strategy promotes optimizing sibling subtrees first, and therefore, deeper tree earlier.

% intuitivelly, one want to optimize the branches of the decision trees with the largest error first, in order to benefit from larger lower bounds earlier. To this end, it



 %for any feature test $\afeat \in \abranch \setminus \abranch'$, if 








%is $O((\numex + 2^{\mdepth}\numfeat \log \numfeat) \numfeat^{\mdepth-1})$, which is often better than 




% $$
% \sum_{v \in \{\afeat,\bar{\afeat\}} \frac{|\allex(\abranch \wedge v)|}{|\allex[\abranch]|} \cdot -\sum_{c \in \{\negclass,\posclass\}} \frac{|\setex{c}(\abranch \wedge v)}{|\setex{c}(\abranch)} \log_{2} \frac{|\setex{c}(\abranch \wedge v)}{|\setex{c}(\abranch)}
% $$


% $$
% 2 - \sum\limits_{v \in \{\afeat,\bar{\afeat}\}}\sum\limits_{c \in \{\negclass,\posclass\}} p(v,\setex{c})^2
% $$



\subsection{Preprocessing}
\label{sec:preprocessing}

Finally, we use two preprocessing techniques, one on the data set and one on the features. Although extremely straightforward (and probably not novel), they both have a significant impact.

\paragraph{Dataset reduction.}
It is easy to adapt \blossom\ to handle weighted data sets by redefining the error as follows, given a weight function $\weight$ on $\allex$:
$
\error[\abranch] = \min\left( \sum_{x \in \negex[\abranch]}\weight[x], \sum_{x \in \posex[\abranch]}\weight[x] \right)
$.

We can use the weighted version to handle noisy data, by merging duplicated datapoints and suppressing inconsistent datapoints.
%
Let $\weight^{\negclass}$ (resp. $\weight^{\posclass}$) denote the number of occurrences of $x$ in $\setex{\negclass}$ (resp. $\setex{\posclass}$). We use the weight function $\weight[x] = |\weight^{\negclass}(x) - \weight^{\posclass}(x)|$. Then, for any datapoint $x$, we remove all but one of its occurrences, in $\setex{\negclass}$ if $\weight^{\negclass}>\weight^{\posclass}$, in $\setex{\posclass}$ if $\weight^{\posclass}>\weight^{\negclass}$, and suppress it completely if $\weight^{\posclass}=\weight^{\negclass}$.
The reported error will then need to be offset by the number of pairs of suppressed inconsistent datapoints, that is:
$
\sum_{x \in \allex}\min(\weight^{\negclass}(x), \weight^{\posclass}(x))
$.
Reducing the number of datapoints in the data set has a non-null, although tiny impact on efficiency. However, suppressing inconsistent datapoints is very important. In particular, proving optimality when the minimum error is positive basically requires to exhaust the search space and is therefore extremely costly. On the other hand, when there exists a perfect tree within the maximum depth, we can stop as soon as we find it. This preprocessing allows to benefit from that when we find a tree whose error is equal to the number of pair of inconsistent datapoints in the original data set.
This preprocessing can be done in $O(\numfeat \numex \log \numex)$ by ordering the datapoints in lexicographic order and then processing them in sequence.

\paragraph{Feature reduction.}
%Redundant features (such that there exists another feature $\afeat'$ with either: $\forall x \in \allex, \afeat \in x \Leftrightarrow \afeat' \in x$, or $\forall x \in \allex, \afeat \in x \Leftrightarrow \afeat' \not\in x$) can be found in $O(\numex\numfeat^2)$ time by comparing pairs of rows in the data set.

A feature $\afeat$ is redundant if there exists another feature $\afeat'$ such that either: $\forall x \in \allex, \afeat \in x \Leftrightarrow \afeat' \in x$, or $\forall x \in \allex, \afeat \in x \Leftrightarrow \afeat' \not\in x$.
%We simply remove such redundant features.
They can be found in $O(\numex\numfeat^2)$ time by comparing pairs of rows of the data set.
%via bitset operations, and therefore in time $O(\numex\numfeat^2)$.

Removing redundant features may appear naive, however, it turns out that the binarization techniques (one-hot encoding) are often not optimized and many redundant features do exist in standard data sets. The number of features ($\numfeat$) has a huge impact on the complexity:
 %of the algorithm since 
 the branching factor of the algorithm is indeed
 %in the tree representing the search space is 
 $2\numfeat$ (see Figure~\ref{fig:searchtree}).
%
Moreover, at every branch, ``informationless'' features (i.e., features $\afeat$ such that $(\forall x \in \posex[\abranch] \afeat \in x) \iff (\forall x \in \negex[\abranch] \afeat \in x)$) can be suppressed at no additional cost since this can be detected when computing the feature ordering criterion.




\section{Experimental Results}
\label{sec:exp}

All experiments were run
on 4 cluster nodes, each with 36 Intel Xeon CPU E5-2695 v4 2.10GHz cores
running Linux Ubuntu 16.04.4. Sources were compiled using g++8. 
Every algorithm was run until completion or until reaching a time limit of one hour, and within a memory limit of 50GB.

We used a collection of 58 data sets formed by the union of the data sets from related work~\cite{narodytska2018learning,dl85,verwer2019learning}, to which we added extra data sets (\texttt{bank}, \texttt{titanic}, \texttt{surgical-deepnet} and \texttt{weather-aus}, as well as \texttt{mnist\_0}, \texttt{adult\_discretized}, \texttt{compas\_discretized} and \texttt{taiwan\_binarised}). Further description of the data sets as well as the raw data from our experimental results are given in appendix. For reason of space, we present aggregated results in this section.

We do not reproduce experiments to assess the accuracy of optimized and heuristic trees. Instead, we train on the whole data set, and focus on the training accuracy. The rationale is that previous experiments show that with a bounded depth, training and testing accuracies are well correlated, and we want to use the largest possible data sets in order to assess how well our algorithm scales.




\subsection{Computing (optimaly) accurate classifiers}

We first compare \blossom to state-of-the-art algorithms, \murtree~\cite{DBLP:journals/corr/abs-2007-12652} and \dleight~\cite{dl85}, as well as the best MIP (\binoct)~\cite{verwer2019learning} and CP (\cp)~\cite{verhaeghe2019learning} models, for computing and proving optimal trees. 

The data sets in Table~\ref{tab:summaryaccsmall} are organized 
in two classes according to 
the number of features \numfeat.
%the size \numfeat\ of their feature set.
Every method is run with a bound $\mdepth$ on the depth shown in the first column. 
We report for both classes and for each depth: the ratio of optimality proofs (opt.); the average training accuracy (acc.); and %the average accuracy (acc.), 
 %as well as 
 the average CPU time (cpu) to prove optimality.
Since \dleight and \binoct exceed the memory limit of 50GB in some cases, we also provide, for those two methods, the ratio of runs where at least one tree is found (sol.). For the same reason, we give their accuracy, marked by a ``$^*$'', as the average increase over \blossom's on these ``successful'' data sets. 
Similarly, the CPU time for all other methods is given as the average increase over \blossom's on data sets for which both methods prove optimality.



\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2.75pt
\input{src/tables/summaryclassesacc.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryaccsmall} Comparison with the state of the art: accuracy and optimality proofs}
\end{table}


\blossom is comparable to \murtree for the number of optimality proofs.
% The number of optimality proofs is similar for \blossom and \murtree.
It is slightly less efficient for  $\mdepth \leq 7$, but slightly more for $\mdepth = 10$. The gap on shallow trees can be explained by \murtree's caching, and because it puts less emphasis on finding good trees faster, but rather tries to exhaust the search space faster. The gap on deep trees can be partly explained by the removal of inconsistent datapoints: whereas \blossom can stop searching when the overall classification error reaches the number of inconsistent datapoints, \murtree must exhaust the search space. 
The difference in CPU time is due to the same phenomenon (when in favor or \blossom), or due to a few data sets, e.g. \texttt{mnist\_0}, where caching is probably helpful (when in favor or \murtree). Results on individual data sets (see Appendix) show that otherwise, both algorithms are comparable for proving optimality.

%Despite what a quick look at Table~\ref{tab:summaryaccsmall} may suggest, both methods have similar speed. The large gaps are either due to the same phenomenon described above (when in favor or \blossom), or due to a few data sets, e.g. \texttt{mnist\_0}, where caching is probably helpful (when in favor or \murtree).


% As \dleight does not provide a solution for every data set (on some instance it goes over the memory limit of 50GB), we provide the number of data sets for which a solution was returned (sol.). Moreover,
% instead of absolute values, we provide the average relative difference in error and accuracy w.r.t. \blossom, however, and only for the data sets where a decision tree was found. Similarly, we report the average cpu time ratio w.r.t. \blossom, however, only for instances which were proven optimal by both algorithms\footnote{every instance proven optimal by \dleight is also proven optimal by \blossom and \murtree}.


% \clearpage

% \begin{table}[t]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=3.75pt
% \input{src/tables/summaryclasses.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:summaryaccsmall} Comparison with the state of the art}
% \end{table}
%
%
% \begin{table}[t]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=3.5pt
% \input{src/tables/summaryclassesgerror.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:summaryaccsmall} Comparison with the state of the art, errors are geometric averages}
% \end{table}


When proving optimality is hard, however, \blossom is clearly the best in terms of accuracy, especially as the depth and the feature set grow. Notice that the accuracy results in Table~\ref{tab:summaryaccsmall} include data sets for which an optimal tree is found, so the gap on other data sets is much larger. Moreover, they are averaged over 58 data sets, so a gap of a fraction of a point is significant: the full results in appendix show that if the gaps are variable, they are consistently in favor of \blossom.
 % both algorithms find trees of similar qualities for $\mdepth \leq 5$ and $\numfeat < 100$, however, \blossom is significantly better as these parameters grow.
Other methods are systematically outperformed. \cp has good results on very shallow trees ($\mdepth \leq 4$) but is ineffective for deeper tree. Indeed, the accuracy actually \emph{decreases} when $\mdepth$ increases! \dleight can also find optimal trees in most cases 
for low values of \numfeat\ and $\mdepth$.
% \numfeat) is low, and for small values of $\mdepth$.
When $\numfeat$ grows, however, it often exceeds the memory limit of 50GB (whereas \blossom does not require more memory than the size of the data set). Finally, \binoct does not produce a single proof and very often exceeds the memory limit.%\footnote{In the experiments in \cite{verwer2019learning} not all datapoints were used.}
Figure~\ref{fig:proofcactus} shows the evolution of the ratio of proofs, averaged across all 58 data sets, over time: \blossom\ 
prove optimality faster when $\mdepth$ grows, but given enough time, \murtree\ matches it for $\mdepth \leq 7$.
 
 
 \begin{figure}[htbp]
 	% \subfloat[maximum depth = 3]{\input{src/tables/xscopt3.tex}}
 	\subfloat[maximum depth = 4]{\input{src/tables/xscopt4.tex}}
 	% \subfloat[maximum depth = 5]{\input{src/tables/xscopt5.tex}}
 	\subfloat[maximum depth = 7]{\input{src/tables/xscopt7.tex}}
 	\subfloat[maximum depth = 10]{\input{src/tables/xscopt10.tex}}
 	\caption{\label{fig:proofcactus}Proof ratio over time, averaged across all data sets}
 \end{figure}

% \clearpage




\subsection{Anytime behavior}



Next, we shift our focus to how fast can we obtain accurate trees and how fast can we improve the accuracy over basic solutions found by heuristics.
We use a well known heuristic as baseline: \cart (we ran its implementation in scikit-learn).
Here we report the average error after a given period of time (3 seconds, 10 seconds, 1 minute or 5 minutes), both for \murtree and \blossom in Table~\ref{tab:summaryspeed}.



\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=3pt
\input{src/tables/summaryaccspeed.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryspeed} Comparison with state the of the art: anytime behavior}
\end{table}


% \medskip

%We can see that the first solution found by \blossom has comparable accuracy to the one found by \cart. 
%The implementation of \cart in scikit-learn does not seem to be very efficient computationally. However, this is not so relevant as it is clear that one greedy run of the heuristic can be implemented to be as fast as the first dive of \blossom. 
The point of this experiment is threefold. Firstly, it shows that the first solution is very similar to that found by \cart. There is actually a slight advantage for \blossom, which can
be explained by the small difference in the heuristic selection of features: whereas \cart systematically selects the feature with minimum Gini impurity, \blossom does so for all \emph{but the deepest feature test}, for which it selects the feature with least classification error. 
Secondly, this first tree
is found extremely quickly, and there is no scaling issue with respect to the depth of the tree or with respect to the size of the data set. Thirdly, even for large data sets and deep trees, the accuracy of the initial classifier can be significantly improved given a reasonable computation time.

%Moreover, although we would need larger data sets to be confident about that, it seems that our algorithm is faster than \cart to find this first decision tree. %One can conjecture that \cart uses more sophisticated heuristic choices to explain these two observations.

%Then, in most cases, it is possible to improve the first solution significantly within a few seconds. Notice that for larger depth, improving the initial solution is harder and the 3s time limit is comparatively tighter than for smaller trees, so the gain of \blossom over \cart is more sensible for small trees.


% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/summaryspeed.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:summaryspeed} Comparison with state the of the art: computing accurate classifiers}
% \end{table}



Figure~\ref{fig:acccactus} reports the evolution of the average accuracy (across all 58 data sets) over time, giving a good view of the difference between \murtree and \blossom during search. The accuracy of the tree returned by \cart is given for reference. 
We can see in those graphs that \murtree finds an initial tree extremely quickly, although its accuracy is very low. This is because \murtree shows progress even when the tree is not complete, e.g., the first solution is always a single node with the most promising feature. We can see in Table~\ref{tab:summaryspeed} that this is indeed always the same first tree, irrespective of the depth.



\begin{figure}[htbp]
	% \subfloat[maximum depth = 3]{\input{src/tables/xscerror3.tex}}
	\subfloat[maximum depth = 4]{\input{src/tables/xscerror4.tex}}
	% \subfloat[maximum depth = 5]{\input{src/tables/xscerror5.tex}}
	\subfloat[maximum depth = 7]{\input{src/tables/xscerror7.tex}}
	\subfloat[maximum depth = 10]{\input{src/tables/xscerror10.tex}}
	\caption{\label{fig:acccactus}Accuracy over time, averaged across all data sets}
\end{figure}


\subsection{Factor analysis}

Finally, we report in Table~\ref{tab:factor} the results of three variants, in order to analyse the impact of the factors described in Section~\ref{sec:ext}. For each variant, we give the average accuracy (acc.), the ratio of proofs (opt.) and the relative increase of cpu time (cpu$^*$), on data sets for which an optimal tree has been found.

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=6pt
\input{src/tables/factorbfsacc.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:factor} Factor analysis}
\end{table}

In the variant ``No heuristic'', the Gini impurity heuristic described in Section~\ref{sec:heuristic} is disabled, and replaced by simply selecting first the feature with minimum error. For shallow trees (depth 3 or 4), since in many cases the search space is completely exhausted, not computing the slightly more costly Gini impurity score may actually be a good choice and we observe run time reduction of about 15\% to 20\%. However, the accuracy of the trees decreases extremely rapidly for larger maximum depth. As a results, many less optimality proofs are obtained, and they take much longer to compute.

In the variant ``No preprocessing'', the preprocessing described in Section~\ref{sec:preprocessing} is disabled. The feature ordering is impacted by the removal of datapoints, and therefore it may happen that, by luck, a more acurate tree is found for the non-preprocessed data set than for the preprocessed one. However, in most cases, the preprocessing does pay off, yielding more optimality proofs, better accuracy, and shorter runtimes. We estimate that most of the gain is due to the removal of redundant features, and of inconsistent datapoints, whereas the fusion of datapoints accounts for only a slight speed-up.

In the variant ``No lower bound'', the bound described in Section~\ref{sec:lb} is disabled. We observe a slight increase in computation time in average (but up to 200\% for some data sets). However, 
%the search space is explored in the same order, and 
it only slightly negatively affects the accuracy and the number of proofs.

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=7pt
% \input{src/tables/factorbfs.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:factor} Factor analysis}
% \end{table}



% \subsection{Balancing size and accuracy}
%
% Most decision trees toolkits somehow try to balance size and accuracy. \blossom uses the standard approach to bound the maximum depth and searches for the tree with maximum accuracy within that limit. Other methods focus on size rather than depth.
% For instance, the algorithm \gosdt~\cite{NEURIPS2019_ac52c626} optimize a linear combination of classification error and number of leaves.
%
% In order to compare with such approaches, we designed a method to trade accuracy for size based on pruning. Given the tree of accuracy $\alpha$ found by \blossom, and given a target accuracy $\tau \leq \alpha$, we suppress the subtree of size $s_i$ and classification error $\alpha_i$ such that $\alpha_i/s_i$ is minimum, as long as the overall accuracy is not lower than $\tau$.  We did not manage, unfortunately, to obtain a relevant comparison with \gosdt, because no setting of the regularization parameter enabled us to obtain trees with more than a dozen leafs. Instead we experimented with \iti~\cite{Utgoff97decisiontree}. We ran it on every data set, and grouped the resulting trees in 4 classes depending on their depths. The first column of Table~\ref{tab:iti} shows the number of data sets in each class. Then for \iti, we report the average classification error and size of the trees.
% For \blossom, we report the same data before and after pruning.
% Over more than half of the data sets, \blossom can find trees that are both smaller and more accurate than those found by \iti. On the first and last classes, however, \iti's trees are slightly smaller, albeit less accurate.
%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/summaryiti.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:iti} ITI}
% \end{table}



\section{Conclusion}

We have introduced a simple, exact, iterative, memory-efficient and anytime algorithm for computing optimaly-accurate tree classifiers of bounded depth.
This algorithm is considerably more efficient than state-of-the-art exact algorithms. Moreover, it has no significant time nor memory overhead with respect to greedy heuristic methods.




\bibliographystyle{plain}
\bibliography{src/references}


\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \textbf{Yes}. %\answerTODO{}
  \item Did you describe the limitations of your work?
    \textbf{Yes (some limitations)}.
		%\answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
		\textbf{No}.
    % \answerTODO{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
		\textbf{No, they were no available}.
    % \answerTODO{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
	\textbf{Yes}.
    % \answerTODO{}
	\item Did you include complete proofs of all theoretical results?
	\textbf{No, a complete proof of correctness would be both pretty dull and potentially long. Only the invariants are given. The proof of the worst case time complexity is complete, however}.
    % \answerTODO{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    %\answerTODO{}
		\textbf{Yes, all the results are in the appendix, the actual code will be made available after the review process to avoid compromising the double-blindness of the reviewing process}.
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
		\textbf{Yes}.
    %\answerTODO{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \textbf{No (not all comparison methods can be randomized, so confidence is obtained by using many data sets and aggregating the results)}.
		%\answerTODO{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
		\textbf{Yes}.
    %\answerTODO{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \textbf{We give the source of the data sets in the Appendix, and cite the authors of the algorithms we compared our algorithm to. We used no other asset.}
  \item Did you mention the license of the assets?
    \textbf{N/a}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \textbf{We will made the few data sets that we binarized ourselve publicly available after the publication.}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \textbf{No}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \textbf{No}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    % \answerTODO{}
		\textbf{N/a}.
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    % \answerTODO{}
		\textbf{N/a}.
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    % \answerTODO{}
		\textbf{N/a}.
\end{enumerate}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Appendix}


Section~\ref{appendix:info} provides some information about the chosen data sets. Section~\ref{appendix:lb} provides an example for the lower bound reasoning described in Section~\ref{sec:lb}. In Section~\ref{appendix:extra} we report the results of some experimental evaluation on balancing size and accuracy with \blossom. Finally, in Section~\ref{appendix:full}, we report the raw data from our experiments (for every method and every data set).


\subsection{Information about the data sets}
\label{appendix:info}


The benchmark of classification data set we used is described in Table~\ref{tab:info}. It consists of 50 data sets 
commonly used in related work articles (specifically, \cite{narodytska2018learning,dl85,verwer2019learning}), to which we added the following large data sets in order to stress how well the different approaches scale.

\begin{itemize}
	\item The data set \texttt{taiwan\_binarised} comes from the \href{https://archive.ics.uci.edu/ml/index.php}{UCI repository} and was discretized using ad-hoc threshold on continuous features.
	\item The data sets \texttt{adult\_discretized} and \texttt{compas\_discretized} [TODO!!]
	\item The data sets \texttt{bank}, \texttt{titanic}, \texttt{surgical-deepnet} and \texttt{weather-aus} come from \href{https://www.kaggle.com/}{Kaggle} and were binarized using the one-hot encoding implemented by the authors of  \cite{narodytska2018learning}.
	\item The data set \texttt{mnist\_0} is the well known data set on hand written digits binarized as follows: every pixel is a binary attribute whose value is 1 if its greyscale is larger than $0.5$ and 0 otherwise. The data point is positive if it is the digit ``0'' and negative otherwise.
\end{itemize}

We report the number of data points ($|\allex|$), the number of features ($|\features|$), the same parameters after preprocessing (respectively $|\allex|^*$ and $|\features|^*$), and the ``noise'' ratio, that is: $2|\posex \cap \negex|/(|\posex|+|\negex|)$.



\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
\tabcolsep=10pt%
\input{src/tables/datasetinfo.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:info} Benchmark and preprocessing data}%
\end{table}%


\subsection{Example of lower bound reasonning}
\label{appendix:lb}

\begin{example}[Lower bound reasoning]
	\label{ex:lb}


	Figure~\ref{fig:lowerbound} shows a snapshot of the excution of \blossom. Every node is labelled with the feature test on that node, and with the values of $\best[\abranch]$ for the branch $\abranch$ ending on that node. When all subtrees of a branch $\abranch$ have been explored (hence $\opt[\abranch]=1$), this is marked by a ``$^*$''. We assume that the branch considered at Line~\ref{line:fail} is $\abranch = \{r, \bar{a}, \bar{c}, g\}$. For instance, we can suppose that a tree rooted at $\abranch$ with feature $e$ has been found (misclassifying 2 data points). Then, search moved to the sibling branch $\{r, \bar{a}, \bar{c}, \bar{g}\}$, which was then optimized for a total error of $4$, and now the pair $(\abranch,e)$ is popped out of \sequence. For all branches $\abranch'$ of $\abranch$, we give the values of $\lb{\abranch',\abranch}$ and $\best[\abranch']$ between brackets. Since there exists $\abranch'$ such that $\lb{\abranch',\abranch} \geq \best[\abranch']$ (e.g., $\emptyset$ and $\{r, \bar{a}\}$), we know that $\abranch$ cannot belong to an improving solution, and hence there is no need to try to extend it further.

	 % the current best classifier cannot be improved as long as


	\begin{figure}
	\begin{center}
% \subfloat[upper bounds] {
		\scalebox{1}{
			\begin{forest}
				for tree={%
					l sep=25pt,
					s sep=10pt,
					node options={shape=rectangle, minimum width=10pt, inner sep=1pt, font=\footnotesize},
		  		edge={-latex, shorten >=1pt, shorten <=1pt},
				}
				[{$r:[50,50]$}
					[{$a:[19,22]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$b$}}
					 [{$h:15$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$a$}}
					 	 [{$e:10^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{h}$}}
					 	 ]
						 [{$c:3$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$h$}}
						 	[{$d:0^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$c$}}]
							[{$f:3^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{c}$}}]
						 ]
					 ]
					 [{$c:[19,17]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{a}$}}
					 	[{$f:15^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$c$}}
							% [$\posclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$f$}}]
							% [$\negclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{f}$}}]
						]
						[{$g:[4,6]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{c}$}}
							[{$e:2$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$g$}}
							]
							[{$h:4^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{g}$}}]
					 	]
					 ]
					]
					[{$d:31^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{b}$}}
					]
				]
			\end{forest}
		}
		% }
		% \subfloat[lower bounds w.r.t. $\{b,\bar{a},c,\bar{g}\}$] {
		% \scalebox{1}{
		% 	\begin{forest}
		% 		for tree={%
		% 			l sep=25pt,
		% 			s sep=10pt,
		% 			node options={shape=rectangle, minimum width=10pt, inner sep=1pt, font=\footnotesize},
		%   		edge={-latex, shorten >=1pt, shorten <=1pt},
		% 		}
		% 		[{$b,[49,50]$}
		% 			[{$a,[18,22]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$b$}}
		% 			 [{$.$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$a$}}
		% 			 ]
		% 			 [{$c,[18,17]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{a}$}}
		% 			 	[{$f,15^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$c$}}
		% 					% [$\posclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$f$}}]
		% 					% [$\negclass$, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{f}$}}]
		% 				]
		% 				[{$g,[3,\infty]$}, edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{c}$}}
		% 					[., edge={very thick}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$g$}}
		% 					]
		% 					[{$h,3^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{g}$}}]
		% 			 	]
		% 			 ]
		% 			]
		% 			[{$d:31^*$}, edge label={node[midway,fill=white,inner sep=2pt,font=\scriptsize]{$\bar{b}$}}
		% 			]
		% 		]
		% 	\end{forest}
		% }
		% }
	\end{center}
	\caption{\label{fig:lowerbound} Example of lower bound computation w.r.t. the branch }
	%\caption{\label{fig:searchtree} The search tree for decision trees. \dynprog explores it depth first, whereas \blossom explores branches in the order given below the leaves.}
	\end{figure}

\end{example}


\subsection{Extra experiments on balancing size and accuracy}
\label{appendix:extra}

Most decision trees toolkits somehow try to balance size and accuracy. \blossom uses the standard approach to bound the maximum depth and searches for the tree with maximum accuracy within that limit. Other methods focus on size rather than depth. 
For instance, the algorithm \gosdt~\cite{NEURIPS2019_ac52c626} optimize a linear combination of classification error and number of leaves. 

In order to compare with such approaches, we designed a method to trade accuracy for size based on pruning. Given the tree of accuracy $\alpha$ found by \blossom, and given a target accuracy $\tau \leq \alpha$, we suppress the subtree of size $s_i$ and classification error $\alpha_i$ such that $\alpha_i/s_i$ is minimum, as long as the overall accuracy is not lower than $\tau$.  We did not manage, unfortunately, to obtain a relevant comparison with \gosdt, because no setting of the regularization parameter enabled us to obtain trees with more than a dozen leafs. Instead we experimented with \iti~\cite{Utgoff97decisiontree}. We ran it on every data set, and grouped the resulting trees in 4 classes depending on their depths. The first column of Table~\ref{tab:iti} shows the number of data sets in each class. Then for \iti, we report the average classification error and size of the trees.
For \blossom, we report the same data before and after pruning. 
Over more than half of the data sets, \blossom can find trees that are both smaller and more accurate than those found by \iti. On the first and last classes, however, \iti's trees are slightly smaller, albeit less accurate.


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=10pt
\input{src/tables/summaryiti.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:iti} ITI}
\end{table}


\subsection{Full experimental results}
\label{appendix:full}

We report here the raw data from our experimental comparison with the state of the art for $\mdepth=3,4,5,7,10$ and for the four size catagories in the following tables:

\tabcolsep=10pt
% \begin{center}
% \begin{tabular}{lrr}
% 	\toprule
% 	\multirow{2}{*}{$\mdepth$}& $\numfeat < 100$ & $\numfeat \geq 100$ \\
% 	 \midrule
% 	$3$ & Table~\ref{tab:all31} & Table~\ref{tab:all32}  \\
% 	$4$ & Table~\ref{tab:all41} & Table~\ref{tab:all42}  \\
% 	$5$ & Table~\ref{tab:all51} & Table~\ref{tab:all52}  \\
% 	$7$ & Table~\ref{tab:all71} & Table~\ref{tab:all72}  \\
% 	$10$ & Table~\ref{tab:all101} & Table~\ref{tab:all102}  \\
% 	\bottomrule
% \end{tabular}
% \end{center}


\begin{center}
	\def\arraystretch{1.25}
\begin{tabular}{lccccc}
	\toprule
	$\numfeat$& $\mdepth = 3$ & $\mdepth = 4$ & $\mdepth = 5$ & $\mdepth = 7$ & $\mdepth = 10$ \\
	 \midrule
	$< 100$ & Table~\ref{tab:all31} & Table~\ref{tab:all41} & Table~\ref{tab:all51} & Table~\ref{tab:all71} & Table~\ref{tab:all101} \\
	$\geq 100$ & Table~\ref{tab:all42} & Table~\ref{tab:all32} & Table~\ref{tab:all52} & Table~\ref{tab:all72} & Table~\ref{tab:all102} \\
	\bottomrule 
\end{tabular}
\end{center}


\def\arraystretch{1}

% in Tables~\ref{tab:all3},
% \ref{tab:all4},
% \ref{tab:all5},
% \ref{tab:all6},
% \ref{tab:all7},
% \ref{tab:all8},
% \ref{tab:all9} and \ref{tab:all10}, respectively.
For every instance, we give the classification error of the best tree found within a time limit of 1h for every method. Moreover, we give the CPU time taken by each method to prove optimality when optimality is proven (in which case we mark it by a ``$^*$''), and to find the best solution otherwise. Notice that \cp\ and \dleight\ are not anytime and hence only report a solution at the end of the time limit when optimality is not proven. In this case, we write $\geq1h$. 

\medskip

Every process was first run with a memory limit of 3.5GB. Many runs of \dleight, \cp\ and \binoct\ went well over that limit and were rerun with a limit of 50GB. Still, 138 runs of \binoct and 164 runs of \dleight (out of 460) went over the limit. As \binoct can output trees anytime, the data for these runs (up until the memory blow-out) are in the tables. For \dleight, however, this is marked as a ``-'' since there was no output.
%\binoct 138
%\dleight 164 %5+10+20+23+30+27+25+24






\tabcolsep=5pt%




\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% %\tabcolsep=2pt%
\input{src/tables/allclasses_smallm_3.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all31} Comparison with state of the art: $\numfeat<100$, depth 3}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% \tabcolsep=2pt%
\input{src/tables/allclasses_bigm_3.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all32} Comparison with state of the art: $\numfeat \geq 100$, depth 3}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% \tabcolsep=2pt%
\input{src/tables/allclasses_smallm_4.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all41} Comparison with state of the art: $\numfeat<100$, depth 4}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% \tabcolsep=2pt%
\input{src/tables/allclasses_bigm_4.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all42} Comparison with state of the art: $\numfeat \geq 100$, depth 4}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% \tabcolsep=2pt%
\input{src/tables/allclasses_smallm_5.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all51} Comparison with state of the art: $\numfeat<100$, depth 5}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% \tabcolsep=2pt%
\input{src/tables/allclasses_bigm_5.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all52} Comparison with state of the art: $\numfeat \geq 100$, depth 5}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
% \tabcolsep=2pt%
\input{src/tables/allclasses_smallm_7.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all71} Comparison with state of the art: $\numfeat<100$, depth 7}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allclasses_bigm_7.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all72} Comparison with state of the art: $\numfeat \geq 100$, depth 7}%
\end{table}%


\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allclasses_smallm_10.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all101} Comparison with state of the art: $\numfeat<100$, depth 10}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allclasses_bigm_10.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all102} Comparison with state of the art: $\numfeat \geq 100$, depth 10}%
\end{table}%



\begin{table}[htbp]%Factor analysis
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_smallm_3.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all31} Factor analysis: $\numfeat<100$, depth 3}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_bigm_3.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all32} Factor analysis: $\numfeat \geq 100$, depth 3}%
\end{table}%


\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_smallm_4.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all41} Factor analysis: $\numfeat<100$, depth 4}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_bigm_4.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all42} Factor analysis: $\numfeat \geq 100$, depth 4}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_smallm_5.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all51} Factor analysis: $\numfeat<100$, depth 5}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_bigm_5.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all52} Factor analysis: $\numfeat \geq 100$, depth 5}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_smallm_7.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all71} Factor analysis: $\numfeat<100$, depth 7}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_bigm_7.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all72} Factor analysis: $\numfeat \geq 100$, depth 7}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_smallm_10.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all101} Factor analysis: $\numfeat<100$, depth 10}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allfactors_bigm_10.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all102} Factor analysis: $\numfeat \geq 100$, depth 10}%
\end{table}%


% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % %\tabcolsep=2pt%
% \input{src/tables/allclasses_smallmall_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all31} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 3}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % %\tabcolsep=2pt%
% \input{src/tables/allclasses_smallbig_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all32} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 3}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % %\tabcolsep=2pt%
% \input{src/tables/allclasses_bigsmall_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all33} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 3}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_bigbig_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all34} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 3}%
% \end{table}%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_smallmall_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all41} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_smallbig_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all42} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_bigsmall_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all43} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_bigbig_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all44} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_smallmall_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all51} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 5}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_smallbig_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all52} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 5}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_bigsmall_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all53} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 5}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_bigbig_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all54} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 5}%
% \end{table}%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_smallmall_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all71} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 7}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_smallbig_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all72} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 7}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% % \tabcolsep=2pt%
% \input{src/tables/allclasses_bigsmall_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all73} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 7}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allclasses_bigbig_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all74} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 7}%
% \end{table}%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allclasses_smallmall_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all101} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 10}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allclasses_smallbig_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all102} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 10}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allclasses_bigsmall_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all103} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 10}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allclasses_bigbig_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all104} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 10}%
% \end{table}%
%
%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallmall_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all31} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 3}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallbig_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all32} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 3}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigsmall_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all33} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 3}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigbig_3.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all34} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 3}%
% \end{table}%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallmall_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all41} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallbig_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all42} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigsmall_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all43} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigbig_4.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all44} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 4}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallmall_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all51} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 5}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallbig_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all52} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 5}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigsmall_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all53} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 5}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigbig_5.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all54} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 5}%
% \end{table}%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallmall_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all71} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 7}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallbig_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all72} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 7}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigsmall_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all73} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 7}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigbig_7.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all74} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 7}%
% \end{table}%
%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallmall_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all101} Comparison with state of the art: $\numex<5000, \numfeat<250$, depth 10}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_smallbig_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all102} Comparison with state of the art: $\numex<5000, \numfeat \geq 250$, depth 10}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigsmall_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all103} Comparison with state of the art: $\numex \geq 5000, \numfeat < 250$, depth 10}%
% \end{table}%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{scriptsize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfactors_bigbig_10.tex}%
% \end{scriptsize}%
% \end{center}%
% \caption{\label{tab:all104} Comparison with state of the art: $\numex \geq 5000, \numfeat \geq 250$, depth 10}%
% \end{table}%


%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/tableiti.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:iti} Comparison with ITI}
% \end{table}



\end{document}


% We report in Table~\ref{tab:summaryacc} data averaged over the 47 data sets described above, for
%
%
% the average accuracy found within the one hour time limit for \blossom and \murtree
%
% on relatively shallow trees (3,4 and 5) in tables~\ref{tab:d3}, \ref{tab:d4} and \ref{tab:d5}, respectively.
% We give the minimum \emph{error}, the cpu in seconds \emph{time} and size of the search space (\emph{choices}) required to prove optimality (when a proof is given, as markes by a 1 in the column \emph{opt}) or to find the best solution (otherwise).


% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=3.7pt
% \input{src/tables/summaryopt.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:summary} Comparison with the state of the art: computing optimal trees}
% \end{table}


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.7pt
\input{src/tables/summaryaccsmall.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryaccsmall} Comparison with the state of the art on data sets with at most 10000 datapoints: computing optimal trees}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.7pt
\input{src/tables/summaryacclarge.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryacclarge} Comparison with the state of the art on data sets with more than 10000 datapoints: computing optimal trees}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.7pt
\input{src/tables/summaryacc.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryacc} Comparison with the state of the art: computing optimal trees}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.7pt
\input{src/tables/summaryaccother.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryacc} Comparison with the state of the art: computing optimal trees}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.7pt
\input{src/tables/summaryaccall.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryacc} Comparison with the state of the art: computing optimal trees}
\end{table}






% \begin{figure}
% 	\subfloat[depth=3]{\input{src/tables/scerror3.tex}}
% 	\subfloat[depth=5]{\input{src/tables/scerror5.tex}}
% 	\subfloat[depth=7]{\input{src/tables/scerror7.tex}}
%
% 	\subfloat[depth=8]{\input{src/tables/scerror8.tex}}
% 	\subfloat[depth=9]{\input{src/tables/scerror9.tex}}
% 	\subfloat[depth=10]{\input{src/tables/scerror10.tex}}
% 	\caption{\label{fig:cactus}Accuracy over time}
% \end{figure}

\subsection{Factor analysis}

Finally, we report results of three variants of \blossom, in order to analyse the relative contributions of the factors described in Section~\ref{sec:ext}. For each variant, we report the average error (error), the ratio of optimality proofs (opt.) and the cpu time ratio with respect to the default setting on data sets for which an optimal tree has been found.

In the variant ``No heuristic'', the Gini impurity heuristic described in Section~\ref{sec:heuristic} is disabled, and replaced by simply selecting first the feature with minimum error. For shallow trees (depth 3 or 4), since in many cases the search space is completely exhausted, not computing the slightly more costly Gini impurity score may actually be a good choice and we observe run time reduction of about 15\% to 20\%. However, the accuracy of the trees decreases extremely rapidly for larger maximum depth. As a results, many less optimality proofs are obtained, and they take much longer to compute.

In the variant ``No preprocessing'', both data set and feature preprocessings described in Section~\ref{sec:preprocessing} are disabled. The feature ordering is impacted by the removal of inconsistent datapoints, and therefore it may happen that, by luck, a more acurate tree is found for the non-preprocessed data set than for the preprocessed one. However, in most cases, the preprocessing does pay of, yielding more optimality proofs, better accuracy, and shorter runtimes (on that last aspect, we estimate that most of the gain is due to the removal of redundant features, and of inconsistent datapoints, whereas the fusion of duplicated datapoints accounts for only a few percent speed-up).

In the variant ``No lower bound'', the lower bound technique described in Section~\ref{sec:lb} is disabled. In this case we observe a small increase in computational time (from 1\% up to 40\%). However, the search space is explored in the same order, and it only slightly negatively affect accuracy and the number of optimality proofs.

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/factorbfs.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:factor} Factor analysis}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=5pt
\input{src/tables/summaryiti.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:iti} ITI}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=5pt
\input{src/tables/tableiti.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:iti} ITI}
\end{table}



\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=5pt
\input{src/tables/gosdt.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:gosdt} GOSDT}
\end{table}



\section{Conclusion}

We have introduced a simple, exact, iterative, memory-efficient and anytime algorithm for computing optimaly-accurate tree classifiers of bounded depth.
This algorithm is considerably more efficient than state-of-the-art exact algorithms. Moreover, it has no significant time nor memory overhead with respect to greedy heuristic methods.


\bibliographystyle{plain}
\bibliography{src/references}


% \end{document}

\clearpage

% \newgeometry{bottom=2cm,top=2cm,margin=1cm}

\section*{Appendix}

The benchmark of classification data set we used is described in Table~\ref{tab:info}. It consists of 30 data sets 
commonly used in related work articles, to which we added some large data sets from Kaggle: \texttt{bank}, \texttt{titanic}, \texttt{surgical-deepnet} and \texttt{weather-aus}, as well as the \texttt{mnist} data sets, \texttt{adult\_discretized} and \texttt{compas\_discretized}. We report the number of data points ($|\allex|$), the number of features ($|\features|$), the same parameters after preprocessing (respectively $|\allex|^*$ and $|\features|^*$), and the ``noise'' ratio, that is: $2|\posex \cap \negex|/(|\posex|+|\negex|)$.

\medskip

Then we report the raw data from our experimental comparison with the state of the art for $\mdepth=3,4,5,6,7,8,9,10$
in Tables~\ref{tab:all3},
\ref{tab:all4},
\ref{tab:all5},
\ref{tab:all6},
\ref{tab:all7},
\ref{tab:all8},
\ref{tab:all9} and \ref{tab:all10}, respectively.
For every instance, we give the classification error of the best tree found within a time limit of 1h for every method. Moreover, we give the CPU time taken by each method to prove optimality when optimality is proven (in which case we mark it by a ``$^*$''), and to find the best solution otherwise. Notice that \cp\ and \dleight\ are not anytime and hence only report a solution at the end of the time limit when optimality is not proven. In this case, we write $\geq1h$. 

\medskip

Every process was first run with a memory limit of 3.5GB. Many runs of \dleight, \cp\ and \binoct\ went well over that limit and were rerun with a limit of 50GB. Still, 138 runs of \binoct and 164 runs of \dleight (out of 460) went over the limit. As \binoct can output trees anytime, the data for these runs (up until the memory blow-out) are in the tables. For \dleight, however, this is marked as a ``-'' since there was no output.
%\binoct 138
%\dleight 164 %5+10+20+23+30+27+25+24


\renewcommand{\arraystretch}{.8}

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
\tabcolsep=10pt%
\input{src/tables/datasetinfo.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:info} Benchmark and preprocessing data}%
\end{table}%

\begin{table}[htbp]%
\begin{center}%
\begin{scriptsize}%
%\tabcolsep=2pt%
\input{src/tables/allopt3.tex}%
\end{scriptsize}%
\end{center}%
\caption{\label{tab:all3} Comparison with state of the art: depth 3}%
\end{table}%

\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt4.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all4} Comparison with state of the art: depth 4}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt5.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all5} Comparison with state of the art: depth 5}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt6.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all6} Comparison with state of the art: depth 6}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt7.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all7} Comparison with state of the art: depth 7}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt8.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all8} Comparison with state of the art: depth 8}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt9.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all9} Comparison with state of the art: depth 9}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/allopt10.tex}
\end{scriptsize}
\end{center}
\caption{\label{tab:all10} Comparison with state of the art: depth 10}
\end{table}


%
%
% \begin{table}[htbp]%
% \begin{center}%
% \begin{footnotesize}%
% %\tabcolsep=2pt%
% \input{src/tables/allfact3.tex}%
% \end{footnotesize}%
% \end{center}%
% \caption{\label{tab:fact3} Factor analysis: depth 3}%
% \end{table}%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact4.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact4} Factor analysis: depth 4}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact5.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact5} Factor analysis: depth 5}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact6.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact6} Factor analysis: depth 6}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact7.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact7} Factor analysis: depth 7}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact8.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact8} Factor analysis: depth 8}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact9.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact9} Factor analysis: depth 9}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=2pt
% \input{src/tables/allfact10.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:fact10} Factor analysis: depth 10}
% \end{table}


\end{document}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allacc9.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 9}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allspeed3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 3}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allspeed4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 4}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allspeed9.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 9}
\end{table}

\medskip




\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 3}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 4}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 5}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt6.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 6}
\end{table}

\medskip


When the maximum depth and number of feature is not too large, both algorithms are comparable, although \blossom is systematically faster. However, when the depth or the number of features grows, the best solution found by \dleight is often of much lower quality. In fact, in most cases, it reaches the time or memory limit without outputing a solution (the missing entries corresponds to \dleight reaching the 50GB memory limit). Notice that \blossom uses a tiny memory space (much lower than the size of the data set).



\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/cpusmall3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d3} Comparison with state of the art on shallow trees (max depth=3)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/cpusmall4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d4} Comparison with state of the art on shallow trees (max depth=4)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/cpusmall5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d5} Comparison with state of the art on shallow trees (max depth=5)}
\end{table}







\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f4} Comparison with state of the art heuristics (max depth=4)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first7.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f7} Comparison with state of the art heuristics (max depth=7)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first10.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f10} Comparison with state of the art heuristics (max depth=10)}
\end{table}


\subsection{Size stats}


\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s3} max depth=3}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s4} max depth=4}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s5} max depth=5}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex7.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s7} max depth=7}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex10.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s10} max depth=10}
\end{table}

%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall3.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha3} Comparison of heuristics (max depth=3)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall4.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha4} Comparison of heuristics (max depth=4)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall5.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha5} Comparison of heuristics (max depth=5)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall7.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha7} Comparison of heuristics (max depth=7)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall10.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha10} Comparison of heuristics (max depth=10)}
% \end{table}
%
%
%
%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst3.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha3} Comparison of heuristics (max depth=3)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst4.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha4} Comparison of heuristics (max depth=4)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst5.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha5} Comparison of heuristics (max depth=5)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst7.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha7} Comparison of heuristics (max depth=7)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst10.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha10} Comparison of heuristics (max depth=10)}
% \end{table}


% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth5b.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=5)}
% \end{table}

% \clearpage

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth8.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=8)}
% \end{table}

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth10.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=10)}
% \end{table}



\end{document}

