\documentclass{llncs}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%% http://ctan.org/pkg/xcolor
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{array}
%\usepackage{amsthm}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{fp}
\usepackage{subfig}
\usetikzlibrary{arrows,shadows,fit,calc,positioning,decorations.pathreplacing,matrix,shapes,petri,topaths,fadings,mindmap,backgrounds,shapes.geometric}
% \usepackage{geometry}
\usepackage{xifthen}
\usepackage{rotating}
\usepackage{forest}
\usepackage{relsize}


% \newgeometry{bottom=2cm,top=2cm}

% \newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


% \newtheorem{theorem}{Theorem}


\newcommand{\tru}[0]{\texttt{true}}
\newcommand{\fal}[0]{\texttt{false}}
% \newcommand{\tru}[0]{1}
% \newcommand{\fal}[0]{0}


\newcommand\cactus[5]{%%
\begin{tikzpicture}[scale=.46]
	\begin{axis}[scatter/classes={
		a={mark=square*,blue!50,draw=blue!50!black},%%
		b={mark=triangle*,red!50,draw=red!50!black},%%
		c={mark=diamond*,green!50,draw=green!50!black},%%
		d={mark=*,black!50!white,draw=black},%%
		e={mark=pentagon*,orange!50,draw=orange!50!black},%%
		f={mark=10-pointed star,draw=blue!60!black},%%
		g={mark=x,draw=black,thick},%%
		h={mark=+,draw=red!50!black},%%
		i={mark=Mercedes star,draw=green!50!black},%%
		j={mark=square,draw=orange!50!black},%%
		k={mark=diamond,draw=red!50!black},%%
		l={mark=o,draw=blue!50!black},%%
		m={mark=asterisk,draw=magenta!50!black},%%
		n={mark=triangle,draw=green!50!black},%%
		o={mark=pentagon,draw=red!50!black},%%
		p={mark=asterisk,draw=black},%%
		q={mark=Mercedes star,draw=magenta!50!black},%%
		r={mark=pentagon*,blue!50,draw=blue!50!black},%%
		s={mark=x,draw=green!50!black},%%
		t={mark=diamond*,red!50,draw=red!50!black}},%%
		mark size=1.5pt,
		ymode=log,
		%xmode=log,
		ylabel=#2,
		xlabel=#1,
		font=\large,
		{#5}
		]

		\foreach \bidule in #4 {

		\addplot[scatter, scatter src=explicit symbolic] coordinates {\bidule};

		}
		\legend{#3}
	\end{axis}
\end{tikzpicture}
}


\newcommand{\setex}[1]{\ensuremath{{\mathcal X}^{#1}}\xspace}
\newcommand{\posex}{{\setex{\top}}\xspace}
\newcommand{\negex}{{\setex{\bot}}\xspace}
\newcommand{\allex}{{\setex{}}\xspace}
\newcommand{\setcube}[1]{\ensuremath{{\mathcal C}^{#1}}\xspace}
\newcommand{\poscube}{{\setcube{1}}\xspace}
\newcommand{\negcube}{{\setcube{0}}\xspace}
\newcommand{\features}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\feat}{\ensuremath{f}}
\newcommand{\classifier}{\ensuremath{f}}
\newcommand{\lit}[1]{\ensuremath{l_{#1}}}
\newcommand{\var}{\ensuremath{x}}
\newcommand{\truelit}[1]{\ensuremath{\var_{#1}}}
\newcommand{\falselit}[1]{\ensuremath{\bar{\var_{#1}}}}
\newcommand{\ex}{\ensuremath{\var}}
\newcommand{\cube}{\ensuremath{c}}
\newcommand{\universe}{\ensuremath{{\mathcal U}}}
\newcommand{\entropy}[1]{\ensuremath{{H}({#1})}\xspace}
\newcommand{\probability}[1]{\ensuremath{{Pr}({#1})}\xspace}


\newcommand{\lb}[1]{\ensuremath{lb({#1})}}


\newcommand{\nodes}[0]{\ensuremath{{\cal N}}}
\newcommand{\bud}[0]{\ensuremath{{\cal B}}}
\newcommand{\sequence}[0]{\ensuremath{{\cal S}}}

\newcommand{\maxd}[0]{\ensuremath{k}}
\newcommand{\anode}[0]{\ensuremath{i}}
\newcommand{\bnode}[0]{\ensuremath{b}}
\newcommand{\afeat}[0]{\ensuremath{f}}
\newcommand{\aclass}[0]{\ensuremath{l}}
\newcommand{\ub}[0]{\ensuremath{ub}}
% \newcommand{\error}[0]{\ensuremath{error}}
% \newcommand{\mdepth}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{depth}{depth({#1})}}}
\newcommand{\weight}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{w}{w({#1})}}}
\newcommand{\test}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{test}{test({#1})}}}
\newcommand{\dom}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{dom}{dom({#1})}}}
\newcommand{\best}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{best}}{{best({#1})}}}}
\newcommand{\maxe}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{max}}{{max({#1})}}}}
\newcommand{\opt}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{optimal}{optimal({#1})}}}
\newcommand{\child}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{child}}{{child({{#1}})}}}}
\newcommand{\error}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{error}}{{error({{#1}})}}}}
\newcommand{\classlabel}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{y}}{{y({{#1}})}}}}

\newcommand{\abranch}[0]{\ensuremath{b}}


\newcommand{\numex}[0]{\ensuremath{n}}
\newcommand{\numfeat}[0]{\ensuremath{m}}
\newcommand{\mdepth}[0]{\ensuremath{k}}



	\SetKwFunction{storebest}{storeBest}
	\SetKwFunction{solution}{solution}
	\SetKwFunction{deadend}{deadEnd}
	\SetKwFunction{expend}{expend}
	\SetKwFunction{backtrack}{backtrack}
	\SetKwFunction{dt}{BTDT}
	\SetKwFunction{pop}{pop}
	\SetKwFunction{push}{push}
	\SetKwFunction{prune}{prune}
	% \SetKwFunction{error}{error}
	\SetKwFunction{grow}{grow}
	\SetKwFunction{branch}{branch}
	\SetKwFunction{select}{select\&remove}
	\SetKwFunction{budalg}{Bud-first-search}
	\SetKwFunction{dynprog}{DynProg}
	\SetKwFunction{dleight}{DL8.5}
	\SetKwFunction{cart}{CART}
	\SetKwFunction{greedy}{BudFS (first sol.)}
  \SetKwFunction{murtree}{Murtree}
	\SetKwFunction{dominated}{Dominated}
	
	
	\newcommand{\nolb}[0]{No lower bound}
	\newcommand{\nopreprocessing}[0]{No preprocessing}
	\newcommand{\noheuristic}[0]{No heuristic}
	

\DontPrintSemicolon

\title{A Simple and Efficient Anytime Algorithm for Computing Optimal Decision Trees}

\author{Emir Demirovi\'c\inst{1} \and Emmanuel Hebrard\inst{2} \and Louis Jean\inst{2}}
\institute{TU DELFT, The Netherlands, email: e.demirovic@tudelft.nl
  \and LAAS-CNRS, Universit\'e de Toulouse, CNRS, France, email: \{hebrard,ljean\}@laas.fr}

\begin{document}


\maketitle






\begin{abstract}
	
\end{abstract}

\section{Introduction}

Ever since the pionneering work decision tree classifiers~\cite{breiman1984classification,10.1023/A:1022643204877}, the question of computing \emph{optimal} decision trees has been alive, and recently a number of algorithms have been introduced for that purpose.
Several variants and criteria have been proposed. %, e.g. computing the perfect classifier of minimum size~\cite{}, 
Among those, computing a depth-bounded decision tree of maximum accuracy is often the preferred criterion~\cite{bertsimas2017optimal,hu2019optimal,dl8,verhaeghe2019learning}, because while being straightforward, it captures many desired features: it is resilient to noisy data, and shallow trees are both easier to explain and less prone to overfitting.

\medskip

Despite the vast offer of exact methods that can potentially provide optimal decision trees, or at least should, given enough time, improve on heuristics, the latter are still vastly more commonly used in practice. Some methods are memory-hungry, some are not anytime, and as far as we know there is not a single algorithm that can provide optimal classifiers while scaling to large datasets, feature space, or tree depth as heuristics do.
% \begin{itemize}
% 	\item MinDT -> does not scale, problem with noise
% 	\item other SAT approaches -> do not scale in some way or another
% 	\item DL8 -> does not scale in memory, slower
% 	\item BinOCT -> ? (probably much slower)
%   \item Murtree -> by far the most efficient, however, does not scale as well on deep trees and not as anytime
% \end{itemize}



\medskip

In this paper we introduce a relatively \emph{simple} algorithm, that is as memory and time efficient as heuristics, and yet more efficient than most exact methods on most datasets. 
This algorithm can be seen as an instance of the more general framework introduced in \cite{DBLP:journals/corr/abs-2007-12652}, however tuned to have the best scalability to large trees and the best anytime behavior as possible.

In a nutshell, this algorithm (\budalg) emulates the dynamic programming algorithm \dleight~\cite{dl8}, while expending non-terminal branches (a.k.a ``buds'') first. As a result, this algorithm is in a sense strictly better than both the standard dynamic programming approach (because it is anytime and at least as fast) and than classic heuristics (because it emulates them during search, without significant overhead).
%but explores the search space so as to improve its anytime behaviour.
Our experimental results show that it outperforms the state of the art, to the exception of \murtree~\cite{DBLP:journals/corr/abs-2007-12652} on relatively shallow trees (typically for maximum depth up to 4), for which this its more sophisticated (albeit complex) algorithmic features can pay off.
In particular, on datasets that \dleight can tackle, \budalg can always find classifiers at least as accurate faster, and when the former can prove optimality, the latter does it orders of magnitude faster.

%
%
% \begin{itemize}
% 	\item Same worst-case complexity than DL8
% 	\item No memory usage
% 	\item Better anytime behaviour than DL8 (in fact as good as the state of the art heuristics)
% 	\item Therefore, strictly better than greedy heuristics and almost always better than DL8
% \end{itemize}
%
%
% We consider the problem of finding the bounded-depth decision tree of maximum accuracy.
% The state of the art includes MIP approaches (BinOCT), a MaxSAT approach based on the SAT encoding proposed by Narodytska et al, and DL8.5.
% The latter algorithm is by far the most efficient, however, it is not \emph{anytime}: the left branch must be optimally solved before a solution of the right branch can be found. Moreover the use of a cache structure means that it uses a lot of memory. This algorithm is practical for a maximum depth of 4 (although using gigabytes of memory) but often not much beyond. Therefore, in a number of cases, a greedy heuristic (such as CART) is still the best method in practice.
%
% \medskip

% In this note we introduce what is essentially an anytime version of DL8.5, without cache.
% This algorithm therefore uses linear (in the size of the tree) memory and anytime, hence in principle strictly better than CART. Moreover, on instance where DL8.5 can find a solution, the algorithm described in this note is significantly faster (by about a factor 10).

\section{Preliminaries}

A binary data set is a pair $\langle \negex,\posex \rangle$ where the negative datapoints $\negex$ (resp. the positive datapoints $\posex$) is a subset of $2^{\features}$. It is associated a label function $\classlabel : 2^{\features} \mapsto \{\top,\bot\}$ such that:
$$
\forall \aclass \in \{\top,\bot\}, \forall \ex \in \setex{\aclass}, \classlabel[\ex] = \aclass
$$
We refer the union of negative and positive datapoints by $\allex = \negex \cup \posex$.
%$\top$ and $\bot$ are class labels and 
A datapoint $\ex$ can equivalently be seen as a subset of $\features$, or as the conjunction:
$$
\bigwedge_{\afeat \in \ex}\afeat \wedge \bigwedge_{\afeat \in \features \setminus \ex}\bar{\afeat}
$$


A \emph{binary decision tree} is a tree whose leaves are labelled with either $\top$ or $\bot$, internal vertices are labelled with features and edges are labelled with either $0$ or $1$.

To a \emph{branch} of a decision tree we can associate the conjunction of features which are on the path from the root to a leaf.
If the feature vertex is exited by a $1$-edge, the feature is positive in the conjunction, otherwise it is negative.

 
%Moreover, if we also consider data points as conjunctions of features (where every feature appears either positively or negatively),
%given a branch $\abranch \subseteq \features$ 
Given a data set $\langle \negex,\posex \rangle$, we can associate a data set $\langle \negex(\abranch),\posex(\abranch) \rangle$ to a branch $\abranch$ where:
\begin{eqnarray*}
\negex(\abranch) = \{\ex \mid \ex \in \negex, \ex \models \abranch\}\\
\posex(\abranch) = \{\ex \mid \ex \in \posex, \ex \models \abranch\}
\end{eqnarray*}

We write $\error[\abranch]$ for $\min(|\negex(\abranch)|, |\posex(\abranch)|)$, and $\error[\abranch,\afeat]$ for $\error[\abranch \wedge \afeat] + \error[\abranch \wedge \bar{\afeat}]$.
A branch \abranch\ is said \emph{pure} iff $\error[\abranch]=0$.

\medskip

Given a binary dataset $\langle \negex,\posex \rangle$ with label function $\classlabel$,
the \emph{minimum error bounded depth decision tree problem} consists in finding the minimum value $\epsilon$ such
there exists a binary tree of depth at most $\maxd$ whose sum of error $\error[\abranch]$ for all branches $\abranch$ from the root to the leaves is equal to $\epsilon$.


\subsection{Dynamic Programming Algorithm}

The solver DL8.5 is a dynamic programming algorithm. It relies on the observation that given a feature test, the two resulting branches are independent subproblems. Algorithm~\ref{alg:dynprog} gives a high level view of DL8.5.


	% \begin{algorithm}
	% 	\caption{Dynamic Programming Algorithm\label{alg:dynprog}}
	% 	\TitleOfAlgo{\dynprog}
	% 	  \KwData{$\negex,\posex,\maxd,\abranch[=(\tru)]$}
	% 	  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
	% 		\lIf{$\maxd = 0$ or $\error[\abranch] = 0$} {
	% 		\Return $\error[\abranch]$
	% 		}
	% 		$\best \gets \error[\abranch]$\;
	% 		\ForEach{$\afeat \in \features \setminus \abranch$} {
	% 				$\best \gets \min(\best, \dynprog(\negex,\posex, \abranch \wedge \afeat, \maxd-1) + \dynprog(\negex,\posex, \abranch \wedge \bar{\afeat}, \maxd-1))$\;
	% 		}
	% 		\Return $\best$\;
	% \end{algorithm}
	
	\begin{algorithm}
		\caption{Dynamic Programming Algorithm\label{alg:dynprog}}
		\TitleOfAlgo{\dynprog}
		  \KwData{$\negex,\posex,\features,\maxd$}
		  \KwResult{The minimum error on $\negex,\posex,\features$ for decision trees of depth $\maxd$}
			$\error \gets \min(|\negex|,|\posex|)$\;
			
			\lIf{$\maxd = 0$ or $\error = 0$} {
			\Return $\error$
			}
			
			\ForEach{$\afeat \in \features$} {
					$\error \gets \min(\error, \dynprog(\negex(\afeat),\posex(\afeat),\features \setminus \{\afeat\},\maxd-1)$\;
					\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  $ + \dynprog(\negex(\bar{\afeat}),\posex(\bar{\afeat}),\features \setminus \{\afeat\},\maxd-1))$\;
			}
			\Return $\error$\;
	\end{algorithm}
	
	
	Let $\numex = |\posex| + |\negex|$, $\numfeat = |\features|$, and let $\mdepth$ be the maximum depth.
	We can safely assume $\mdepth \leq \numfeat$ (otherwise all features can be tested on every branch) and $\mdepth \leq \log \numex$ (otherwise we could have one distinct branch per datapoint), and it is often assumed that
	 $\mdepth \ll \numfeat$ and $\mdepth \ll \log \numex$. 
	%The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta(2^{\mdepth-1}{\numfeat \choose \mdepth})$, to explore at most ${\numfeat \choose \mdepth}$ combinations of features for at most $2^{\mdepth-1}$ branches. 
	%The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta(2^{\mdepth}{\numfeat \choose \mdepth})$, to explore $\Theta({\numfeat \choose \mdepth})$ combinations of features for $\Theta(2^{\mdepth})$ branches. 
	The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta((2\numfeat)^{\mdepth})$. 
	Moreover, at each call, the data set must be split into two subsets. However, this splitting procedure can be done in time $\Theta(\numex)$ amortised over a given level of a particular decision tree, since the data sets associated to branches up to a given level form a partition of the original data set.
	Therefore, since Algorithm~\ref{alg:dynprog} independently explores $\Theta({\numfeat \choose \mdepth})$ sets of branches of depth $\mdepth$, it runs in
	$\Theta((\numex+2^\mdepth){\numfeat \choose \mdepth})$ time (hence $O(\numex\numfeat^\mdepth)$ time, since we suppose $\mdepth \ll \numfeat$ and $\mdepth \ll \log \numex$).
	
	Note that this is a significant improvement with respect to the $\Theta(\numfeat^{2^{\mdepth}})$ trees (with redundant branches) explored by a brute-force algorithm.
	 %
	 % a brute-force algorithm, since the total number of decision trees of depth $\mdepth$ with $\numfeat$ attributes is $\prod_{x=1}^{\mdepth}(1-x+\numfeat)^{2^{\mdepth}}$. If we assume $\mdepth \in O(1)$, this is $\Theta(\numfeat^{2^{\mdepth}})$ distinct trees.
	 %



\section{An Anytime Algorithm}

In a nutshell the algorithm we propose works as follows:

\medskip

We start from a singleton set \bud\ of open branches or \emph{buds}.
% (open branches are branches of length strictly less than $\maxd$ that are not pure). % The set \nodes\ contains all the nodes of the current tree, open or closed, it is initially equal to \bud. Finally,
Moreover, we use an initially empty stack of decisions $\sequence$.

\begin{itemize}
	\item As long as there is a bud ($\bud \neq \emptyset$), we pick any one $\abranch \in \bud$ and check if it can be expended in Line~\ref{line:leaves}. If its length is $\mdepth$ the error at this leaf is recorded in $\best[\abranch]$. Otherwise, we pick a feature $\afeat$ marked as \emph{available} for \abranch, add the pair $(\abranch,\afeat)$ to \sequence\ and expend the tree with the two branches $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$. 
	%
	%
	%
	%
	%  a feature $\afeat$ marked as \emph{available} for \abranch, add the pair $(\abranch,\afeat)$ to \sequence\ and expend the tree with the two branches $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$.
	% %These new nodes are added to $\nodes$.
	% They are added to $\bud$ if their depth is strictly less than $\maxd-1$ (the last feature test is chosen according to minimum error) and if they are not pure, otherwise they are terminal tests and we record the corresponding error.

\item If there is no bud ($\bud = \emptyset$), then the tree is complete: every branch is of size $\mdepth$ or is pure. In that case we pop the last assignment $(\abranch,\afeat)$ from \sequence, mark the feature $\afeat$ not available for branch $\abranch$ and update the recorded best error of its subtrees. If there is at least one available feature for branch $\abranch$, we add $\abranch$ to $\bud$. 
Otherwise, we consider it \emph{terminal} and its error is the sum of the error of its best subtrees. This branch will never be expended anymore since it is not added to $\bud$.
% and we 
%store the minimum error recorded for any of the possible features.

\end{itemize}


Notice that to simplify the pseudo-code, we use branches to directly index array-like data structures in Algorithm~\ref{alg:bud}. Actually, a set of \emph{indices} (at most $2^{\mdepth}$ of them are necessary) are used as proxy for branches in all contexts. At Line~\ref{line:storebest}, the indices for $\abranch \wedge \afeat$, $\abranch \wedge \bar{\afeat}$ are released, and a free index is marked as used when expending a branch at Line~\ref{line:branching}. Moreover, the pseudo-code in Algorithm~\ref{alg:bud} does show how the best subtrees of terminal branches are recorded, nor how the overall best error is updated.



%The classification error of a tree is equal to the sum of the error of its terminal banches, the algorithm returns the minimum value encountered when exploring the possible decision trees.
%In other words, this algorithm will first build a complete tree by expanding non-pure, non-maximal branches in any order. When all leaves are pure of a maximal depth, the last test of the last expanded branch w



% \medskip
		

		\begin{theorem}
			The time complexities of Algorithms~\ref{alg:dynprog} and Algorithm~\ref{alg:bud} are equal.
			\end{theorem}
			
			\begin{proof}[sketch]
				The proof follows from the fact that both algorithm explore exactly the same set of branches, albeit not in the same order.
				Both algorithm eventually explore every combination of (negated) feature of size $\mdepth$.
				
				We show that this is true for Algorithm~\ref{alg:bud} by recursion on the maximum depth $\mdepth$.
				For $\mdepth = 0$, both algorithm return $\error[\emptyset]=\min(|\negex|, |\posex|)$.
				
				Now suppose that for $\mdepth \leq d$ every branch $\abranch$ explored by \dynprog (a recursive call ends on this branch) is also explored by \budalg ($\best[\abranch]$ is set in Line~\ref{line:best}), and let $\mdepth=d+1$. Without loss of generality, we can take an arbitrary branch $\abranch$ of length $d$ and show that the same extentions of $\abranch$ are explored by both algorithms.
				If $\error[\abranch]=0$, then no extension of $\abranch$ is explored by either algorithm.
				Otherwise, \dynprog explores every branch $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$ for $\afeat \in \features \setminus \abranch$. Since $\abranch$ is explored when $\mdepth = d$ and since $\error[\abranch] \neq 0$, then $\abranch$ will fail the test on Line~\ref{line:leaves} and for a feature $\afeat \in \features \setminus \abranch$, the branches $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$ will be explored, and the pair $(\abranch, \afeat)$ will be added to $\sequence$. 
				Moreover, the algorithm will not stop until $\sequence$ is not empty, hence $(\abranch, \afeat)$ will be poped out of $\sequence$, $\afeat$ removed from $\dom[\abranch]$ and $\abranch$ reinserted into $\bud$. Therefore, for every $\afeat \in \features \setminus \abranch$, $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$ will eventually be explored exactly once.
				\hfill$\square$
			\end{proof}
			
			\medskip
			
			The key difference between Algorithms~\ref{alg:dynprog} and \ref{alg:bud} is the order in which branches are explored (see Figure~\ref{fig:searchtree}). In particular, \dynprog must complete the first recursive call before outputing a full tree. Therefore, the computation time before to find a first tree is $\Theta((\numex+2^{\mdepth-1}){\numfeat-1 \choose \mdepth-1})$, that is $O(\numex(\numfeat-1)^{\mdepth-1})$ time. On the other hand, \budalg finds a first tree in $\Theta(2^{\mdepth}+\numex\numfeat) = \Theta(\numex\numfeat)$ time.
			
			
		
			
			
			
% 			% First, notice that the task of splitting the data set on eevry branch of the search tree can be done exactly as in DL8, that is, in $\Theta(\numex)$ amortised time for each of the $\mdepth$ tree levels.
%
% 			% \begin{proof}
%
% 			First, all branches eventually reached. Consider an arbitrary branch $\abranch$.
%
%
% 			\medskip
%
%
% 			$\best[\abranch]$ is the minimal error of any subtree rooted at $\abranch$ with a feature in $\features \setminus \dom[\abranch]$ is $\best[\abranch]$.
% 			This is true for pure branches
%
%
% 			 and maximum-depth branches (code after condition in Line~\ref{line:leaves}). Now consider a branch $\abranch$ such that $|\abranch|<\mdepth$ and $\forall \afeat \in \features, \error[\abranch,\afeat] > 0$.
%
% 			\medskip
%
%
%
%
% 			Let a branch $\abranch$ be \emph{explored} iff it was put in the stack $\sequence$ and $\dom[\abranch] = \emptyset$.
% 			If a branch $\abranch$ is explored, then $\best[\abranch]$ is the minimal error of any subtree rooted at $\abranch$.
% 			This is true for pure branches and maximum-depth branches (code after condition in Line~\ref{line:leaves}). Now consider a branch $\abranch$ such that $|\abranch|<\mdepth$ and $\forall \afeat \in \features, \error[\abranch,\afeat] > 0$.
%
% 			\medskip
%
%
%
% 				First, we show that the algorithm is correct. In particular the following property holds:
% 				after Line~\ref{line:storebest}, the minimum error of any subtree rooted at $\abranch$ with a feature in $\features \setminus \dom[\abranch]$ is $\best[\abranch]$.
%
%
% 				Observe that $|\abranch| \leq \mdepth-2$. Indeed, no pair $(\abranch,\afeat)$ is put on $\sequence$ unless $|\abranch| \leq \mdepth-2$. Now suppose that $|\abranch|=\mdepth-2$. Then $\best[\abranch \wedge \afeat]$ is by definition the minimal error of any single-node tree rooted at $\abranch \wedge \afeat$ and likewise, $\best[\abranch \wedge \bar{\afeat}]$ is the minimal error of any single-node tree rooted at $\abranch \wedge \bar{\afeat}$. Therefore, $\best[\abranch \wedge \afeat] + \best[\abranch \wedge \bar{\afeat}]$ is the minimal error of a depth 2 tree rooted at $\abranch$ with a test on $\afeat$. Since all features in $\features \setminus \dom[\abranch]$ have been tried (or belong to $\abranch$) and the minimum was kept, the property holds.
%
% 				Suppose now that the property holds for $|\abranch|=\mdepth-x$ with $x>2$. Then by the same reasoning as above, the property will also hold for $|\abranch|=\mdepth-x-1$. Therefore is always hold.
%
%
%
% 			\medskip
%
%
%
%
%
%
% 			The key is to observe that given a branch $\abranch$ and a feature $\afeat$, just as in DL8, the complexity of computing
% 			$\error[\abranch,\afeat]$ is equal to the complexity of computing $\error[\abranch \wedge \afeat]$ plus the complexity of computing $\error[\abranch \wedge \bar{\afeat}]$.
% 			Indeed, wlog, let the branch $\abranch \wedge \afeat$ be chosen before $\abranch \wedge \bar{\afeat}$.
% 				Both branches will be completed up to the maximal depth, however,
% 			the test appended to $\abranch \wedge \afeat$ will no change until
%
%
%
%
% 			the best possible errors for $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$
%
%
% 			% \end{proof}
%
% 			We first show that the number of assignment of tests (Line~\ref{line:assignment}) is $2^{\mdepth-1}\numfeat^{\mdepth-1}$.
% 			For $\mdepth=1$ this is true since there is a single assigned test (with the feature $\argmin_{\afeat \in \features}(\error[\emptyset,\afeat])$).
%
% 			Now suppose that the property holds for depth $\mdepth-1$ and consider depth $\mdepth$.
%
%
%
%
% 			No proof is given, but independent subtrees are not explored in Algorithm~\ref{alg:bud} hence both algorithms do the same computation, except not in the same order. Notice that Algorithm~\ref{alg:bud} eagerly compute the conditional error $\error[\abranch,\afeat]$ for every feature $\afeat \in \features$, which incurs an extra factor $\numfeat$ for the data set partitionning task. However, in return the last test of each branch is chosen in $O(1)$ so we gain the same factor $\numfeat$.
%
% 			The big difference is that whereas the cost of finding a first solution is $\Theta((\numex + 2^{\mdepth-1})\numfeat^{\mdepth-1})$ for Algorithm~\ref{alg:dynprog}, it is equal to $\Theta(\mdepth\numex + 2^\mdepth)$ for Algorithm~\ref{alg:bud}, which in practice is very important, as shown in the experimental section.
%
% 	%T(m,k) = 2mT(m-1, k-1)
%
%
%
%
% % \clearpage





\begin{algorithm}
\begin{footnotesize}
		\caption{Blossom Algorithm\label{alg:bud}}
		\TitleOfAlgo{\budalg}
		  \KwData{$\negex,\posex, \maxd$}
		  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth $\maxd$}
		$\sequence \gets []$\;
		$\bud \gets \{\emptyset\}$\;
		$\error \gets \min(|\negex|,|\posex|)$\;
		$\dom \gets (\lambda : {2^{\features}} \mapsto \features)$\;
		$\best \gets (\lambda : {2^{\features}} \mapsto \infty)$\;
		
		\While{$|\sequence| + |\bud| > 0$}{
		\lnl{line:dive}\eIf{$\bud \neq \emptyset$}{
			%$\abranch \gets \select{\bud}$\;
			\lnl{line:budchoice}pick and remove $\abranch$ from $\bud$\;
			
			\lnl{line:leaves}\eIf{$|\abranch| = \maxd$ or $\error[\abranch] = 0$} {
				% $\error \gets \error + \error[\abranch,\afeat]$\;
				\lnl{line:best}$\best[\abranch] \gets \error[\abranch]$\;
			}{
			\lnl{line:assignment} pick and remove $\afeat$ from $\dom[\abranch]$\;
			% $\dom[\abranch] \gets \dom[\abranch] \setminus \{\afeat\}$\;
			push $(\abranch,\afeat)$ on $\sequence$\; 
			\lnl{line:branching}\lForEach{$v \in \{\afeat, \bar{\afeat}\}$}{
						$\bud \gets \bud \cup \{\abranch \wedge v\}$
					}
			}
		}{
			% $\best[\emptyset] \gets \min(\best[\emptyset], \error)$\;
			\lnl{line:backtrack}\Repeat{$\dom[\abranch] \neq \emptyset$ or $|\sequence|=0$}{
				pop $(\abranch,\afeat)$ from $\sequence$\;
				\lnl{line:storebest}$\best[\abranch] \gets \min(\best[\abranch], \best[\abranch \wedge \afeat] +  \best[\abranch \wedge \bar{\afeat}])$\;
				% $\error \gets \error - \best[\abranch \wedge \afeat] -  \best[\abranch \wedge \bar{\afeat}]$\;
				\lIf{$\dom[\abranch] \neq \emptyset$} {
					$\bud \gets \bud \cup \{\abranch\}$
					% \lIf{$\opt[\abranch]$}{$\error \gets \error - \best[\abranch]$}
				} 
				% \lElse {
				% 	$\error \gets \error + \best[\abranch]$%$\error[\abranch,\afeat]$
				% }
				% {
				% 	$\opt[\abranch] \gets \tru$\;
				% }
			}
		}
		}
		\Return $\best[\emptyset]$\;
	\end{footnotesize}
	\end{algorithm}
	
	
	% \begin{algorithm}
	% 	\caption{Anytime Algorithm\label{alg:bud}}
	% 	\TitleOfAlgo{\budalg}
	% 	  \KwData{$\negex,\posex, \maxd$}
	% 	  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
	% 	$\sequence \gets []$\;
	% 	$\bud \gets \{\emptyset\}$\;
	% 	$\error \gets \min(|\negex|,|\posex|)$\;
	% 	$\dom \gets (\lambda : {2^{\features}} \mapsto \features)$\;
	% 	$\best \gets (\lambda : {2^{\features}} \mapsto \infty)$\;
	% 	% $\opt \gets (\lambda : {2^{\features}} \mapsto \fal)$\;
	%
	% 	\While{$|\sequence| + |\bud| > 0$}{
	% 	\eIf{$\bud \neq \emptyset$}{
	% 		%$\abranch \gets \select{\bud}$\;
	% 		pick and remove $\abranch$ from $\bud$\;
	% 		\lnl{line:assignment}$\afeat \gets \argmin_{\afeat \in \dom[\abranch]}(\error[\abranch,\afeat])$\;
	% 		\lnl{line:leaves}\eIf{$\error[\abranch,\afeat] = 0$ or $|\abranch| = \maxd-1$} {
	% 			$\error \gets \error + \error[\abranch,\afeat]$\;
	% 			$\best[\abranch] \gets \error[\abranch,\afeat]$\;
	% 			$\dom[\abranch] \gets \emptyset$\;
	% 			% $\opt[\abranch] \gets \tru$\;
	% 		}{
	% 		$\dom[\abranch] \gets \dom[\abranch] \setminus \{\afeat\}$\;
	% 		push $(\abranch,\afeat)$ on $\sequence$\; % $ \gets \sequence \oplus (\abranch,\afeat)$\;
	% 		\ForEach{$v \in \{\afeat, \bar{\afeat}\}$}{
	% 				\lIf{$\error[\abranch,v] > 0$}{
	% 					$\bud \gets \bud \cup \{\abranch \wedge v\}$
	% 				}
	% 		}
	% 		}
	% 	}{
	% 		$\best[\emptyset] \gets \min(\best[\emptyset], \error)$\;
	% 		\Repeat{$\dom[\abranch] \neq \emptyset$ or $|\sequence|=0$}{
	% 			pop $(\abranch,\afeat)$ from $\sequence$\;
	% 			\lnl{line:storebest}$\best[\abranch] \gets \min(\best[\abranch], \best[\abranch \wedge \afeat] +  \best[\abranch \wedge \bar{\afeat}])$\;
	% 			$\error \gets \error - \best[\abranch \wedge \afeat] -  \best[\abranch \wedge \bar{\afeat}]$\;
	% 			\lIf{$\dom[\abranch] \neq \emptyset$} {
	% 				$\bud \gets \bud \cup \{\abranch\}$
	% 				% \lIf{$\opt[\abranch]$}{$\error \gets \error - \best[\abranch]$}
	% 			}
	% 			\lElse {
	% 				$\error \gets \error + \best[\abranch]$%$\error[\abranch,\afeat]$
	% 			}
	% 			% {
	% 			% 	$\opt[\abranch] \gets \tru$\;
	% 			% }
	% 		}
	% 	}
	% 	}
	% 	\Return $\best[\emptyset]$\;
	% \end{algorithm}
	
	
	
	\begin{figure}
	\begin{center}
		\tabcolsep=0pt
		\scalebox{1}{
			\begin{forest}
				for tree={%
					l sep=20pt,
					s sep=3.5pt,
					node options={shape=rectangle, minimum width=10pt, inner sep=0pt, font=\footnotesize},
		  		edge={thick, -latex, shorten >=1pt, shorten <=1pt},
				}
				[{$\emptyset$}
					[{$a$}
						[{\begin{tabular}{c}$b$\\1\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\2\end{tabular}}]
						[{\begin{tabular}{c}$c$\\7\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\8\end{tabular}}]
					]
					[{$\bar{a}$}
						[{\begin{tabular}{c}$b$\\3\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\4\end{tabular}}]
						[{\begin{tabular}{c}$c$\\5\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\6\end{tabular}}]
					]
					[{$b$}
						[{\begin{tabular}{c}$a$\\9\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\10\end{tabular}}]
						[{\begin{tabular}{c}$c$\\15\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\16\end{tabular}}]
					]
					[{$\bar{b}$}
						[{\begin{tabular}{c}$a$\\11\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\12\end{tabular}}]
						[{\begin{tabular}{c}$c$\\13\end{tabular}}]
						[{\begin{tabular}{c}$\bar{c}$\\14\end{tabular}}]
					]
					[{$c$}
						[{\begin{tabular}{c}$a$\\17\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\18\end{tabular}}]
						[{\begin{tabular}{c}$b$\\23\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\24\end{tabular}}]
					]
					[{$\bar{c}$}
						[{\begin{tabular}{c}$a$\\19\end{tabular}}]
						[{\begin{tabular}{c}$\bar{a}$\\20\end{tabular}}]
						[{\begin{tabular}{c}$b$\\21\end{tabular}}]
						[{\begin{tabular}{c}$\bar{b}$\\22\end{tabular}}]
					]
				]
			\end{forest}
		}
	\end{center}
	\caption{\label{fig:searchtree} The search tree for decision trees. \texttt{DynProg} explores it depth first, whereas \texttt{Bud-first-search} explores branches in the order given below the leaves.}
	%\caption{\label{fig:searchtree} The search tree for decision trees. \dynprog explores it depth first, whereas \budalg explores branches in the order given below the leaves.}
	\end{figure}
	
	
	
	
	% \begin{figure}
	% \begin{center}
	% 	\tabcolsep=0pt
	% 	\scalebox{1}{
	% 		\begin{forest}
	% 			for tree={%
	% 				l sep=20pt,
	% 				s sep=3.5pt,
	% 				node options={shape=rectangle, minimum width=10pt, inner sep=0pt, font=\footnotesize},
	% 	  		edge={thick, -latex, shorten >=1pt, shorten <=1pt},
	% 			}
	% 			[{$\emptyset$}
	% 			 [{$a,\bar{a}$}
	% 			  [{$a \wedge b$,$a \wedge \bar{b}$}
	% 					[{$a \wedge b \wedge c$,$a \wedge b \wedge \bar{c}$}]
	% 					[{$a \wedge \bar{b} \wedge c$,$a \wedge \bar{b} \wedge \bar{c}$}]
	% 				]
	% 			  [{$\bar{a} \wedge b$,$\bar{a} \wedge \bar{b}$}]
	% 			 ]
	% 			 [{$b$}]
	% 			 [{$c$}]
	% 			]
	% 		\end{forest}
	% 	}
	% \end{center}
	% \caption{\label{fig:searchtree} The search tree for decision trees. \texttt{DynProg} explores it depth first, whereas \texttt{Bud-first-search} explores branches in the order given below the leaves.}
	% %\caption{\label{fig:searchtree} The search tree for decision trees. \dynprog explores it depth first, whereas \budalg explores branches in the order given below the leaves.}
	% \end{figure}
	
	
	% \begin{tabular}{c|ll}
	% 	\# & \bud & \sequence \\
	% 	\hline
	% 	1 & $\{\emptyset\}$ & $[]$ \\
	% 	2 & $\{a,\bar{a}\}$ & $[(\emptyset,a)]$ \\
	% 	3 & $\{a \wedge b,a \wedge \bar{b},\bar{a}\}$ & $[(\emptyset,a),(a,b)]$ \\
	% 	4 & $\{a \wedge \bar{b},\bar{a}\}$ & $[(\emptyset,a),(a,b)]$ \\
	% 	5 & $\{\bar{a}\}$ & $[(\emptyset,a),(a,b)]$ \\
	% 	6 & $\{\bar{a} \wedge b, \bar{a} \wedge \bar{b}\}$ & $[(\emptyset,a),(a,b),(\bar{a},b)]$ \\
	% 	7 & $\{\bar{a} \wedge \bar{b}\}$ & $[(\emptyset,a),(a,b),(\bar{a},b)]$ \\
	% 	8 & $\{\}$ & $[(\emptyset,a),(a,b),(\bar{a},b)]$ \\
	% 	9 & $\{\bar{a} \wedge c, \bar{a} \wedge \bar{c}\}$ & $[(\emptyset,a),(a,b),(\bar{a},c)]$ \\
	% 	10 & $\{\bar{a} \wedge \bar{c}\}$ & $[(\emptyset,a),(a,b),(\bar{a},c)]$ \\
	% 	11 & $\{\}$ & $[(\emptyset,a),(a,b),(\bar{a},c)]$ \\
	% 	12 & $\{a \wedge c, a \wedge \bar{c}\}$ & $[(\emptyset,a),(a,c)]$ \\
	% 	13 & $\{a \wedge \bar{c}\}$ & $[(\emptyset,a),(a,c)]$ \\
	% 	14 & $\{\}$ & $[(\emptyset,a),(a,c)]$ \\
	% \end{tabular}


% For readability, we cut the algorithm into four blocks. The initialisation procedure (Algorithm~\ref{alg:init}) set up the data structures used in all other procedures:
% \begin{itemize}
% 	\item \sequence\ is simply the list of nodes in the current tree, ordered as they are explored.
% 	\item \nodes\ is the set of integers used to index a node of the current tree
% 	\item \bud\ is the set of nodes which do no have an assigned test yet
% 	\item \mdepth\ stores the depth of a node
% 	\item \test\ stores the feature tested at a node
% 	\item \dom\ stores the set of possible features which have no yet been tried for this node
% 	\item \best\ stores the error of the best subtree rooted at a node
% 	\item \opt\ indicates whether the best subtree of a given node is optimal
% 	\item \child\ stores the children of a node (children can be nodes or $\{\top, \bot\}$)
% 	\item $\error{\anode}$ $\min(|\posex(\anode)|,|\negex(\anode)|)$
% 	\item $\error{\anode,\afeat}$ $\min(|\posex(\anode=\afeat)|,|\negex(\anode=\afeat)|)$
% \end{itemize}
%
% Algorithm~\ref{alg:search} is a bactracking procedure which expends a current decision tree
%
% 	\begin{algorithm}
% 		\caption{Data Structures\label{alg:init}}
% 		\TitleOfAlgo{Initialise}
% 		$\sequence \gets []$\;
% 		$\bud \gets \emptyset$\;
% 		$\nodes \gets \emptyset$\;
% 		$\ub \gets \min(|\negex|,|\posex|)$\;
% 		$\error \gets ub$\;
%
% 		$\child \gets (\lambda : \mathbb{N} \times \{\fal, \tru\} \mapsto \emptyset)$\;
% 		$\mdepth \gets (\lambda : \mathbb{N} \mapsto 0)$\;
%
% 		$\test \gets (\lambda : \mathbb{N} \mapsto \emptyset)$\;
% 		$\dom \gets (\lambda : \mathbb{N} \mapsto \features)$\;
%
% 		$\best \gets (\lambda : \mathbb{N} \mapsto \infty)$\;
% 		$\opt \gets (\lambda : \mathbb{N} \mapsto \fal)$\;
% 	\end{algorithm}
%
%
%
% 	\begin{algorithm}
% 		\caption{Create a new node after branching\label{alg:alloc}}
% 		\TitleOfAlgo{\grow}
% 	  \KwData{integer \anode}
%
% 		$\nodes \gets \nodes \cup \{\anode\}$\;
% 		$\dom[\anode] \gets \features$ sorted by decreasing conditional error $\min(|\posex(\anode=\afeat)|,|\negex(\anode=\afeat)|)$\;
% 		$\test[\anode] \gets \pop(\dom[\anode])$\;
%
%
% 		\eIf{$\mdepth[\anode]=k-1$ or $\error{\anode,\test[\anode]}$}
% 		{
% 			$\best[\anode] = \error{\anode,\test[\anode]}$\; %\min(|\posex(\anode=\test[\anode])|,|\negex(\anode=\test[\anode])|)$\;
% 			$\opt[\anode] = \tru$\;
% 			\ForEach{$branch \in \{\tru, \fal\}$}
% 			{
% 				% $\child[\anode,branch] \gets (|\posex(\anode=\test[\anode])| > |\negex(\anode=\test[\anode])|)$\;
% 				\lIf{$|\posex(\anode=\test[\anode])| > |\negex(\anode=\test[\anode])|$}{$\child[\anode,branch] \gets \top$}
% 				\lElse{$\child[\anode,branch] \gets \bot$}
% 			}
% 		}
% 		{
% 			$\bud \gets \bud \cup \{n\}$\;
% 			$\best[\anode] = \min(|\posex(\anode)|, |\negex(\anode)|)$\;
% 			$\opt[\anode] \gets \fal$\;
% 		}
%
%
% 	\end{algorithm}
%
%
% 	\begin{algorithm}
% 		\caption{Suppress a node and all its descendants\label{alg:free}}
% 		\TitleOfAlgo{\prune}
% 	  \KwData{integer \anode}
%
% 		$\bud \gets \bud \setminus \{\anode\}$\;
% 		$\nodes \gets \nodes \setminus \{\anode\}$\;
%
% 		\ForEach{$branch \in \{\tru, \fal\}$}
% 		{
% 		\lIf{$\child[\anode,branch] \not\in \{\top, \bot\}$}
% 		{
% 			$\prune{\child[\anode,branch]}$
% 		}
% 		}
%
% 		\lIf{$\mdepth[\anode] = k-1$ or $\opt[\anode]$}{$error \gets error - \best[\anode]$}
%
% 	\end{algorithm}
%
%
% \begin{algorithm}
% 	\caption{Search loop\label{alg:search}}
%   \TitleOfAlgo{\dt}
%   \KwData{$\negex,\posex, k$}
%   \KwResult{A decision tree}
%
% 	$\bnode \gets 0$\;
% 	$\posex(1),\negex(1) \gets \posex, \negex$\;
% 	$\grow{\bud, \sequence, 1}$\;
%
% 	\While{\textbf{true}}{
% 		\eIf{$\bud = \emptyset$} {
% 			$\ub \gets \min(\ub,\error)$\;
% 			$deadend \gets \fal$\;
% 			\Repeat{$deadend$}{
% 				\lIf{$\bnode > 0$}{$\opt[\bnode] \gets \tru$}
% 				\lIf{$\bnode = 1$}{\Return}
% 				$\bnode \gets \pop{\sequence}$\;
% 				$\best[\bnode] \gets \min(\best[\bnode], \best(\child[\bnode,\tru]) + \best(\child[\bnode,\fal]))$\;
% 				$\test[\bnode] \gets \pop{\dom[\bnode]}$\;
% 				$\prune(\child[\bnode,\tru])$\;
% 				$\prune(\child[\bnode,\fal])$\;
%
% 				$deadend \gets \best[\bnode] = 0 ~\vee~ \dom[\bnode] = \emptyset$\;
% 				\If{$deadend$}
% 				{
% 				$\opt[\bnode] \gets \tru$\;
% 				$\error \gets \error + \error{\bnode}$\; %$\best[\bnode]$\;
% 				}
% 			}
% 			$\bud \gets \bud \cup \{b\}$\;
% 			$\error \gets \error + \min(|\posex(\bnode)|, |\negex(\bnode)|)$\;
% 		}
% 		{
% 			\If{$b = 0$}{
% 				$b=\select{\bud}$\;
% 				% $\bud \gets \bud \setminus \{b\}$\;
% 				$\push(\bnode,\sequence)$\;
% 			}
% 			$c_{\tru}, c_{\fal} = \argmin_{x,y}(\mathbb{N} \setminus \nodes)$\;
% 			$\posex(c_{\tru}),\negex(c_{\tru}),\posex(c_{\fal}),\negex(c_{\fal}) \gets \branch(\posex(\bnode),\negex(\bnode),\test[\bnode])$\;
% 			\ForEach{$branch \in \{\tru, \fal\}$}{
% 				\eIf{$\min(|\posex(c_{branch})|,|\negex(c_{branch})|) = 0$}
% 				{
% 					\lIf{$|\posex(c_{branch})|>|\negex(c_{branch})|$}{$\child[\bnode,branch] \gets \top$}
% 					\lElse{$\child[\bnode,branch] \gets \bot$}
% 				}{
% 					$\child[\bnode,branch] \gets c_{branch}$\;
% 					$\mdepth[c_{branch}] \gets \mdepth[\bnode]+1$\;
% 					$\grow(\bud, \sequence, c_{branch})$\;
% 				}
% 			}
% 			$\bnode \gets 0$\;
% 		}
% 	}
%
% \end{algorithm}

%\clearpage

\section{Extensions of the Algorithm}

The pseudo-code given in Algorithm~\ref{alg:bud} shows the basic structure of the algorithm. 
As discussed earlier, some important (but rather tedious) parts of the algorithms have been omitted, such as how the best subtrees are stored in Line~\ref{line:storebest} when the best classification score is updated.
We discuss here only improvements that have an impact on the efficiency of the algorithm.




\subsection{Heuristic Ordering}

In order to quickly find highly accurate trees, it is important to follow a heuristic. We experimented with three heuristics based on scores to minimize: \emph{error}, \emph{entropy}~\cite{10.1023/A:1022643204877}, and \emph{Gini impurity}~\cite{breiman1984classification}. 
Each of these heuristics associate a score to a feature $\afeat$ at a branch $\abranch$:
\begin{eqnarray}
	\textrm{error}: & \error[\abranch,\afeat] \\
	%\textrm{minimum entropy:} & \sum_{v \in \{\afeat,\bar{\afeat}\}} \frac{|\allex(\abranch \wedge v)|}{|\allex(\abranch)|} \cdot -\sum_{c \in \{\bot,\top\}} \frac{|\setex{c}(\abranch \wedge v)|}{|\setex{c}(\abranch)} \log_{2} \frac{|\setex{c}(\abranch \wedge v)|}{|\setex{c}(\abranch)|} \\
	\textrm{entropy:} & \sum\limits_{v \in \{\afeat,\bar{\afeat}\}} -p(v,\allex) \sum\limits_{c \in \{\bot,\top\}} p(v,\setex{c}) \log_{2} p(v,\setex{c}) \\
	\textrm{Gini impurity:} & 2 - \sum\limits_{v \in \{\afeat,\bar{\afeat}\}}\sum\limits_{c \in \{\bot,\top\}} p(v,\setex{c})^2
\end{eqnarray}
With $p(v,{\cal S}) = \frac{|{\cal S}(\abranch \wedge v)|}{|{\cal S}(\abranch)|}$.
%The minimum error is simply defined as 

The feature tests at Line~\ref{line:assignment} of Algorithm~\ref{alg:bud} are explored in non-decreasing order with respect to one of the scores above.


In the datasets we used, the Gini impurity was significantly better, and hence all reported experiment results are using Gini impurity unless stated otherwise. For branches of length $\mdepth-1$, however, we use the error instead. Indeed, the optimal feature $\afeat$ for a branch $\abranch$ that cannot be extended further is the one minimizing $\error[\abranch,\afeat]$. This means that we actually do not have to try other features for that node, which means that we effectively restrict search to branches of length $\mdepth-1$.


%We order the possible features for branch $\abranch$ in non-decreasing order with respect to a score above and 
%explore the features in that order in Line~\ref{line:assignment}.
Computing this order costs $\Theta(\numfeat \log \numfeat)$ for each of the $2^{\mdepth}\numfeat^{\mdepth}$ branches added to $\bud$ at Line~\ref{line:branching}. However, since the depth is effectively reduced by one, the resulting complexity 
%(excluding the time for splitting the dataset) 
is $O((\numex + 2^{\mdepth} \log \numfeat) \numfeat^{\mdepth})$. This very slight increase is often inconsequencial, since we often have $\numex \geq 2^{\mdepth} \log \numfeat$.

The feature ordering has a very significant impact on how quickly the algorithm can improve the accuracy of the classifier. Moreover, it also has an impact (though much less significant) on the computational time necessary to explore the whole search space and prove optimality, because of the lower bound technique detailed in the next section.


\subsection{Lower Bound}

It is possible to fail early using a lower bound on the error given prior decisions in the same way as \murtree, following the idea introduced in \cite{dl8}. The idea is that once some subtrees along a branch $\abranch$ are optimal and the sum of their errors is larger than the current upper bound (the best solution found so far) then there is no need to continue exploring branch $\abranch$. Line~\ref{line:leaves} can be changed to ``\textbf{If} $\bud \neq \emptyset ~\& \not\exists \abranch \in \bud, \dominated{\abranch}$ \textbf{then}''. %Notice that when a branch is ``pruned'' in this way, its 


%In this case, we can fail by forcing 


First, observe that $\best[\abranch]$ is an upper bound on the classification error for any subtree rooted at $\abranch$, since this value comes from an actual tree (of depth $\mdepth - |\abranch|$ for the dataset $\langle \negex(\abranch),\posex(\abranch) \rangle$). It is possible to propagate this upper bound to parent nodes efficiently (in $O(|\branch|)$ time). Here we assume that this is done every time the value $\best[\abranch]$ is actually updated, by recursively applying the same update procedure to the parent.


Now, when the loop in Line~\ref{line:backtrack} makes two or more iterations, it means that for the penultimate branch $\abranch$ popped out of \sequence, all possible subtrees have been explored, and therefore, $\best[\abranch]$ is also a lower bound on the classification error for any subtree rooted at $\abranch$. Let $\opt[\abranch]$ be 1 if $\abranch$ has been ``backtracked over'' in this way and 0 otherwise.


Let $\abranch$ be a bud in $\bud$. Trivially, if $|\abranch|=\mdepth-1$ then this branch will entail $\min\{\error[\afeat] \mid \afeat \in \dom[\abranch]\}$ misclassification.
However, for any parent $\abranch'$ of $\abranch$, we can define a lower bound $\lb{\abranch',\abranch}$, \emph{given the feature tests} $\abranch \setminus \abranch'$ as follows:
$$
\lb{\abranch',\abranch} = \sum\limits_{\abranch' \subset \abranch'' \wedge \afeat \subseteq \abranch}\opt[\abranch'' \wedge \bar{\afeat}]\best[\abranch'' \wedge \bar{\afeat}]
$$
This lower bound is correct as long as the branch $\abranch$ belongs to the decision tree.
The procedure $\dominated{\abranch}$ can therefore simply check, for all parent $\abranch'$ of $\abranch$ up until the root ($\emptyset$), whether $\lb{\abranch',\abranch} \geq \best[\abranch']$. As a result, a branch $\abranch$ which is guaranteed, by this reasoning, to never belong to a non-dominated tree will not be explored further.

\medskip

This reasoning will be more effective when good upper bounds are found early, hence the feature ordering heuristic discussed in the previous section has an impact. Moreover, the order in which buds are expended (the choice of branch in Line\ref{line:budchoice}) has an impact as well. We found that the simplest branch selection strategy was also the one giving the best results: we expend first the branch that was inserted into \bud\ first (i.e., \bud\ is \emph{FIFO}). One possible explanation is that by avoiding to unnecessarily ``jump'' to different parts of the decision tree, this strategy promotes optimizing sibling subtrees first, and therefore, deeper tree earlier.

% intuitivelly, one want to optimize the branches of the decision trees with the largest error first, in order to benefit from larger lower bounds earlier. To this end, it



 %for any feature test $\afeat \in \abranch \setminus \abranch'$, if 








%is $O((\numex + 2^{\mdepth}\numfeat \log \numfeat) \numfeat^{\mdepth-1})$, which is often better than 




% $$
% \sum_{v \in \{\afeat,\bar{\afeat\}} \frac{|\allex(\abranch \wedge v)|}{|\allex(\abranch)|} \cdot -\sum_{c \in \{\bot,\top\}} \frac{|\setex{c}(\abranch \wedge v)}{|\setex{c}(\abranch)} \log_{2} \frac{|\setex{c}(\abranch \wedge v)}{|\setex{c}(\abranch)}
% $$


% $$
% 2 - \sum\limits_{v \in \{\afeat,\bar{\afeat}\}}\sum\limits_{c \in \{\bot,\top\}} p(v,\setex{c})^2
% $$



\subsection{Preprocessing}

Finally, we use two preprocessing techniques, one on the dataset and one on the features. Although extremely straightforward (and probably not novel), they both have a significant impact.

\paragraph{Dataset reduction.}
It is easy to adapt \budalg (or most decision tree classifiers, actually) to handle weighted datasets by redefining the error as follows, given a weight function $\weight$ on $\allex$:
$$
\error[\abranch] = \min\left( \sum\limits_{x \in \negex(\abranch)}\weight[x], \sum\limits_{x \in \posex(\abranch)}\weight[x] \right)
$$
We can use the weighted version to handle noisy data, by merging duplicated datapoints and suppressing inconsistent datapoints.

Let $\weight_{\bot}$ (resp. $\weight_{\top}$) denote the number of occurrences of $x$ in $\setex{\bot}$ (resp. $\setex{\top}$). We use the weight function $\weight[x] = |\weight_{\bot}(x) - \weight_{\top}(x)|$. Then, for any datapoint $x$, we remove all but one of its occurrences, in $\setex{\bot}$ if $\weight_{\bot}>\weight_{\top}$, in $\setex{\top}$ if $\weight_{\top}>\weight_{\bot}$, and suppress it completely if $\weight_{\top}=\weight_{\bot}$.
The reported error will then need to be offset by the number of pair of suppressed inconsistant datapoints, that is:
$
\sum_{x \in \allex}\min(\weight_{\bot}(x), \weight_{\top}(x))
$.
Reducing the number of datapoints in the dataset has a non-null, although tiny impact on efficiency. However, suppressing inconsistent datapoints is very important. In particular, proving optimality when the minimum error is positive basically requires to exhaust the search space and is therefore extremely costly. On the other hand, when there exists a perfect tree within the maximum depth, we can stop as soon as we find it. This preprocessing allows to benefit from that when we find a tree whose error is equal to the number of pair of inconsistent datapoints in the original dataset.
This preprocessing can be done in $O(\numfeat \numex \log \numex)$ by ordering the datapoints in lexicographic order and then processing them in sequence.

\paragraph{Feature reduction.}

A feature $\afeat$ is redundant if there exists a feature $\afeat'$ such that either: $\forall x \in \allex, \afeat \in x \iff \afeat' \in x$, or $\forall x \in \allex, \afeat \in x \iff \afeat' \in x$. We simply remove such redundant features. This can be done by comparing pairs of rows of the dataset via bitset operations, and therefore in time $O(\numex\numfeat^2)$.

This may appear very naive, however, it turns out that the binarization techniques (one-hot encoding) used to turn general datasets into binary dataset are often not optimized and many redundant features do exist. The number of features ($\numfeat$) has a huge impact on the complexity of the algorithm since the branching factor in the tree representing the search space is $2\numfeat$ (see Figure~\ref{fig:searchtree}).

Moreover, at every branch, ``informationless'' features (i.e., features $\afeat$ such that $(\forall x \in \posex(\abranch) \afeat \in x) \iff (\forall x \in \negex(\abranch) \afeat \in x)$) can be suppressed at no additional cost since this can be detected when computing the feature ordering criterion.




\section{Experimental Results}

All experiments were run
on 7 cluster nodes, each with 36 Intel Xeon CPU E5-2695 v4 2.10GHz cores
running Linux Ubuntu 16.04.4. Sources were compiled using g++8. 
Every algorithm was run ten times with the same set of distinct random seeds and a time limit of one hour. We imposed a memory limit of 3.5GB on every instance in the data sets, except for \dleight, where the limit was raised to 50GB.

We used 47 datasets [from...], and for each dataset, we set the maximum depth using integers in $[3,10]$. We organized the results by providing averaged data over the 47 datasets, for each value of maximum depth.
The raw experimental data from these experiments can be found in the Appendix.



\subsection{Optimizing trees}

We first compare our algorithm (\budalg) to the state-of-the-art algorithms \murtree and \dleight, for computing optimal trees and proving optimality. We therefore report in Table~\ref{tab:summaryacc} the average error (error), the average accuracy (acc.), 
the ratio of optimality proofs (opt.), as well as the average cpu time (cpu) to either prove optimality or find the best solution.
As \dleight does not provide a solution for every dataset (on some instance it goes over the memory limit of 50GB), we provide the number of datasets for which a solution was returned (sol.). Moreover, 
instead of absolute values, we provide the average relative difference in error and accuracy w.r.t. \budalg, however, and only for the datasets where a decision tree was found. Similarly, we report the average cpu time ratio w.r.t. \budalg, however, only for instances which were proven optimal by both algorithms\footnote{every instance proven optimal by \dleight is also proven optimal by \budalg and \murtree}.


% We report in Table~\ref{tab:summaryacc} data averaged over the 47 datasets described above, for
%
%
% the average accuracy found within the one hour time limit for \budalg and \murtree
%
% on relatively shallow trees (3,4 and 5) in tables~\ref{tab:d3}, \ref{tab:d4} and \ref{tab:d5}, respectively.
% We give the minimum \emph{error}, the cpu in seconds \emph{time} and size of the search space (\emph{choices}) required to prove optimality (when a proof is given, as markes by a 1 in the column \emph{opt}) or to find the best solution (otherwise).


% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=3.7pt
% \input{src/tables/summaryopt.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:summary} Comparison with the state of the art: computing optimal trees}
% \end{table}


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.5pt
\input{src/tables/summaryacc.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryacc} Comparison with the state of the art: computing optimal trees}
\end{table}


\subsection{Computing deep classifiers}

Next, we shift our focus to how fast can we obtain accurate trees and how fast can we improve the accuracy over basic solutions found by heuristics.
Therefore, in order to have a baseline, we use a well known heuristic: \cart (we ran its implementation in scikit-learn).
Here we report the average error after a given period of time (3 seconds, 10 seconds, 1 minute or 5 minutes) in Table~\ref{tab:summaryspeed}.


\medskip

We can see that the first solution found by our algorithm has comparable accuracy (actually, often slightly better) to the one found by \cart. The implementation of \cart in scikit-learn does not seem to be very efficient computationally. However, this is not so relevant as it is clear that one greedy run of the heuristic can be implemented to be as fast as the first dive of \budalg. The point of this experiment is threefold: first, it shows that the first solution is very similar to that found by \cart (here we believe that the only difference is to use the feature yielding the minimum error instead of the feature with minimum Gini impurity for the deepest feature tests); second, this tree
is found extremely quickly, and there is no scaling issue with respect to the depth of the tree or with respect to the size of the dataset; third, even for large datasets and deep trees, the accuracy of the initial classifier can be significantly improved given a reasonable computation time.

%Moreover, although we would need larger data sets to be confident about that, it seems that our algorithm is faster than \cart to find this first decision tree. %One can conjecture that \cart uses more sophisticated heuristic choices to explain these two observations.

%Then, in most cases, it is possible to improve the first solution significantly within a few seconds. Notice that for larger depth, improving the initial solution is harder and the 3s time limit is comparatively tighter than for smaller trees, so the gain of \budalg over \cart is more sensible for small trees.


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.5pt
\input{src/tables/summaryspeed.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:summaryspeed} Comparison with state the of the art: computing accurate trees}
\end{table}


Figure~\ref{fig:cactus} reports the evolution of accuracy over time, giving a good view of the difference between \murtree and \budalg during search. The accuracy of the tree returned by \cart is given for reference. 
We can see in those graphs that \murtree finds an initial tree extremely quickly, although with very low accuracy. This is because \murtree show progress even when the tree is not complete, i.e., the first solution is always a single node with the most promising feature. We can see in Table~\ref{tab:summaryspeed} that this is indeed always the same feature, irrespective of the maximum depth.




% \begin{figure}
% 	\subfloat[depth=3]{\input{src/tables/scerror3.tex}}
% 	\subfloat[depth=5]{\input{src/tables/scerror5.tex}}
% 	\subfloat[depth=7]{\input{src/tables/scerror7.tex}}
%
% 	\subfloat[depth=8]{\input{src/tables/scerror8.tex}}
% 	\subfloat[depth=9]{\input{src/tables/scerror9.tex}}
% 	\subfloat[depth=10]{\input{src/tables/scerror10.tex}}
% 	\caption{\label{fig:cactus}Accuracy over time}
% \end{figure}

\subsection{Factor analysis}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=1.5pt
\input{src/tables/factorbfs.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:factor} Factor analysis}
\end{table}


\bibliographystyle{plain}
\bibliography{src/references}

\clearpage

% \newgeometry{bottom=2cm,top=2cm,margin=1cm}

\section*{Appendix}

\begin{table}[htbp]%
\begin{center}%
\begin{footnotesize}%
\tabcolsep=2pt%
\input{src/tables/allopt3.tex}%
\end{footnotesize}%
\end{center}%
\caption{\label{tab:all3} Comparison with state of the art: depth 3}%
\end{table}%

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt4.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all4} Comparison with state of the art: depth 4}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt5.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all5} Comparison with state of the art: depth 5}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt6.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all6} Comparison with state of the art: depth 6}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt7.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all7} Comparison with state of the art: depth 7}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt8.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all8} Comparison with state of the art: depth 8}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt9.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all9} Comparison with state of the art: depth 9}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allopt10.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:all10} Comparison with state of the art: depth 10}
\end{table}




\begin{table}[htbp]%
\begin{center}%
\begin{footnotesize}%
\tabcolsep=2pt%
\input{src/tables/allfact3.tex}%
\end{footnotesize}%
\end{center}%
\caption{\label{tab:fact3} Factor analysis: depth 3}%
\end{table}%

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact4.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact4} Factor analysis: depth 4}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact5.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact5} Factor analysis: depth 5}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact6.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact6} Factor analysis: depth 6}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact7.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact7} Factor analysis: depth 7}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact8.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact8} Factor analysis: depth 8}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact9.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact9} Factor analysis: depth 9}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{footnotesize}
\tabcolsep=2pt
\input{src/tables/allfact10.tex}
\end{footnotesize}
\end{center}
\caption{\label{tab:fact10} Factor analysis: depth 10}
\end{table}


\end{document}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allacc9.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 9}
\end{table}


\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allspeed3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 3}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allspeed4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 4}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allspeed9.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 9}
\end{table}

\medskip




\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 3}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 4}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 5}
\end{table}

\medskip

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=4pt
\input{src/tables/allopt6.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:all} Comparison with state of the art: depth 6}
\end{table}

\medskip


When the maximum depth and number of feature is not too large, both algorithms are comparable, although \budalg is systematically faster. However, when the depth or the number of features grows, the best solution found by \dleight is often of much lower quality. In fact, in most cases, it reaches the time or memory limit without outputing a solution (the missing entries corresponds to \dleight reaching the 50GB memory limit). Notice that \budalg uses a tiny memory space (much lower than the size of the data set).



\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/cpusmall3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d3} Comparison with state of the art on shallow trees (max depth=3)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/cpusmall4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d4} Comparison with state of the art on shallow trees (max depth=4)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/cpusmall5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d5} Comparison with state of the art on shallow trees (max depth=5)}
\end{table}







\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f4} Comparison with state of the art heuristics (max depth=4)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first7.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f7} Comparison with state of the art heuristics (max depth=7)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first10.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f10} Comparison with state of the art heuristics (max depth=10)}
\end{table}


\subsection{Size stats}


\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s3} max depth=3}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s4} max depth=4}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s5} max depth=5}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex7.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s7} max depth=7}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex10.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s10} max depth=10}
\end{table}

%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall3.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha3} Comparison of heuristics (max depth=3)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall4.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha4} Comparison of heuristics (max depth=4)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall5.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha5} Comparison of heuristics (max depth=5)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall7.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha7} Comparison of heuristics (max depth=7)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall10.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha10} Comparison of heuristics (max depth=10)}
% \end{table}
%
%
%
%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst3.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha3} Comparison of heuristics (max depth=3)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst4.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha4} Comparison of heuristics (max depth=4)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst5.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha5} Comparison of heuristics (max depth=5)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst7.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha7} Comparison of heuristics (max depth=7)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst10.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha10} Comparison of heuristics (max depth=10)}
% \end{table}


% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth5b.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=5)}
% \end{table}

% \clearpage

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth8.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=8)}
% \end{table}

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth10.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=10)}
% \end{table}



\end{document}

