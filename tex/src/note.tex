\documentclass{article}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%% http://ctan.org/pkg/xcolor
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{array}
%\usepackage{amsthm}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{tikz}
\usepackage{fp}
\usepackage{subfig}
\usetikzlibrary{arrows,shadows,fit,calc,positioning,decorations.pathreplacing,matrix,shapes,petri,topaths,fadings,mindmap,backgrounds,shapes.geometric}
\usepackage{geometry}



\newcommand{\setex}[1]{\ensuremath{{\mathcal X}^{#1}}\xspace}
\newcommand{\posex}{{\setex{1}}\xspace}
\newcommand{\negex}{{\setex{0}}\xspace}
\newcommand{\setcube}[1]{\ensuremath{{\mathcal C}^{#1}}\xspace}
\newcommand{\poscube}{{\setcube{1}}\xspace}
\newcommand{\negcube}{{\setcube{0}}\xspace}
\newcommand{\features}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\classifier}{\ensuremath{f}}
\newcommand{\lit}[1]{\ensuremath{l_{#1}}}
\newcommand{\var}{\ensuremath{x}}
\newcommand{\truelit}[1]{\ensuremath{\var_{#1}}}
\newcommand{\falselit}[1]{\ensuremath{\bar{\var_{#1}}}}
\newcommand{\ex}{\ensuremath{\var}}
\newcommand{\cube}{\ensuremath{c}}
\newcommand{\universe}{\ensuremath{{\mathcal U}}}



\title{Reducing Data Sets by Computing Explanations}

% \author{Emmanuel Hebrard\inst{1} \and George Katsirelos\inst{2}}
% \institute{LAAS-CNRS, Universit\'e de Toulouse, CNRS, France, email: hebrard@laas.fr
  % \and MIAT, UR-875, INRA, France, email: gkatsi@gmail.com \footnote{The second author was partially supported by the french ``Agence nationale de
% la Recherche'', project DEMOGRAPH, reference ANR-16-C40-0028.}}

\begin{document}
	\newgeometry{bottom=3cm,top=3cm}

\maketitle

\section*{Problem statement}

We consider a data set composed a set \posex of positive examples and a set \negex of negative examples over a set of binary features $\features$.
%$ = \{1,\ldots,m\}$.
The classic task in machine learning is to compute a function $\classifier : 2^{\features} \mapsto \{0,1\}$ that \emph{generalizes} the data set.

In this note we show how we can reduce the data sets by representing \posex and \negex as unions of \emph{cubes} (or DNFs) that we will denote respectively \poscube and \negcube, respectively. Let $\truelit{i}$ be a literal standing for ``feature $i$ is true'' and its negation $\falselit{i}$ standing for ``feature $i$ is false''. An instance $\ex$ is a conjunction $\bigwedge_{i \in \features}\lit{i}$ where $\lit{i}$ is either $\truelit{i}$ or $\falselit{i}$, an example is an instance $\ex \in \posex \cup \negex$.
A cube $\cube \in \poscube \cup \negcube$ is a conjunction $\bigwedge_{i \in {\mathcal S}}\lit{i}$ where $\lit{i}$ is either $\truelit{i}$ or $\falselit{i}$, for a subset ${\mathcal S}$ of \features.
The cubes \poscube and \negcube have the following properties:
\begin{eqnarray}
	\forall \ex \in \posex \exists \cube \in \poscube & \ex \vdash \cube \label{poscomplete} \\
	\forall \ex \in \negex \exists \cube \in \negcube & \ex \vdash \cube \label{negcomplete} \\
	\forall \cube \in \poscube  \forall \cube' \in \negcube  & \cube \wedge \cube' \vdash \perp \label{consistent}
\end{eqnarray}

Properties (\ref{poscomplete}) and (\ref{negcomplete}) state that \poscube and \negcube are complete, i.e., they entail every positive and negative example, respectively. Property (\ref{consistent}) states that \poscube and \negcube are consistent, i.e., there is no instance $\ex$ such that $\poscube \vdash \ex$ and $\negcube \vdash \ex$.

 The reduced data set $(\poscube,\negcube)$ can be seen as a classifier $\classifier : 2^{\features} \mapsto \{0,1,*\}$ with a ``do not know'' value $*$ since it generalizes the original data set to some extent:
 $$
 \classifier(\ex) = \begin{cases} 1 \textrm{~if~} \poscube \vdash \ex\\ 0 \textrm{~if~} \negcube \vdash \ex\\ * \textrm{~otherwise} \end{cases}
 $$
 
 Moreover, standard machine learning techniques can be applied almost directly on such a reduced data set, thus improving their scalability (without degrading their accuracy?): usually the irrelevant features can be handled, and if not, it is always possible to generate full examples by random sampling within a cube.
 
 
 \section*{Algorithm}
 
 
 We represent instances and cubes as subsets of
 $\universe = \{\truelit{i} \mid i\in \features \} \cup \{\falselit{i} \mid i\in \features \}$.
% $\bar{\features} = \{i+|\features| : i \in \features\}$.
 %That is, the set $\ex \subseteq U$ is interpreted as the conjunction $\biwedge_{i \in \ex}$
 Notice that $\truelit{i} \in \ex$ and $\falselit{i} \in \ex$ are mutually exclusive, but $\truelit{i} \not\in \ex$ and $\falselit{i} \not\in \ex$ are not.
 Given a set $\cube \in \universe$ we denote $\bar{\cube}$ the set $\{\truelit{i} \mid \falselit{i} \in \cube\} \cup \{\falselit{i} \mid \truelit{i} \in \cube\}$.
 
 These sets are implemented as bitsets over $2|\features|$ bits, with the first $|\features|$ bits standing for positive literals and the second $|\features|$ bits standing for negative literals. Notice that computing $\bar{\cube}$ can be done by swaping the two halves of the bitset standing for $\cube$. Taking the complement would work for examples (and instances), but not for cubes.
 
 
 \begin{algorithm}
	\SetKwFunction{explain}{Comprime}
   \TitleOfAlgo{\explain} 
   \KwData{A data set $(\posex,\negex)$}
   \KwResult{An explanation set $(\poscube, \negcube)$}
	 $(\poscube, \negcube) \gets (\emptyset, \emptyset)$\;
	 \While{$\posex \cup \negex \neq \emptyset$}
	 {
			\lnl{l:classselect} choose $c \in \{y \mid \setex{y} \neq \emptyset\}$\;
			\lnl{l:exselect} choose $\ex^y \in \setex{y}$\;
			$implicant \gets \emptyset$\;
			$candidates \gets \ex^y$\;
			\ForEach{$\ex \in \setcube{1-y} \cup \setex{1-y}$}
			{
				\If{$implicant \cap \bar{\ex} = \emptyset$}
				{
					$\delta \gets \ex^y \cap \bar{\ex}$\;
					\If{$candidates \cap \delta = \emptyset$} {
						choose $\lit{i} \in candidates$\;
						$implicant \gets implicant \cup \{\lit{i}\}$\;
						$candidates \gets \ex^y$\;
					}
					$candidates \gets candidates \cap \delta$\;
				}
			}
			choose $\lit{i} \in candidates$\;
			$implicant \gets implicant \cup \{\lit{i}\}$\;
			$\setcube{y} \gets \setcube{y} \cup \{implicant\}$\;
			$\setex{y} \gets \setex{y} \setminus \{\ex \mid \ex \in \setex{y}, implicant \subseteq \ex\}$
	 }
	 \Return{$(\poscube, \negcube)$}\;
	 
\end{algorithm}


\section*{Experimental results}

We ran \explain on some data sets, whose number of examples and (binary) features are given respectively in the second and third columns of the following tables. We report the total \emph{size} of the set of cubes, i.e., the sum of the number of literals in all the cubes in Table~\ref{tab:vol}; the number of cubes in Table~\ref{tab:exa}; and the average number of literals per cube in Table~\ref{tab:fea};

We tested several variants with five strategies for selecting the next class for which we compute a cube (at Line~\ref{l:classselect}):
\begin{itemize}
	\item \texttt{positive} always selects examples from $\posex$ first
		\item \texttt{negative} always selects examples from $\negex$ first
	\item \texttt{altern} always selects from the $\setex{1-c}$ where $\setex{c}$ is the previous choice
\item \texttt{uniform} selects from $\posex$ with probability $1/2$ and from $\negex$ otherwise
\item \texttt{biased} selects from $\posex$ with probability $|\posex|/(|\posex|+|\negex|)$ and from $\negex$ otherwise
\end{itemize}
and two strategies for selecting the example $\ex^y$ to minimize in $\setex{y}$ (at Line~\ref{l:exselect}):
\begin{itemize}
	\item \texttt{first} selects examples in input order
	\item \texttt{random} selects any example randomly with uniform probability
\end{itemize}

\newgeometry{left=.1cm,bottom=.1cm,top=.1cm}

\begin{center}

\begin{table}[h!]
\begin{center}
\begin{scriptsize}
\tabcolsep=2.1pt
\input{src/tables/vol.tex}
\end{scriptsize}
\end{center}
\vspace{-.1cm}
\caption{\label{tab:vol} Total size (total number of literals)}
\end{table}

\vspace{-.1cm}

\begin{table}[h!]
\begin{center}
\begin{scriptsize}
\tabcolsep=2pt
\input{src/tables/exa.tex}
\end{scriptsize}
\end{center}
\vspace{-.25cm}
\caption{\label{tab:exa} Number of explanations}
\end{table}

\vspace{-.1cm}

\begin{table}[h!]
\begin{center}
\begin{scriptsize}
\tabcolsep=2.3pt
\input{src/tables/fea.tex}
\end{scriptsize}
\end{center}
\vspace{-.25cm}
\caption{\label{tab:fea} Mean number of features}
\end{table}

\end{center}

\restoregeometry




\end{document}

