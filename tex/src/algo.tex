\documentclass{article}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}%% http://ctan.org/pkg/xcolor
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{array}
%\usepackage{amsthm}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{tikz}
\usepackage{fp}
\usepackage{subfig}
\usetikzlibrary{arrows,shadows,fit,calc,positioning,decorations.pathreplacing,matrix,shapes,petri,topaths,fadings,mindmap,backgrounds,shapes.geometric}
\usepackage{geometry}
\usepackage{xifthen}
\usepackage{rotating}

\newgeometry{bottom=2cm,top=2cm}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newtheorem{theorem}{Theorem}


\newcommand{\tru}[0]{\texttt{true}}
\newcommand{\fal}[0]{\texttt{false}}
% \newcommand{\tru}[0]{1}
% \newcommand{\fal}[0]{0}


\newcommand{\setex}[1]{\ensuremath{{\mathcal X}^{#1}}\xspace}
\newcommand{\posex}{{\setex{\top}}\xspace}
\newcommand{\negex}{{\setex{\bot}}\xspace}
\newcommand{\setcube}[1]{\ensuremath{{\mathcal C}^{#1}}\xspace}
\newcommand{\poscube}{{\setcube{1}}\xspace}
\newcommand{\negcube}{{\setcube{0}}\xspace}
\newcommand{\features}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\feat}{\ensuremath{f}}
\newcommand{\classifier}{\ensuremath{f}}
\newcommand{\lit}[1]{\ensuremath{l_{#1}}}
\newcommand{\var}{\ensuremath{x}}
\newcommand{\truelit}[1]{\ensuremath{\var_{#1}}}
\newcommand{\falselit}[1]{\ensuremath{\bar{\var_{#1}}}}
\newcommand{\ex}{\ensuremath{\var}}
\newcommand{\cube}{\ensuremath{c}}
\newcommand{\universe}{\ensuremath{{\mathcal U}}}
\newcommand{\entropy}[1]{\ensuremath{{H}({#1})}\xspace}
\newcommand{\probability}[1]{\ensuremath{{Pr}({#1})}\xspace}



\newcommand{\nodes}[0]{\ensuremath{{\cal N}}}
\newcommand{\bud}[0]{\ensuremath{{\cal B}}}
\newcommand{\sequence}[0]{\ensuremath{{\cal S}}}

\newcommand{\maxd}[0]{\ensuremath{k}}
\newcommand{\anode}[0]{\ensuremath{i}}
\newcommand{\bnode}[0]{\ensuremath{b}}
\newcommand{\afeat}[0]{\ensuremath{f}}
\newcommand{\aclass}[0]{\ensuremath{l}}
\newcommand{\ub}[0]{\ensuremath{ub}}
% \newcommand{\error}[0]{\ensuremath{error}}
\newcommand{\depth}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{depth}{depth({#1})}}}
\newcommand{\test}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{test}{test({#1})}}}
\newcommand{\dom}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{dom}{dom({#1})}}}
\newcommand{\best}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{best}}{{best({#1})}}}}
\newcommand{\maxe}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{max}}{{max({#1})}}}}
\newcommand{\opt}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{terminal}{terminal({#1})}}}
\newcommand{\child}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{child}}{{child({{#1}})}}}}
\newcommand{\error}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{error}}{{error({{#1}})}}}}
\newcommand{\classlabel}[1][]{\ensuremath{\ifthenelse{\equal{#1}{}}{{y}}{{y({{#1}})}}}}

\newcommand{\abranch}[0]{\ensuremath{b}}



	\SetKwFunction{storebest}{storeBest}
	\SetKwFunction{solution}{solution}
	\SetKwFunction{deadend}{deadEnd}
	\SetKwFunction{expend}{expend}
	\SetKwFunction{backtrack}{backtrack}
	\SetKwFunction{dt}{BTDT}
	\SetKwFunction{pop}{pop}
	\SetKwFunction{push}{push}
	\SetKwFunction{prune}{prune}
	% \SetKwFunction{error}{error}
	\SetKwFunction{grow}{grow}
	\SetKwFunction{branch}{branch}
	\SetKwFunction{select}{select\&remove}
	\SetKwFunction{budalg}{Bud-first-search}
	\SetKwFunction{dynprog}{DynProg}
	\SetKwFunction{dleight}{DL8.5}
	\SetKwFunction{cart}{CART}
	\SetKwFunction{greedy}{BudFS (first sol.)}
	
	

\DontPrintSemicolon

\title{Backtracking DT}

% \author{Emmanuel Hebrard\inst{1} \and George Katsirelos\inst{2}}
% \institute{LAAS-CNRS, Universit\'e de Toulouse, CNRS, France, email: hebrard@laas.fr
  % \and MIAT, UR-875, INRA, France, email: gkatsi@gmail.com \footnote{The second author was partially supported by the french ``Agence nationale de
% la Recherche'', project DEMOGRAPH, reference ANR-16-C40-0028.}}

\begin{document}


\maketitle

\section*{Introduction}

We consider the problem of finding the bounded-depth decision tree of maximum accuracy.
The state of the art includes MIP approaches (BinOCT), a MaxSAT approach based on the SAT encoding proposed by Narodytska et al, and DL8.5.
The latter algorithm is by far the most efficient, however, it is not \emph{anytime}: the left branch must be optimally solved before a solution of the right branch can be found. Moreover the use of a cache structure means that it uses a lot of memory. This algorithm is practical for a maximum depth of 4 (although using gigabytes of memory) but often not much beyond. Therefore, in a number of cases, a greedy heuristic (such as CART) is still the best method in practice.

\medskip

In this note we introduce what is essentially an anytime version of DL8.5, without cache.
This algorithm therefore uses linear (in the size of the tree) memory and anytime, hence in principle strictly better than CART. Moreover, on instance where DL8.5 can find a solution, the algorithm described in this note is significantly faster (by about a factor 10).

\section*{Notations}

A binary data set is a pair $\langle \negex,\posex \rangle$ where $\negex$ (resp. $\posex$) is a subset of $2^{\features}$. It is associated a label function $\classlabel : 2^{\features} \mapsto \{\top,\bot\}$ such that:
$$
\forall \aclass \in \{\top,\bot\}, \forall \ex \in \setex{\aclass}, \classlabel[\ex] = \aclass
$$
%$\top$ and $\bot$ are class labels and 
A datapoint $\ex$ can equivalently be seen as a subset of $\features$, or as the conjunction:
$$
\bigwedge_{\afeat \in \ex}\afeat \wedge \bigwedge_{\afeat \in \features \setminus \ex}\bar{\afeat}
$$

A \emph{branch} of a decision tree is a conjunction of (possibly negated) features. 
%Moreover, if we also consider data points as conjunctions of features (where every feature appears either positively or negatively),
%given a branch $\abranch \subseteq \features$ 
Given a data set $\langle \negex,\posex \rangle$, we can associate a data set $\langle \negex(\abranch),\posex(\abranch) \rangle$ to a branch $\abranch$ where:
\begin{eqnarray*}
\negex(\abranch) = \{\ex \mid \ex \in \negex, \ex \models \abranch\}\\
\posex(\abranch) = \{\ex \mid \ex \in \posex, \ex \models \abranch\}
\end{eqnarray*}

We write $\error[\abranch]$ for $\min(|\negex(\abranch)|, |\posex(\abranch)|)$, and $\error[\abranch,\afeat]$ for $\error[\abranch \wedge \afeat] + \error[\abranch \wedge \bar{\afeat}]$.
A branch \abranch\ is said \emph{pure} iff $\error[\abranch]=0$.

\medskip

Given a binary dataset $\langle \negex,\posex \rangle$ with label function $\classlabel$,
the \emph{minimum error bounded depth decision tree problem} consists in finding the minimum value $\epsilon$ such
there exists a binary tree of depth at most $\maxd$ whose sum of error $\error[\abranch]$ for all branches $\abranch$ from the root to the leaves is equal to $\epsilon$.


\section*{Dynamic Programming Algorithm}

The solver DL8.5 is a dynamic programming algorithm. It relies on the observation that given a feature test, the two resulting branches are independent subproblems. Algorithm~\ref{alg:dynprog} gives a high level view of DL8.5.


	% \begin{algorithm}
	% 	\caption{Dynamic Programming Algorithm\label{alg:dynprog}}
	% 	\TitleOfAlgo{\dynprog}
	% 	  \KwData{$\negex,\posex,\maxd,\abranch[=(\tru)]$}
	% 	  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
	% 		\lIf{$\maxd = 0$ or $\error[\abranch] = 0$} {
	% 		\Return $\error[\abranch]$
	% 		}
	% 		$\best \gets \error[\abranch]$\;
	% 		\ForEach{$\afeat \in \features \setminus \abranch$} {
	% 				$\best \gets \min(\best, \dynprog(\negex,\posex, \abranch \wedge \afeat, \maxd-1) + \dynprog(\negex,\posex, \abranch \wedge \bar{\afeat}, \maxd-1))$\;
	% 		}
	% 		\Return $\best$\;
	% \end{algorithm}
	
	\begin{algorithm}
		\caption{Dynamic Programming Algorithm\label{alg:dynprog}}
		\TitleOfAlgo{\dynprog}
		  \KwData{$\negex,\posex,\maxd$}
		  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
			$\error \gets \min(|\negex|,|\posex|)$\;
			
			\lIf{$\maxd = 0$ or $\error = 0$} {
			\Return $\error$
			}
			
			\ForEach{$\afeat \in \features$} {
					$\error \gets \min(\error, \dynprog(\negex(\afeat),\posex(\afeat), \maxd-1) + \dynprog(\negex(\bar{\afeat}),\posex(\bar{\afeat}), \maxd-1))$\;
			}
			\Return $\error$\;
	\end{algorithm}



\section*{An Anytime Algorithm}

In a nutshell the algorithm we propose works as follows:

\medskip

We start from a singleton set \bud\ of open branches or \emph{buds}.
% (open branches are branches of length strictly less than $\maxd$ that are not pure). % The set \nodes\ contains all the nodes of the current tree, open or closed, it is initially equal to \bud. Finally,
Moreover, we use an initially empty stack of decisions $\sequence$.

\begin{itemize}
	\item As long as there is a bud ($\bud \neq \emptyset$), we pick any one $\abranch \in \bud$, a feature $\afeat$ marked as \emph{available} for \abranch, add the pair $(\abranch,\afeat)$ to \sequence\ and expend the tree with the two branches $\abranch \wedge \afeat$ and $\abranch \wedge \bar{\afeat}$. 
	%These new nodes are added to $\nodes$. 
	They are added to $\bud$ if their depth is strictly less than $\maxd-1$ (the last feature test is chosen according to minimum error) and if they are not pure, otherwise they are terminal tests and we record the corresponding error.

\item As long as there is no bud ($\bud = \emptyset$), we pop the last assignment $(\abranch,\afeat)$ from \sequence, mark the feature $\afeat$ not available for branch $\abranch$ and update the recorded best error of its subtrees. If there is at least one available feature for branch $\abranch$, we add $\abranch$ to $\bud$. 
Otherwise, we consider it terminal and its error is the sum of the error of its best subtrees.
% and we 
%store the minimum error recorded for any of the possible features.

\end{itemize}

The classification error of a tree is equal to the sum of the error of its terminal banches, the algorithm returns the minimum value encountered when exploring the possible decision trees.

\medskip
		
		Let $n = |\posex| + |\negex|$, $m = |\features|$, with the maximum depth $k \leq m$. 
		The number of recursive calls for Algorithm~\ref{alg:dynprog} is $\Theta(2^km^k)$. At each call, the data set must be split into two subsets. This splitting procedure can be done in time $\Theta(n)$ amortised over a given level of a given decision tree. Since every branch of the tree is independent, Algorithm~\ref{alg:dynprog} explores $O(m^d)$ trees of depth $d$. Therefore Algorithm~\ref{alg:dynprog} runs in $\Theta((n+2^k)m^k)$ time.
		
		\begin{theorem}
			The time complexities of Algorithms~\ref{alg:dynprog} and Algorithm~\ref{alg:bud} are equal.
			\end{theorem}
			
			No proof is given, but independent subtrees are not explored in Algorithm~\ref{alg:bud} hence both algorithms do the same computation, except not in the same order. Notice that Algorithm~\ref{alg:bud} eagerly compute the conditional error $\error[\abranch,\afeat]$ for every feature $\afeat \in \features$, which incurs an extra factor $m$ for the data set partitionning task. However, in return the last test of each branch is chosen in $O(1)$ so we gain the same factor $m$.
			
			The big difference is that whereas the cost of finding a first solution is $\Theta((n + 2^{k-1})m^{k-1})$ for Algorithm~\ref{alg:dynprog}, it is equal to $\Theta(kn + 2^k)$ for Algorithm~\ref{alg:bud}, which in practice is very important, as shown in the experimental section.
		
	%T(m,k) = 2mT(m-1, k-1)	
		
		


% \clearpage





	\begin{algorithm}
		\caption{Anytime Algorithm\label{alg:bud}}
		\TitleOfAlgo{\budalg}
		  \KwData{$\negex,\posex, \maxd$}
		  \KwResult{The minimum error on $\negex,\posex$ for decision trees of depth at most $\maxd$}
		$\sequence \gets []$\;
		$\bud \gets \{\emptyset\}$\;
		$\error \gets \min(|\negex|,|\posex|)$\;
		$\dom \gets (\lambda : {2^{\features}} \mapsto \features)$\;
		$\best \gets (\lambda : {2^{\features}} \mapsto \infty)$\;
		% $\opt \gets (\lambda : {2^{\features}} \mapsto \fal)$\;
		
		\While{$|\sequence| + |\bud| > 0$}{
		\eIf{$\bud \neq \emptyset$}{
			$\abranch \gets \select{\bud}$\;
			$\afeat \gets \argmin_{\afeat \in \dom[\abranch]}(\error[\abranch,\afeat])$\;
			\eIf{$\error[\abranch,\afeat] = 0$ or $|\abranch| = \maxd-1$} {
				$\error \gets \error + \error[\abranch,\afeat]$\;
				$\best[\abranch] \gets \error[\abranch,\afeat]$\;
				% $\opt[\abranch] \gets \tru$\;
			}{
			$\dom[\abranch] \gets \dom[\abranch] \setminus \{\afeat\}$\;
			push $(\abranch,\afeat)$ on $\sequence$\; % $ \gets \sequence \oplus (\abranch,\afeat)$\;
			\ForEach{$v \in \{\afeat, \bar{\afeat}\}$}{
					\lIf{$\error[\abranch,v] > 0$}{
						$\bud \gets \bud \cup \{\abranch \wedge v\}$
					}
			}
			}
		}{
			$\best[\emptyset] \gets \min(\best[\emptyset], \error)$\;
			\Repeat{$\dom[\abranch] = \emptyset$ and $|\sequence|>0$}{
				pop $(\abranch,\afeat)$ from $\sequence$\;
				$\best[\abranch] \gets \min(\best[\abranch], \best[\abranch \wedge \afeat] +  \best[\abranch \wedge \bar{\afeat}])$\;
				$\error \gets \error - \best[\abranch \wedge \afeat] -  \best[\abranch \wedge \bar{\afeat}]$\;
				\lIf{$\dom[\abranch] \neq \emptyset$} {
					$\bud \gets \bud \cup \{\abranch\}$
					% \lIf{$\opt[\abranch]$}{$\error \gets \error - \best[\abranch]$}
				} 
				\lElse {	
					$\error \gets \error + \error[\abranch,\afeat]$
				}
				% {
				% 	$\opt[\abranch] \gets \tru$\;
				% }
			}
		}
		}
		\Return $\best[\emptyset]$\;
	\end{algorithm}
	
	



% For readability, we cut the algorithm into four blocks. The initialisation procedure (Algorithm~\ref{alg:init}) set up the data structures used in all other procedures:
% \begin{itemize}
% 	\item \sequence\ is simply the list of nodes in the current tree, ordered as they are explored.
% 	\item \nodes\ is the set of integers used to index a node of the current tree
% 	\item \bud\ is the set of nodes which do no have an assigned test yet
% 	\item \depth\ stores the depth of a node
% 	\item \test\ stores the feature tested at a node
% 	\item \dom\ stores the set of possible features which have no yet been tried for this node
% 	\item \best\ stores the error of the best subtree rooted at a node
% 	\item \opt\ indicates whether the best subtree of a given node is optimal
% 	\item \child\ stores the children of a node (children can be nodes or $\{\top, \bot\}$)
% 	\item $\error{\anode}$ $\min(|\posex(\anode)|,|\negex(\anode)|)$
% 	\item $\error{\anode,\afeat}$ $\min(|\posex(\anode=\afeat)|,|\negex(\anode=\afeat)|)$
% \end{itemize}
%
% Algorithm~\ref{alg:search} is a bactracking procedure which expends a current decision tree
%
% 	\begin{algorithm}
% 		\caption{Data Structures\label{alg:init}}
% 		\TitleOfAlgo{Initialise}
% 		$\sequence \gets []$\;
% 		$\bud \gets \emptyset$\;
% 		$\nodes \gets \emptyset$\;
% 		$\ub \gets \min(|\negex|,|\posex|)$\;
% 		$\error \gets ub$\;
%
% 		$\child \gets (\lambda : \mathbb{N} \times \{\fal, \tru\} \mapsto \emptyset)$\;
% 		$\depth \gets (\lambda : \mathbb{N} \mapsto 0)$\;
%
% 		$\test \gets (\lambda : \mathbb{N} \mapsto \emptyset)$\;
% 		$\dom \gets (\lambda : \mathbb{N} \mapsto \features)$\;
%
% 		$\best \gets (\lambda : \mathbb{N} \mapsto \infty)$\;
% 		$\opt \gets (\lambda : \mathbb{N} \mapsto \fal)$\;
% 	\end{algorithm}
%
%
%
% 	\begin{algorithm}
% 		\caption{Create a new node after branching\label{alg:alloc}}
% 		\TitleOfAlgo{\grow}
% 	  \KwData{integer \anode}
%
% 		$\nodes \gets \nodes \cup \{\anode\}$\;
% 		$\dom[\anode] \gets \features$ sorted by decreasing conditional error $\min(|\posex(\anode=\afeat)|,|\negex(\anode=\afeat)|)$\;
% 		$\test[\anode] \gets \pop(\dom[\anode])$\;
%
%
% 		\eIf{$\depth[\anode]=k-1$ or $\error{\anode,\test[\anode]}$}
% 		{
% 			$\best[\anode] = \error{\anode,\test[\anode]}$\; %\min(|\posex(\anode=\test[\anode])|,|\negex(\anode=\test[\anode])|)$\;
% 			$\opt[\anode] = \tru$\;
% 			\ForEach{$branch \in \{\tru, \fal\}$}
% 			{
% 				% $\child[\anode,branch] \gets (|\posex(\anode=\test[\anode])| > |\negex(\anode=\test[\anode])|)$\;
% 				\lIf{$|\posex(\anode=\test[\anode])| > |\negex(\anode=\test[\anode])|$}{$\child[\anode,branch] \gets \top$}
% 				\lElse{$\child[\anode,branch] \gets \bot$}
% 			}
% 		}
% 		{
% 			$\bud \gets \bud \cup \{n\}$\;
% 			$\best[\anode] = \min(|\posex(\anode)|, |\negex(\anode)|)$\;
% 			$\opt[\anode] \gets \fal$\;
% 		}
%
%
% 	\end{algorithm}
%
%
% 	\begin{algorithm}
% 		\caption{Suppress a node and all its descendants\label{alg:free}}
% 		\TitleOfAlgo{\prune}
% 	  \KwData{integer \anode}
%
% 		$\bud \gets \bud \setminus \{\anode\}$\;
% 		$\nodes \gets \nodes \setminus \{\anode\}$\;
%
% 		\ForEach{$branch \in \{\tru, \fal\}$}
% 		{
% 		\lIf{$\child[\anode,branch] \not\in \{\top, \bot\}$}
% 		{
% 			$\prune{\child[\anode,branch]}$
% 		}
% 		}
%
% 		\lIf{$\depth[\anode] = k-1$ or $\opt[\anode]$}{$error \gets error - \best[\anode]$}
%
% 	\end{algorithm}
%
%
% \begin{algorithm}
% 	\caption{Search loop\label{alg:search}}
%   \TitleOfAlgo{\dt}
%   \KwData{$\negex,\posex, k$}
%   \KwResult{A decision tree}
%
% 	$\bnode \gets 0$\;
% 	$\posex(1),\negex(1) \gets \posex, \negex$\;
% 	$\grow{\bud, \sequence, 1}$\;
%
% 	\While{\textbf{true}}{
% 		\eIf{$\bud = \emptyset$} {
% 			$\ub \gets \min(\ub,\error)$\;
% 			$deadend \gets \fal$\;
% 			\Repeat{$deadend$}{
% 				\lIf{$\bnode > 0$}{$\opt[\bnode] \gets \tru$}
% 				\lIf{$\bnode = 1$}{\Return}
% 				$\bnode \gets \pop{\sequence}$\;
% 				$\best[\bnode] \gets \min(\best[\bnode], \best(\child[\bnode,\tru]) + \best(\child[\bnode,\fal]))$\;
% 				$\test[\bnode] \gets \pop{\dom[\bnode]}$\;
% 				$\prune(\child[\bnode,\tru])$\;
% 				$\prune(\child[\bnode,\fal])$\;
%
% 				$deadend \gets \best[\bnode] = 0 ~\vee~ \dom[\bnode] = \emptyset$\;
% 				\If{$deadend$}
% 				{
% 				$\opt[\bnode] \gets \tru$\;
% 				$\error \gets \error + \error{\bnode}$\; %$\best[\bnode]$\;
% 				}
% 			}
% 			$\bud \gets \bud \cup \{b\}$\;
% 			$\error \gets \error + \min(|\posex(\bnode)|, |\negex(\bnode)|)$\;
% 		}
% 		{
% 			\If{$b = 0$}{
% 				$b=\select{\bud}$\;
% 				% $\bud \gets \bud \setminus \{b\}$\;
% 				$\push(\bnode,\sequence)$\;
% 			}
% 			$c_{\tru}, c_{\fal} = \argmin_{x,y}(\mathbb{N} \setminus \nodes)$\;
% 			$\posex(c_{\tru}),\negex(c_{\tru}),\posex(c_{\fal}),\negex(c_{\fal}) \gets \branch(\posex(\bnode),\negex(\bnode),\test[\bnode])$\;
% 			\ForEach{$branch \in \{\tru, \fal\}$}{
% 				\eIf{$\min(|\posex(c_{branch})|,|\negex(c_{branch})|) = 0$}
% 				{
% 					\lIf{$|\posex(c_{branch})|>|\negex(c_{branch})|$}{$\child[\bnode,branch] \gets \top$}
% 					\lElse{$\child[\bnode,branch] \gets \bot$}
% 				}{
% 					$\child[\bnode,branch] \gets c_{branch}$\;
% 					$\depth[c_{branch}] \gets \depth[\bnode]+1$\;
% 					$\grow(\bud, \sequence, c_{branch})$\;
% 				}
% 			}
% 			$\bnode \gets 0$\;
% 		}
% 	}
%
% \end{algorithm}

%\clearpage

\section*{Improvements}

There are a number of implementation details (among them how to return the actual tree rather than simply its classification error) and other improvements, which we will list later.


% \clearpage

\section*{Experiments}

\subsection*{Comparison with \dleight}

We first compare our algorithm (\budalg) against \dleight on relatively shallow trees (3,4 and 5) in tables~\ref{tab:d3}, \ref{tab:d4} and \ref{tab:d5}, respectively. 
We give the minimum \emph{error}, maximum \emph{accuracy} (acc.), the cpu \emph{time} and size of the search space (\emph{choices}) required to prove optimality (when a proof is given, as markes by a 1 in the column \emph{opt}) or to find the best solution (otherwise).

\medskip

When the maximum depth and number of feature is not too large, both algorithms are comparable, although \budalg is systematically faster. However, when the depth or the number of features grows, the best solution found by \dleight is often of much lower quality. In fact, in most cases, it reaches the time or memory limit without outputing a solution (the missing entries corresponds to \dleight reaching the 50GB memory limit). Notice that \budalg uses a tiny memory space (much lower than the size of the data set).



\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/depth3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d3} Comparison with state of the art for optimal small trees (max depth=3)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/depth4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d4} Comparison with state of the art for optimal small trees (max depth=4)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/depth5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:d5} Comparison with state of the art for optimal small trees (max depth=5)}
\end{table}



\subsection*{Comparison with \cart}

Next, we compare our algorithm (\budalg) against \cart (its implemention in scikit-learn) on deeper trees (4 and 7 and 10). Here we give the accuracy of the first solution found by \budalg (\greedy) and the best solution found given a 3 seconds time limit (\budalg) in tables \ref{tab:d3}, \ref{tab:d4} and \ref{tab:d5}.

\medskip

We can see that the first solution found by our algorithm has comparable accuracy (actually, often slightly better) to the one found by \cart. Moreover, although we would need larger data sets to be confident about that, it seems that our algorithm is faster than \cart to find this first decision tree. %One can conjecture that \cart uses more sophisticated heuristic choices to explain these two observations.

Then, in most cases, it is possible to improve the first solution significantly within a few seconds. Notice that for larger depth, improving the initial solution is harder and the 3s time limit is comparatively tighter than for smaller trees, so the gain of \budalg over \cart is more sensible for small trees.





\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f4} Comparison with state of the art heuristics (max depth=4)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first7.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f7} Comparison with state of the art heuristics (max depth=7)}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=5pt
\input{src/tables/first10.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:f10} Comparison with state of the art heuristics (max depth=10)}
\end{table}


\subsection*{Size stats}


\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex3.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s3} max depth=3}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex4.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s4} max depth=4}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex5.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s5} max depth=5}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex7.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s7} max depth=7}
\end{table}

\begin{table}[htbp]
\begin{center}
\begin{normalsize}
\tabcolsep=3pt
\input{src/tables/sizex10.tex}
\end{normalsize}
\end{center}
\caption{\label{tab:s10} max depth=10}
\end{table}

%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall3.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha3} Comparison of heuristics (max depth=3)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall4.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha4} Comparison of heuristics (max depth=4)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall5.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha5} Comparison of heuristics (max depth=5)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall7.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha7} Comparison of heuristics (max depth=7)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicall10.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha10} Comparison of heuristics (max depth=10)}
% \end{table}
%
%
%
%
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst3.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha3} Comparison of heuristics (max depth=3)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst4.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha4} Comparison of heuristics (max depth=4)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst5.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha5} Comparison of heuristics (max depth=5)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst7.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha7} Comparison of heuristics (max depth=7)}
% \end{table}
%
% \begin{table}[htbp]
% \begin{center}
% \begin{normalsize}
% \tabcolsep=5pt
% \input{src/tables/heuristicfirst10.tex}
% \end{normalsize}
% \end{center}
% \caption{\label{tab:ha10} Comparison of heuristics (max depth=10)}
% \end{table}


% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth5b.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=5)}
% \end{table}

% \clearpage

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth8.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=8)}
% \end{table}

% \begin{table}[htbp]
% \begin{center}
% \begin{footnotesize}
% \tabcolsep=5pt
% \input{src/tables/depth10.tex}
% \end{footnotesize}
% \end{center}
% \caption{\label{tab:thetable} Restarts (max depth=10)}
% \end{table}



\cite{LABORIE2003151}

\cite{test}

\bibliographystyle{plain}
\bibliography{src/bib}


\end{document}

